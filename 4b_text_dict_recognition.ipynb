{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022671ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time as time\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93447a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #visualisation\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce85991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ad1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418eaec",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9ac27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multicore Loading Time = 1.496028184890747\n",
      "34179\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "labelled = pd.read_csv('data/mature_labelled.csv', index_col=0, dtype='string')\n",
    "e = time.time()\n",
    "print(\"Multicore Loading Time = {}\".format(e-s))\n",
    "\n",
    "print(len(labelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43db245a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_date</th>\n",
       "      <th>pubmed_date</th>\n",
       "      <th>article_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_short</th>\n",
       "      <th>journal_country</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_affils</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>references_pmids</th>\n",
       "      <th>feature</th>\n",
       "      <th>include</th>\n",
       "      <th>mature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pmid, doi, title, abstract, article_date, pubmed_date, article_type, lang, journal, journal_short, journal_country, authors, author_affils, keywords, mesh_terms, references_pmids, feature, include, mature]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled[labelled.isnull().all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e936edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = labelled[['pmid', 'feature']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5e8c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34179 entries, 1 to 172538\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   pmid    34179 non-null  string\n",
      " 1   text    34179 non-null  string\n",
      "dtypes: string(2)\n",
      "memory usage: 801.1+ KB\n"
     ]
    }
   ],
   "source": [
    "selected = selected.rename(columns={\"feature\":\"text\"})\n",
    "selected.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462c141",
   "metadata": {},
   "source": [
    "## Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53d5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = selected.fillna(\"\") #handle NaN values to allow regex over all cells\n",
    "\n",
    "groups_1 = groups.applymap(lambda x:x.lower() if type(x) == str else x) #reduce all to lowercase\n",
    "\n",
    "groups_2 = groups_1.replace(r\"[\\([{})\\]]\", \"\", regex=True) #remove brackets\n",
    "\n",
    "groups_3 = groups_2.replace(\"' \", \"'\", regex=True) #remove quote+space in front of word\n",
    "\n",
    "groups_4 = groups_3.replace(\"\"\"[\\.'!?]\"\"\", \"\", regex=True) #remove punctuation\n",
    "\n",
    "groups = groups_4.replace('\"', \"\", regex=True) #remove double quote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca6231",
   "metadata": {},
   "source": [
    "## Tag Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5552fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = groups[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9106efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CLASSES\n",
    "######################\n",
    "# NEURAL NETWORK / nn\n",
    "# SUPPORT VECTOR MACHINE / svm\n",
    "# STANDARD REGRESSIONS /reg\n",
    "# DECISION TREES / dt\n",
    "# DISCRIMINANT ANALYSIS / da\n",
    "# NAIVE BAYES / nb\n",
    "# K-NEAREST NEIGHBOUR / knn\n",
    "# \n",
    "# TRANSFER LEARNING / tl\n",
    "# FEDERATED LEARNING / fl\n",
    "# UNSUPERVISED LEARNING / unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0681097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 20276, '1': 13903})\n"
     ]
    }
   ],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "## text\n",
    "text = ['neural net', 'deep learning', 'convolutional', 'back propagation', 'lstm', ' cnn']\n",
    "\n",
    "algo['nn_text'] = np.where(groups['text'].str.contains(\"neural net\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['nn_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['nn_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['nn_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6901da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 29687, '1': 4492})\n"
     ]
    }
   ],
   "source": [
    "## SUPPORT VECTOR MACHINE\n",
    "\n",
    "## text\n",
    "text = ['vector machine', 'support vector', 'svm', 'vector regression']\n",
    "\n",
    "algo['svm_text'] = np.where(groups['text'].str.contains(\"support vector machine\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['svm_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['svm_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['svm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a5802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32171, '1': 2008})\n"
     ]
    }
   ],
   "source": [
    "## MULTIVARIABLE REGRESSION\n",
    "\n",
    "## text\n",
    "text = ['logistic regression', 'linear regression', 'multivariable regression', 'multivariate regression',\n",
    "       'simple regression', 'univariate logistic', 'multivariate linear', 'multivariable linear', 'linear model', 'logistic model',\n",
    "        'glm', 'regularized regression', 'ridge regression', 'sparse regression', 'stepwise regression', 'kernel regression',\n",
    "       'process regression']\n",
    "\n",
    "algo['reg_text'] = np.where(groups['text'].str.contains(\"univariate regression\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['reg_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['reg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['reg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e628feb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31365, '1': 2814})\n"
     ]
    }
   ],
   "source": [
    "## DECISION TREE\n",
    "\n",
    "## text\n",
    "text = ['regression tree', 'random forest', 'ensemble tree', 'adaboost', 'xgboost', 'gradient boost']\n",
    "\n",
    "algo['dt_text'] = np.where(groups['text'].str.contains(\"decision tree\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['dt_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['dt_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['dt_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f515e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33575, '1': 604})\n"
     ]
    }
   ],
   "source": [
    "## DISCRIMINANT ANALYSIS\n",
    "\n",
    "## text\n",
    "text = ['discriminant analysis', 'linear discriminant', 'linear discrimination']\n",
    "\n",
    "algo['da_text'] = np.where(groups['text'].str.contains(\"discrimination analysis\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['da_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['da_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['da_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30f5c41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33965, '1': 214})\n"
     ]
    }
   ],
   "source": [
    "## NAIVE BAYES\n",
    "\n",
    "## text\n",
    "text = ['probabilistic classif']\n",
    "\n",
    "algo['nb_text'] = np.where(groups['text'].str.contains(\"naive bayes\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['nb_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['nb_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['nb_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29971592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33372, '1': 807})\n"
     ]
    }
   ],
   "source": [
    "## TRANSFER LEARNING\n",
    "\n",
    "## text\n",
    "algo['tl_text'] = np.where(groups['text'].str.contains(\"transfer learning\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['tl_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d920132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34163, '1': 16})\n"
     ]
    }
   ],
   "source": [
    "## FEDERATED LEARNING\n",
    "\n",
    "## text\n",
    "algo['fl_text'] = np.where(groups['text'].str.contains(\"federated learning\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['fl_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146ea1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33629, '1': 550})\n"
     ]
    }
   ],
   "source": [
    "## K-NEAREST NEIGHBOUR\n",
    "\n",
    "## text\n",
    "algo['knn_text'] = np.where(groups['text'].str.contains(\"k-nearest\"), \"1\", \"0\")\n",
    "algo['knn_text'] = np.where(groups['text'].str.contains(\"k nearest neighbour\"), \"1\", algo['knn_text'])\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['knn_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "621c52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33576, '1': 603})\n"
     ]
    }
   ],
   "source": [
    "## UNSUPERVISED LEARNING\n",
    "\n",
    "## text\n",
    "text = ['k-means', 'means cluster', 'hierarchical cluster', 'unsupervised learning', 'unsupervised algorithm',\n",
    "       'unsupervised model', 'unsupervised method', 'latent class analysis']\n",
    "\n",
    "algo['unsup_text'] = np.where(groups['text'].str.contains(\"clustering algorithm\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['unsup_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['unsup_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['unsup_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41bd3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINE\n",
    "labelled['algo_neural_net'] = np.where(algo['nn_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_support_vector'] = np.where(algo['svm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_regression'] = np.where(algo['reg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_decision_tree'] = np.where(algo['dt_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_discriminant'] = np.where(algo['da_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_naive_bayes'] = np.where(algo['nb_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_transfer'] = np.where(algo['tl_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_federated'] = np.where(algo['fl_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_k_nearest'] = np.where(algo['knn_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_unsupervised'] = np.where(algo['unsup_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "#algo.to_csv('output/algo_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a87f48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_date</th>\n",
       "      <th>pubmed_date</th>\n",
       "      <th>article_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_short</th>\n",
       "      <th>journal_country</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_affils</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>references_pmids</th>\n",
       "      <th>feature</th>\n",
       "      <th>include</th>\n",
       "      <th>mature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104924</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.</td>\n",
       "      <td>Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']</td>\n",
       "      <td>['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']</td>\n",
       "      <td>['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104927</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.</td>\n",
       "      <td>The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']</td>\n",
       "      <td>['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']</td>\n",
       "      <td>['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118652</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States.</td>\n",
       "      <td>Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>NeuroImage</td>\n",
       "      <td>Neuroimage</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']</td>\n",
       "      <td>['Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']</td>\n",
       "      <td>['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34687853</td>\n",
       "      <td>10.1016/j.mri.2021.10.024</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.</td>\n",
       "      <td>To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']</td>\n",
       "      <td>['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']</td>\n",
       "      <td>['Endometrial cancer', 'Radiomic machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34687850</td>\n",
       "      <td>10.1016/j.mri.2021.10.023</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.</td>\n",
       "      <td>Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']</td>\n",
       "      <td>['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']</td>\n",
       "      <td>['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmid                               doi  \\\n",
       "1   34688173  10.1016/j.compbiomed.2021.104924   \n",
       "2   34688172  10.1016/j.compbiomed.2021.104927   \n",
       "8   34687858  10.1016/j.neuroimage.2021.118652   \n",
       "9   34687853         10.1016/j.mri.2021.10.024   \n",
       "10  34687850         10.1016/j.mri.2021.10.023   \n",
       "\n",
       "                                                                                                                                                                              title  \\\n",
       "1                                                             A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.   \n",
       "2                                                                             A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.   \n",
       "8                                                                                                                       Causal Decoding of Individual Cortical Excitability States.   \n",
       "9   Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.   \n",
       "10                                       MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                abstract  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                                                           The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8   Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "\n",
       "   article_date pubmed_date     article_type lang  \\\n",
       "1    2021-10-06  2021-10-24  Journal Article  eng   \n",
       "2    2021-10-11  2021-10-24  Journal Article  eng   \n",
       "8    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "9    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "10   2021-10-20  2021-10-24  Journal Article  eng   \n",
       "\n",
       "                              journal       journal_short journal_country  \\\n",
       "1   Computers in biology and medicine     Comput Biol Med   United States   \n",
       "2   Computers in biology and medicine     Comput Biol Med   United States   \n",
       "8                          NeuroImage          Neuroimage   United States   \n",
       "9          Magnetic resonance imaging  Magn Reson Imaging     Netherlands   \n",
       "10         Magnetic resonance imaging  Magn Reson Imaging     Netherlands   \n",
       "\n",
       "                                                                                                                                                                                                               authors  \\\n",
       "1                                                                                             ['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']   \n",
       "2                                                                                                                                                  ['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']   \n",
       "8                                                                                                                                             ['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']   \n",
       "9   ['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']   \n",
       "10                                                                                                                                          ['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         author_affils  \\\n",
       "1   ['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']   \n",
       "\n",
       "                                                                                                                         keywords  \\\n",
       "1           ['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']   \n",
       "2   ['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']   \n",
       "8                                             ['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']   \n",
       "9                                                                             ['Endometrial cancer', 'Radiomic machine learning']   \n",
       "10                              ['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']   \n",
       "\n",
       "   mesh_terms references_pmids  \\\n",
       "1        <NA>             <NA>   \n",
       "2        <NA>             <NA>   \n",
       "8        <NA>             <NA>   \n",
       "9        <NA>             <NA>   \n",
       "10       <NA>             <NA>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                 A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8   Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "\n",
       "   include mature algo_neural_net algo_support_vector algo_regression  \\\n",
       "1      1.0    1.0               1                   0               0   \n",
       "2      1.0    0.0               0                   0               0   \n",
       "8      1.0    0.0               0                   0               0   \n",
       "9      1.0    1.0               0                   0               0   \n",
       "10     1.0    0.0               0                   0               0   \n",
       "\n",
       "   algo_decision_tree algo_discriminant algo_naive_bayes algo_transfer  \\\n",
       "1                   0                 0                0             0   \n",
       "2                   0                 0                0             1   \n",
       "8                   0                 0                0             0   \n",
       "9                   0                 0                0             0   \n",
       "10                  0                 0                0             0   \n",
       "\n",
       "   algo_federated algo_k_nearest algo_unsupervised  \n",
       "1               0              0                 0  \n",
       "2               0              0                 0  \n",
       "8               0              0                 0  \n",
       "9               0              0                 0  \n",
       "10              0              0                 0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27a66f",
   "metadata": {},
   "source": [
    "## Tag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dd88371",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = groups[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66f82d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CLASSES\n",
    "######################\n",
    "# BIO_MARKER / bio\n",
    "# GENOMIC / gene\n",
    "# IMAGING / imaging\n",
    "    ### XR / xr\n",
    "    ### CT / ct\n",
    "    ### MRI / mri\n",
    "# ECHO / echo\n",
    "# US / us\n",
    "# MAMMOGRAM / mamm\n",
    "# OCT / oct\n",
    "# EEG / eeg\n",
    "# ECG / ecg\n",
    "# EMG / emg\n",
    "# DERMASCOPY / derm\n",
    "# CELLULAR_PATH / histo\n",
    "# ENDOSCOPY / endo\n",
    "#\n",
    "# NATURAL_LANGUAGE / nlp\n",
    "# EHR RECORDS / ehr\n",
    "#\n",
    "# WEARABLE_SENSORS / sensor\n",
    "# SMARTPHONE / phone\n",
    "# PATIENT REPORTED / prom\n",
    "# DIGITAL STETH / sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ccefab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32828, '1': 1351})\n"
     ]
    }
   ],
   "source": [
    "## XR\n",
    "\n",
    "## text\n",
    "text = ['xr', 'x-ray', 'radiograph']\n",
    "\n",
    "feat['xr_text'] = np.where(groups['text'].str.contains(\"cxr\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['xr_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['xr_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['xr_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd75e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31198, '1': 2981})\n"
     ]
    }
   ],
   "source": [
    "## CT\n",
    "\n",
    "## text\n",
    "text = ['computed tomograph', 'axial tomograph', 'ct scan', 'ct image', 'ct slice', ' ct ', ' ct-',\n",
    "       'tomography scan', 'computerised tomograph', 'computerized tomograph', 'assisted tomograph']\n",
    "\n",
    "feat['ct_text'] = np.where(groups['text'].str.contains(\"cat scan\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['ct_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['ct_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##exclude\n",
    "\n",
    "feat['ct_text'] = np.where(groups['text'].str.contains(\"optical coherence\"), \"0\", feat['ct_text']) #exclude oct\n",
    "feat['ct_text'] = np.where(groups['text'].str.contains(\"coherence tomograph\"), \"0\", feat['ct_text']) #exclude oct\n",
    "        \n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['ct_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3b9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 30726, '1': 3453})\n"
     ]
    }
   ],
   "source": [
    "## MRI\n",
    "\n",
    "## text\n",
    "text = ['magnetic resonance']\n",
    "\n",
    "feat['mri_text'] = np.where(groups['text'].str.contains(\" mri\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['mri_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['mri_text'] ) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['mri_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d00d6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33988, '1': 191})\n"
     ]
    }
   ],
   "source": [
    "## ECHO\n",
    "\n",
    "## text\n",
    "text = ['echo-cardio', 'echokardio', 'cardiac echo']\n",
    "\n",
    "feat['echo_text'] = np.where(groups['text'].str.contains(\"echocardio\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['echo_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['echo_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['echo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1af88f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33003, '1': 1176})\n"
     ]
    }
   ],
   "source": [
    "## US\n",
    "\n",
    "## text\n",
    "text = ['sonography', 'ultra-sound', 'ultrasonograph', 'doppler']\n",
    "\n",
    "feat['us_text'] = np.where(groups['text'].str.contains(\"ultrasound\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['us_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['us_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['us_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e10bf909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33072, '1': 1107})\n"
     ]
    }
   ],
   "source": [
    "## ECG\n",
    "\n",
    "## text\n",
    "text = [' ecg', ' ekg', 'electrokardio', 'electro-cardio', 'holter monitor', 'cardiac monitor']\n",
    "\n",
    "feat['ecg_text'] = np.where(groups['text'].str.contains(\"electrocardio\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['ecg_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['ecg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['ecg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf6f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32309, '1': 1870})\n"
     ]
    }
   ],
   "source": [
    "## EEG\n",
    "\n",
    "## text\n",
    "text = [' eeg']\n",
    "\n",
    "feat['eeg_text'] = np.where(groups['text'].str.contains(\"electroenc\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['eeg_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['eeg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['eeg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "566f93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33656, '1': 523})\n"
     ]
    }
   ],
   "source": [
    "## EMG\n",
    "\n",
    "## text\n",
    "text = ['myoelectric', 'electro-myo']\n",
    "\n",
    "feat['emg_text'] = np.where(groups['text'].str.contains(\"electromyo\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['emg_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['emg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['emg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "170697e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129872</th>\n",
       "      <td>hand motion classification using a multi-channel surface electromyography sensor the human hand has multiple degrees of freedom dof for achieving high-dexterity motions identifying and replicating human hand motions are necessary to perform precise and delicate operations in many applications, such as haptic applications surface electromyography semg sensors are a low-cost method for identifying hand motions, in addition to the conventional methods that use data gloves and vision detection the identification of multiple hand motions is challenging because the error rate typically increases significantly with the addition of more hand motions thus, the current study proposes two new methods for feature extraction to solve the problem above the first method is the extraction of the energy ratio features in the time-domain, which are robust and invariant to motion forces and speeds for the same gesture the second method is the extraction of the concordance correlation features that describe the relationship between every two channels of the multi-channel semg sensor system the concordance correlation features of a multi-channel semg sensor system were shown to provide a vast amount of useful information for identification furthermore, a new cascaded-structure classifier is also proposed, in which 11 types of hand gestures can be identified accurately using the newly defined features experimental results show that the success rate for the identification of the 11 gestures is significantly high</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105758</th>\n",
       "      <td>a robust myoelectric pattern recognition using online sequential extreme learning machine for finger movement classification a robust myoelectric pattern-recognition-system requires a system that should work in the real application as good as in the laboratory however, this demand should be handled properly and rigorously to achieve a robust myoelectric system electrode shift is an issue that usually emerges when dealing with robustness issue in daily life, the placement of electrodes becomes a significant issue that can downgrade the performance of the system this paper proposed a new way to overcome the robustness issue by conducting an update to the system to anticipate changes in the future such as electrode shift, improvement in muscle strength or any other issue such update will be used to generate an adaptation the adaptation is done according to the users need by employing an online sequential extreme learning os-elm to learn the training data chunk by chunk os-elm enables the myoelectric system to learn from a small number of data to avoid cumbersome training process the day-to-day experiment shows that the proposed system can maintain its performance on average accuracy around 85% whereas the non-adaptive system could not</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113264</th>\n",
       "      <td>characterization of a benchmark database for myoelectric movement classification in this paper, we characterize the ninapro database and its use as a benchmark for hand prosthesis evaluation the database is a publicly available resource that aims to support research on advanced myoelectric hand prostheses the database is obtained by jointly recording surface electromyography signals from the forearm and kinematics of the hand and wrist while subjects perform a predefined set of actions and postures besides describing the acquisition protocol, overall features of the datasets and the processing procedures in detail, we present benchmark classification results using a variety of feature representations and classifiers our comparison shows that simple feature representations such as mean absolute value and waveform length can achieve similar performance to the computationally more demanding marginal discrete wavelet transform with respect to classification methods, the nonlinear support vector machine was found to be the only method consistently achieving high performance regardless of the type of feature representation furthermore, statistical analysis of these results shows that classification accuracy is negatively correlated with the subjects body mass index the analysis and the results described in this paper aim to be a strong baseline for the ninapro database thanks to the ninapro database and the characterization described in this paper, the scientific community has the opportunity to converge to a common position on hand movement recognition by surface electromyography, a field capable to strongly affect hand prosthesis capabilities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141421</th>\n",
       "      <td>the application of machine learning algorithms to the analysis of electromyographic patterns from arthritic patients the main aim of our study was to investigate the possibility of applying machine learning techniques to the analysis of electromyographic patterns emg collected from arthritic patients during gait the emg recordings were collected from the lower limbs of patients with arthritis and compared with those of healthy subjects co with no musculoskeletal disorder the study involved subjects suffering from two forms of arthritis, viz, rheumatoid arthritis ra and hip osteoarthritis oa the analysis of the data was plagued by two problems which frequently render the analysis of this type of data extremely difficult one was the small number of human subjects that could be included in the investigation based on the terms specified in the inclusion and exclusion criteria for the study the other was the high intra- and inter-subject variability present in emg data we identified some of the muscles differently employed by the arthritic patients by using machine learning techniques to classify the two groups and then identified the muscles that were critical for the classification for the classification we employed least-squares kernel lsk algorithms, neural network algorithms like the kohonen self organizing map, learning vector quantification and the multilayer perceptron finally we also tested the more classical technique of linear discriminant analysis lda the performance of the different algorithms was compared the lsk algorithm showed the highest capacity for classification our study demonstrates that the newly developed lsk algorithm is adept for the treatment of biological data the muscles that were most important for distinguishing the ra from the co subjects were the soleus and biceps femoris for separating the oa and co subjects however, it was the gluteus medialis muscle our study demonstrates how classification with emg data can be used in the clinical setting while such procedures are unnecessary for the diagnosis of the type of arthritis present, an understanding of the muscles which are responsible for the classification can help to better identify targets for rehabilitative measures</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169300</th>\n",
       "      <td>on automatic identification of upper-limb movements using small-sized training sets of emg signals we evaluate the performance of a variety of neural and fuzzy networks for discrimination among three planar arm-pointing movements by means of electromyographic emg signals, when learning is based on small-sized training sets the aim of this work is to underline the importance that the sparse data problem has in designing pattern classifiers with good generalisation properties the results indicate that one of the proposed fuzzy networks is more robust than the other classifiers when working with small training sets</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "129872                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    hand motion classification using a multi-channel surface electromyography sensor the human hand has multiple degrees of freedom dof for achieving high-dexterity motions identifying and replicating human hand motions are necessary to perform precise and delicate operations in many applications, such as haptic applications surface electromyography semg sensors are a low-cost method for identifying hand motions, in addition to the conventional methods that use data gloves and vision detection the identification of multiple hand motions is challenging because the error rate typically increases significantly with the addition of more hand motions thus, the current study proposes two new methods for feature extraction to solve the problem above the first method is the extraction of the energy ratio features in the time-domain, which are robust and invariant to motion forces and speeds for the same gesture the second method is the extraction of the concordance correlation features that describe the relationship between every two channels of the multi-channel semg sensor system the concordance correlation features of a multi-channel semg sensor system were shown to provide a vast amount of useful information for identification furthermore, a new cascaded-structure classifier is also proposed, in which 11 types of hand gestures can be identified accurately using the newly defined features experimental results show that the success rate for the identification of the 11 gestures is significantly high   \n",
       "105758                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          a robust myoelectric pattern recognition using online sequential extreme learning machine for finger movement classification a robust myoelectric pattern-recognition-system requires a system that should work in the real application as good as in the laboratory however, this demand should be handled properly and rigorously to achieve a robust myoelectric system electrode shift is an issue that usually emerges when dealing with robustness issue in daily life, the placement of electrodes becomes a significant issue that can downgrade the performance of the system this paper proposed a new way to overcome the robustness issue by conducting an update to the system to anticipate changes in the future such as electrode shift, improvement in muscle strength or any other issue such update will be used to generate an adaptation the adaptation is done according to the users need by employing an online sequential extreme learning os-elm to learn the training data chunk by chunk os-elm enables the myoelectric system to learn from a small number of data to avoid cumbersome training process the day-to-day experiment shows that the proposed system can maintain its performance on average accuracy around 85% whereas the non-adaptive system could not    \n",
       "113264                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           characterization of a benchmark database for myoelectric movement classification in this paper, we characterize the ninapro database and its use as a benchmark for hand prosthesis evaluation the database is a publicly available resource that aims to support research on advanced myoelectric hand prostheses the database is obtained by jointly recording surface electromyography signals from the forearm and kinematics of the hand and wrist while subjects perform a predefined set of actions and postures besides describing the acquisition protocol, overall features of the datasets and the processing procedures in detail, we present benchmark classification results using a variety of feature representations and classifiers our comparison shows that simple feature representations such as mean absolute value and waveform length can achieve similar performance to the computationally more demanding marginal discrete wavelet transform with respect to classification methods, the nonlinear support vector machine was found to be the only method consistently achieving high performance regardless of the type of feature representation furthermore, statistical analysis of these results shows that classification accuracy is negatively correlated with the subjects body mass index the analysis and the results described in this paper aim to be a strong baseline for the ninapro database thanks to the ninapro database and the characterization described in this paper, the scientific community has the opportunity to converge to a common position on hand movement recognition by surface electromyography, a field capable to strongly affect hand prosthesis capabilities    \n",
       "141421  the application of machine learning algorithms to the analysis of electromyographic patterns from arthritic patients the main aim of our study was to investigate the possibility of applying machine learning techniques to the analysis of electromyographic patterns emg collected from arthritic patients during gait the emg recordings were collected from the lower limbs of patients with arthritis and compared with those of healthy subjects co with no musculoskeletal disorder the study involved subjects suffering from two forms of arthritis, viz, rheumatoid arthritis ra and hip osteoarthritis oa the analysis of the data was plagued by two problems which frequently render the analysis of this type of data extremely difficult one was the small number of human subjects that could be included in the investigation based on the terms specified in the inclusion and exclusion criteria for the study the other was the high intra- and inter-subject variability present in emg data we identified some of the muscles differently employed by the arthritic patients by using machine learning techniques to classify the two groups and then identified the muscles that were critical for the classification for the classification we employed least-squares kernel lsk algorithms, neural network algorithms like the kohonen self organizing map, learning vector quantification and the multilayer perceptron finally we also tested the more classical technique of linear discriminant analysis lda the performance of the different algorithms was compared the lsk algorithm showed the highest capacity for classification our study demonstrates that the newly developed lsk algorithm is adept for the treatment of biological data the muscles that were most important for distinguishing the ra from the co subjects were the soleus and biceps femoris for separating the oa and co subjects however, it was the gluteus medialis muscle our study demonstrates how classification with emg data can be used in the clinical setting while such procedures are unnecessary for the diagnosis of the type of arthritis present, an understanding of the muscles which are responsible for the classification can help to better identify targets for rehabilitative measures   \n",
       "169300                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   on automatic identification of upper-limb movements using small-sized training sets of emg signals we evaluate the performance of a variety of neural and fuzzy networks for discrimination among three planar arm-pointing movements by means of electromyographic emg signals, when learning is based on small-sized training sets the aim of this work is to underline the importance that the sparse data problem has in designing pattern classifiers with good generalisation properties the results indicate that one of the proposed fuzzy networks is more robust than the other classifiers when working with small training sets   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \n",
       "129872       0       0        0         0       0        0        0        1  \n",
       "105758       0       0        0         0       0        0        0        1  \n",
       "113264       0       0        0         0       0        0        0        1  \n",
       "141421       0       0        0         0       0        0        0        1  \n",
       "169300       0       0        0         0       0        0        0        1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['emg_text']==\"1\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50c69805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32009, '1': 2170})\n"
     ]
    }
   ],
   "source": [
    "## CELLULAR PATHOLOGY\n",
    "\n",
    "## text\n",
    "text = ['histopath', 'histology', 'histochem', 'immunohist', 'cytolog', 'cytochem', 'cellular path', 'microscopy',\n",
    "       'smear', 'cytometry', 'hematoxylin', 'specimens', 'stain', 'tissue sample', 'tissue section', 'brushing']\n",
    "\n",
    "feat['histo_text'] = np.where(groups['text'].str.contains(\"histologic\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['histo_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['histo_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['histo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f53163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33245, '1': 934})\n"
     ]
    }
   ],
   "source": [
    "## OCT / retinal\n",
    "\n",
    "## text\n",
    "text = ['coherence tomog', ' oct ', 'retinal photo', 'retinal imag', 'retinal tomograph',\n",
    "        'laser ophth', 'fundus imag', 'fundus phot', 'fundal imag', 'fundal phot']\n",
    "\n",
    "feat['oct_text'] = np.where(groups['text'].str.contains(\"optical coherence\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['oct_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['oct_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output    \n",
    "print('text counts:')\n",
    "print(Counter(feat['oct_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "615c8ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128682</th>\n",
       "      <td>sensitivity and specificity of machine learning classifiers and spectral domain oct for the diagnosis of glaucoma purpose to investigate the sensitivity and specificity of machine learning classifiers mlc and spectral domain optical coherence tomography sd-oct for the diagnosis of glaucoma methods sixty-two patients with early to moderate glaucomatous visual field damage and 48 healthy individuals were included all subjects underwent a complete ophthalmologic examination, achromatic standard automated perimetry, and rnfl imaging with sd-oct cirrus hd-oct; carl zeiss meditec, inc, dublin, california, usa receiver operating characteristic roc curves were obtained for all sd-oct parameters subsequently, the following mlcs were tested: classification tree ctree, random forest ran, bagging bag, adaboost m1 ada, ensemble selection ens, multilayer perceptron mlp, radial basis function rbf, naive-bayes nb, and support vector machine svm areas under the roc curves arocs obtained for each parameter and each mlc were compared results the mean age was 570±92 years for healthy individuals and 599±90 years for glaucoma patients p=0103 mean deviation values were -41±24 db for glaucoma patients and -15±16 db for healthy individuals p&lt;0001 the sd-oct parameters with the greater arocs were inferior quadrant 0813, average thickness 0807, 7 oclock position 0765, and 6 oclock position 0754 the arocs from classifiers varied from 0785 ada to 0818 bag the aroc obtained with bag was not significantly different from the aroc obtained with the best single sd-oct parameter p=093 conclusions the sd-oct showed good diagnostic accuracy in a group of patients with early glaucoma in this series, mlcs did not improve the sensitivity and specificity of sd-oct for the diagnosis of glaucoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82803</th>\n",
       "      <td>comparison of machine-learning classification models for glaucoma management this study develops an objective machine-learning classification model for classifying glaucomatous optic discs and reveals the classificatory criteria to assist in clinical glaucoma management in this study, 163 glaucoma eyes were labelled with four optic disc types by three glaucoma specialists and then randomly separated into training and test data all the images of these eyes were captured using optical coherence tomography and laser speckle flowgraphy to quantify the ocular structure and blood-flow-related parameters a total of 91 parameters were extracted from each eye along with the patientsbackground information machine-learning classifiers, including the neural network nn, naïve bayes nb, support vector machine svm, and gradient boosted decision trees gbdt, were trained to build the classification models, and a hybrid feature selection method that combines minimum redundancy maximum relevance and genetic-algorithm-based feature selection was applied to find the most valid and relevant features for nn, nb, and svm a comparison of the performance of the three machine-learning classification models showed that the nn had the best classification performance with a validated accuracy of 878% using only nine ocular parameters these selected quantified parameters enabled the trained nn to classify glaucomatous optic discs with relatively high performance without requiring color fundus images</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89683</th>\n",
       "      <td>leveraging uncertainty information from deep neural networks for disease detection deep learning dl has revolutionized the field of computer vision and image processing in medical imaging, algorithmic solutions based on dl have been shown to achieve high performance on tasks that previously required medical experts however, dl-based solutions for disease detection have been proposed without methods to quantify and control their uncertainty in a decision in contrast, a physician knows whether she is uncertain about a case and will consult more experienced colleagues if needed here we evaluate drop-out based bayesian uncertainty measures for dl in diagnosing diabetic retinopathy dr from fundus images and show that it captures uncertainty better than straightforward alternatives furthermore, we show that uncertainty informed decision referral can improve diagnostic performance experiments across different networks, tasks and datasets show robust generalization depending on network capacity and task/dataset difficulty, we surpass 85% sensitivity and 80% specificity as recommended by the nhs when referring 0-20% of the most uncertain decisions for further inspection we analyse causes of uncertainty by relating intuitions from 2d visualizations to the high-dimensional image space while uncertainty is sensitive to clinically relevant cases, sensitivity to unfamiliar data samples is task dependent, but can be rendered more robust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61024</th>\n",
       "      <td>clinical interpretable deep learning model for glaucoma diagnosis despite the potential to revolutionise disease diagnosis by performing data-driven classification, clinical interpretability of convnet remains challenging in this paper, a novel clinical interpretable convnet architecture is proposed not only for accurate glaucoma diagnosis but also for the more transparent interpretation by highlighting the distinct regions recognised by the network to the best of our knowledge, this is the first work of providing the interpretable diagnosis of glaucoma with the popular deep learning model we propose a novel scheme for aggregating features from different scales to promote the performance of glaucoma diagnosis, which we refer to as m-lap moreover, by modelling the correspondence from binary diagnosis information to the spatial pixels, the proposed scheme generates glaucoma activations, which bridge the gap between global semantical diagnosis and precise location in contrast to previous works, it can discover the distinguish local regions in fundus images as evidence for clinical interpretable glaucoma diagnosis experimental results, performed on the challenging origa datasets, show that our method on glaucoma diagnosis outperforms state-of-the-art methods with the highest auc 088 remarkably, the extensive results, optic disc segmentation dice of 09 and local disease focus localization based on the evidence map, demonstrate the effectiveness of our methods on clinical interpretability</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41599</th>\n",
       "      <td>automated identification of retinopathy of prematurity by image-based deep learning retinopathy of prematurity rop is a leading cause of childhood blindness worldwide but can be a treatable retinal disease with appropriate and timely diagnosis this study was performed to develop a robust intelligent system based on deep learning to automatically classify the severity of rop from fundus images and detect the stage of rop and presence of plus disease to enable automated diagnosis and further treatment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29265</th>\n",
       "      <td>automated quantitative assessment of retinal fluid volumes as important biomarkers in neovascular age-related macular degeneration to evaluate retinal fluid volume data extracted from optical coherence tomography oct scans by artificial intelligence algorithms in the treatment of neovascular age-related macular degeneration nv-amd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>multicolor image classification using the multimodal information bottleneck network mmib-net for detecting diabetic retinopathy multicolor mc imaging is an imaging modality that records confocal scanning laser ophthalmoscope cslo fundus images, which can be used for the diabetic retinopathy dr detection by utilizing this imaging technique, multiple modal images can be obtained in a single case additional symptomatic features can be obtained if these images are considered during the diagnosis of dr however, few studies have been carried out to classify mc images using deep learning methods, let alone using multi modal features for analysis in this work, we propose a novel model which uses the multimodal information bottleneck network mmib-net to classify the mc images for the detection of dr our model can extract the features of multiple modalities simultaneously while finding concise feature representations of each modality using the information bottleneck theory mc images classification can be achieved by picking up the combined representations and features of all modalities in our experiments, it is shown that the proposed method can achieve an accurate classification of mc images comparative experiments also demonstrate that the use of multimodality and information bottleneck improves the performance of mc images classification to the best of our knowledge, this is the first report of dr identification utilizing the multimodal information bottleneck convolutional neural network in mc images</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51496</th>\n",
       "      <td>deep learning segmentation for optical coherence tomography measurements of the lower tear meniscus the tear meniscus contains most of the tear fluid and therefore is a good indicator for the state of the tear film previously, we used a custom-built optical coherence tomography oct system to study the lower tear meniscus by automatically segmenting the image data with a thresholding-based segmentation algorithm tbsa in this report, we investigate whether the results of this image segmentation algorithm are suitable to train a neural network in order to obtain similar or better segmentation results with shorter processing times considering the class imbalance problem, we compare two approaches, one directly segmenting the tear meniscus dsa, the other first localizing the region of interest and then segmenting within the higher resolution image section lsa a total of 6658 images labeled by the tbsa were used to train deep convolutional neural networks with supervised learning five-fold cross-validation reveals a sensitivity of 9636% and 9643%, a specificity of 9998% and 9986% and a jaccard index of 9324% and 9316% for the dsa and lsa, respectively average segmentation times are up to 228 times faster than the tbsa additionally, we report the behavior of the dsa and lsa in cases challenging for the tbsa and further test the applicability to measurements acquired with a commercially available oct system the application of deep learning for the segmentation of the tear meniscus provides a powerful tool for the assessment of the tear film, supporting studies for the investigation of the pathophysiology of dry eye-related diseases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71646</th>\n",
       "      <td>weakly supervised lesion localization for age-related macular degeneration detection using optical coherence tomography images age-related macular degeneration amd is the main cause of irreversible blindness among the elderly and require early diagnosis to prevent vision loss, and careful treatment is essential optical coherence tomography oct, the most commonly used imaging method in the retinal area for the diagnosis of amd, is usually interpreted by a clinician, and oct can help diagnose disease on the basis of the relevant diagnostic criteria, but these judgments can be somewhat subjective we propose an algorithm for the detection of amd based on a weakly supervised convolutional neural network cnn model to support computer-aided diagnosis cad system our main contributions are the following three things 1 we propose a concise cnn model for oct images, which outperforms the existing large cnn models using vgg16 and googlenet architectures 2 we propose an algorithm called expressive gradients eg that extends the existing integrated gradients ig algorithm so as to exploit not only the input-level attribution map, but also the high-level attribution maps due to enriched gradients, eg can highlight suspicious regions for diagnosis of amd better than the guided-backpropagation method and ig 3 our method provides two visualization options: overlay and top-k bounding boxes, which would be useful for cad through experimental evaluation using 10,100 clinical oct images from amd patients, we demonstrate that our eg algorithm outperforms the ig algorithm in terms of localization accuracy and also outperforms the existing object detection methods in terms of class accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125116</th>\n",
       "      <td>superpixel classification based optic disc and optic cup segmentation for glaucoma screening glaucoma is a chronic eye disease that leads to vision loss as it cannot be cured, detecting the disease in time is important current tests using intraocular pressure iop are not sensitive enough for population based glaucoma screening optic nerve head assessment in retinal fundus images is both more promising and superior this paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening in optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc a self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation for optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance the proposed segmentation methods have been evaluated in a database of 650 images with optic disc and optic cup boundaries manually marked by trained professionals experimental results show an average overlapping error of 95% and 241% in optic disc and optic cup segmentation, respectively the results also show an increase in overlapping error as the reliability score is reduced, which justifies the effectiveness of the self-assessment the segmented optic disc and optic cup are then used to compute the cup to disc ratio for glaucoma screening our proposed method achieves areas under curve of 0800 and 0822 in two data sets, which is higher than other methods the methods can be used for segmentation and glaucoma screening the self-assessment will be used as an indicator of cases with large errors and enhance the clinical deployment of the automatic segmentation and screening</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8084</th>\n",
       "      <td>multimodal machine learning using visual fields and peripapillary circular oct scans in detection of glaucomatous optic neuropathy to develop and validate a multimodal artificial intelligence algorithm, fusionnet, using the pattern deviation probability plots from visual field vf reports and circular peripapillary oct scans to detect glaucomatous optic neuropathy gon</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36192</th>\n",
       "      <td>detection of morphologic patterns of diabetic macular edema using a deep learning approach based on optical coherence tomography images to develop a deep learning dl model to detect morphologic patterns of diabetic macular edema dme based on optical coherence tomography oct images</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130986</th>\n",
       "      <td>mrmr optimized classification for automatic glaucoma diagnosis min-redundancy max-relevance mrmr is a feature selection methodology based on information theory we explore the mrmr principle for automatic glaucoma diagnosis optimal candidate feature sets are acquired from a composition of clinical screening data and retinal fundus image data an mrmr optimized classifier is further trained using the candidate feature sets to find the optimized classifier we tested the proposed methodology on eye records of 650 subjects collected from singapore eye research institute the experimental results demonstrate that the new classifier is much compact by using less than ¼ of the initial feature set the ranked feature set also enables the clinicians to better access the diagnostic process of the algorithm the work is a further step towards the advancement of the automatic glaucoma diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79763</th>\n",
       "      <td>optical coherence tomography biomarkers to distinguish diabetic macular edema from pseudophakic cystoid macular edema using machine learning algorithms in diabetic patients presenting with macular edema me shortly after cataract surgery, identifying the underlying pathology can be challenging and influence management our aim was to develop a simple clinical classifier able to confirm a diabetic etiology using few spectral domain optical coherence tomography parameters</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40697</th>\n",
       "      <td>artificial intelligence mapping of structure to function in glaucoma to develop an artificial intelligence ai-based structure-function sf map relating retinal nerve fiber layer rnfl damage on spectral domain optical coherence tomography sdoct to functional loss on standard automated perimetry sap</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59898</th>\n",
       "      <td>detection of optical coherence tomography-defined thin-cap fibroatheroma in the coronary artery using deep learning the aim of this study was to develop a deep learning model for classifying frames with versus without optical coherence tomography oct-derived thin-cap fibroatheroma tcfa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26150</th>\n",
       "      <td>predicting incremental and future visual change in neovascular age-related macular degeneration using deep learning to evaluate the predictive usefulness of quantitative imaging biomarkers, acquired automatically from oct scans, of cross-sectional and future visual outcomes of patients with neovascular age-related macular degeneration amd starting anti-vascular endothelial growth factor vegf therapy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79403</th>\n",
       "      <td>unsupervised identification of disease marker candidates in retinal oct imaging data the identification and quantification of markers in medical images is critical for diagnosis, prognosis, and disease management supervised machine learning enables the detection and exploitation of findings that are known a priori after annotation of training examples by experts however, supervision does not scale well, due to the amount of necessary training examples, and the limitation of the marker vocabulary to known entities in this proof-of-concept study, we propose unsupervised identification of anomalies as candidates for markers in retinal optical coherence tomography oct imaging data without a constraint to a priori definitions we identify and categorize marker candidates occurring frequently in the data and demonstrate that these markers show a predictive value in the task of detecting disease a careful qualitative analysis of the identified data driven markers reveals how their quantifiable occurrence aligns with our current understanding of disease course, in early- and late age-related macular degeneration amd patients a multi-scale deep denoising autoencoder is trained on healthy images, and a one-class support vector machine identifies anomalies in new data clustering in the anomalies identifies stable categories using these markers to classify healthy-, early amd- and late amd cases yields an accuracy of 8140% in a second binary classification experiment on a publicly available data set healthy versus intermediate amd, the model achieves an area under the roc curve of 0944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80177</th>\n",
       "      <td>an automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color fundus photographs the goal of this study was to describe the development and validation of an artificial intelligence-based, deep learning algorithm dla for the detection of referable diabetic retinopathy dr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78333</th>\n",
       "      <td>exudate detection in fundus images using deeply-learnable features presence of exudates on a retina is an early sign of diabetic retinopathy, and automatic detection of these can improve the diagnosis of the disease convolutional neural networks cnns have been used for automatic exudate detection, but with poor performance this study has investigated different deep learning techniques to maximize the sensitivity and specificity we have compared multiple deep learning methods, and both supervised and unsupervised classifiers for improving the performance of automatic exudate detection, ie, cnns, pre-trained residual networks resnet-50 and discriminative restricted boltzmann machines the experiments were conducted on two publicly available databases: i diaretdb1 and ii e-ophtha the results show that resnet-50 with support vector machines outperformed other networks with an accuracy and sensitivity of 98% and 099, respectively this shows that resnet-50 can be used for the analysis of the fundus images to detect exudates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "128682                                                      sensitivity and specificity of machine learning classifiers and spectral domain oct for the diagnosis of glaucoma purpose to investigate the sensitivity and specificity of machine learning classifiers mlc and spectral domain optical coherence tomography sd-oct for the diagnosis of glaucoma methods sixty-two patients with early to moderate glaucomatous visual field damage and 48 healthy individuals were included all subjects underwent a complete ophthalmologic examination, achromatic standard automated perimetry, and rnfl imaging with sd-oct cirrus hd-oct; carl zeiss meditec, inc, dublin, california, usa receiver operating characteristic roc curves were obtained for all sd-oct parameters subsequently, the following mlcs were tested: classification tree ctree, random forest ran, bagging bag, adaboost m1 ada, ensemble selection ens, multilayer perceptron mlp, radial basis function rbf, naive-bayes nb, and support vector machine svm areas under the roc curves arocs obtained for each parameter and each mlc were compared results the mean age was 570±92 years for healthy individuals and 599±90 years for glaucoma patients p=0103 mean deviation values were -41±24 db for glaucoma patients and -15±16 db for healthy individuals p<0001 the sd-oct parameters with the greater arocs were inferior quadrant 0813, average thickness 0807, 7 oclock position 0765, and 6 oclock position 0754 the arocs from classifiers varied from 0785 ada to 0818 bag the aroc obtained with bag was not significantly different from the aroc obtained with the best single sd-oct parameter p=093 conclusions the sd-oct showed good diagnostic accuracy in a group of patients with early glaucoma in this series, mlcs did not improve the sensitivity and specificity of sd-oct for the diagnosis of glaucoma   \n",
       "82803                                                                                                                                                                                                                                                                                                                                                          comparison of machine-learning classification models for glaucoma management this study develops an objective machine-learning classification model for classifying glaucomatous optic discs and reveals the classificatory criteria to assist in clinical glaucoma management in this study, 163 glaucoma eyes were labelled with four optic disc types by three glaucoma specialists and then randomly separated into training and test data all the images of these eyes were captured using optical coherence tomography and laser speckle flowgraphy to quantify the ocular structure and blood-flow-related parameters a total of 91 parameters were extracted from each eye along with the patientsbackground information machine-learning classifiers, including the neural network nn, naïve bayes nb, support vector machine svm, and gradient boosted decision trees gbdt, were trained to build the classification models, and a hybrid feature selection method that combines minimum redundancy maximum relevance and genetic-algorithm-based feature selection was applied to find the most valid and relevant features for nn, nb, and svm a comparison of the performance of the three machine-learning classification models showed that the nn had the best classification performance with a validated accuracy of 878% using only nine ocular parameters these selected quantified parameters enabled the trained nn to classify glaucomatous optic discs with relatively high performance without requiring color fundus images   \n",
       "89683                                                                                                                                                                                                                                                                                                                                                                                                          leveraging uncertainty information from deep neural networks for disease detection deep learning dl has revolutionized the field of computer vision and image processing in medical imaging, algorithmic solutions based on dl have been shown to achieve high performance on tasks that previously required medical experts however, dl-based solutions for disease detection have been proposed without methods to quantify and control their uncertainty in a decision in contrast, a physician knows whether she is uncertain about a case and will consult more experienced colleagues if needed here we evaluate drop-out based bayesian uncertainty measures for dl in diagnosing diabetic retinopathy dr from fundus images and show that it captures uncertainty better than straightforward alternatives furthermore, we show that uncertainty informed decision referral can improve diagnostic performance experiments across different networks, tasks and datasets show robust generalization depending on network capacity and task/dataset difficulty, we surpass 85% sensitivity and 80% specificity as recommended by the nhs when referring 0-20% of the most uncertain decisions for further inspection we analyse causes of uncertainty by relating intuitions from 2d visualizations to the high-dimensional image space while uncertainty is sensitive to clinically relevant cases, sensitivity to unfamiliar data samples is task dependent, but can be rendered more robust   \n",
       "61024                                                                                                                                                                                                                                                                                                                                            clinical interpretable deep learning model for glaucoma diagnosis despite the potential to revolutionise disease diagnosis by performing data-driven classification, clinical interpretability of convnet remains challenging in this paper, a novel clinical interpretable convnet architecture is proposed not only for accurate glaucoma diagnosis but also for the more transparent interpretation by highlighting the distinct regions recognised by the network to the best of our knowledge, this is the first work of providing the interpretable diagnosis of glaucoma with the popular deep learning model we propose a novel scheme for aggregating features from different scales to promote the performance of glaucoma diagnosis, which we refer to as m-lap moreover, by modelling the correspondence from binary diagnosis information to the spatial pixels, the proposed scheme generates glaucoma activations, which bridge the gap between global semantical diagnosis and precise location in contrast to previous works, it can discover the distinguish local regions in fundus images as evidence for clinical interpretable glaucoma diagnosis experimental results, performed on the challenging origa datasets, show that our method on glaucoma diagnosis outperforms state-of-the-art methods with the highest auc 088 remarkably, the extensive results, optic disc segmentation dice of 09 and local disease focus localization based on the evidence map, demonstrate the effectiveness of our methods on clinical interpretability   \n",
       "41599                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       automated identification of retinopathy of prematurity by image-based deep learning retinopathy of prematurity rop is a leading cause of childhood blindness worldwide but can be a treatable retinal disease with appropriate and timely diagnosis this study was performed to develop a robust intelligent system based on deep learning to automatically classify the severity of rop from fundus images and detect the stage of rop and presence of plus disease to enable automated diagnosis and further treatment   \n",
       "29265                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   automated quantitative assessment of retinal fluid volumes as important biomarkers in neovascular age-related macular degeneration to evaluate retinal fluid volume data extracted from optical coherence tomography oct scans by artificial intelligence algorithms in the treatment of neovascular age-related macular degeneration nv-amd   \n",
       "9743                                                                                                                                                                                                                                                                                                                                  multicolor image classification using the multimodal information bottleneck network mmib-net for detecting diabetic retinopathy multicolor mc imaging is an imaging modality that records confocal scanning laser ophthalmoscope cslo fundus images, which can be used for the diabetic retinopathy dr detection by utilizing this imaging technique, multiple modal images can be obtained in a single case additional symptomatic features can be obtained if these images are considered during the diagnosis of dr however, few studies have been carried out to classify mc images using deep learning methods, let alone using multi modal features for analysis in this work, we propose a novel model which uses the multimodal information bottleneck network mmib-net to classify the mc images for the detection of dr our model can extract the features of multiple modalities simultaneously while finding concise feature representations of each modality using the information bottleneck theory mc images classification can be achieved by picking up the combined representations and features of all modalities in our experiments, it is shown that the proposed method can achieve an accurate classification of mc images comparative experiments also demonstrate that the use of multimodality and information bottleneck improves the performance of mc images classification to the best of our knowledge, this is the first report of dr identification utilizing the multimodal information bottleneck convolutional neural network in mc images   \n",
       "51496                                                                                                                                                                                            deep learning segmentation for optical coherence tomography measurements of the lower tear meniscus the tear meniscus contains most of the tear fluid and therefore is a good indicator for the state of the tear film previously, we used a custom-built optical coherence tomography oct system to study the lower tear meniscus by automatically segmenting the image data with a thresholding-based segmentation algorithm tbsa in this report, we investigate whether the results of this image segmentation algorithm are suitable to train a neural network in order to obtain similar or better segmentation results with shorter processing times considering the class imbalance problem, we compare two approaches, one directly segmenting the tear meniscus dsa, the other first localizing the region of interest and then segmenting within the higher resolution image section lsa a total of 6658 images labeled by the tbsa were used to train deep convolutional neural networks with supervised learning five-fold cross-validation reveals a sensitivity of 9636% and 9643%, a specificity of 9998% and 9986% and a jaccard index of 9324% and 9316% for the dsa and lsa, respectively average segmentation times are up to 228 times faster than the tbsa additionally, we report the behavior of the dsa and lsa in cases challenging for the tbsa and further test the applicability to measurements acquired with a commercially available oct system the application of deep learning for the segmentation of the tear meniscus provides a powerful tool for the assessment of the tear film, supporting studies for the investigation of the pathophysiology of dry eye-related diseases   \n",
       "71646                                                                                                                                                   weakly supervised lesion localization for age-related macular degeneration detection using optical coherence tomography images age-related macular degeneration amd is the main cause of irreversible blindness among the elderly and require early diagnosis to prevent vision loss, and careful treatment is essential optical coherence tomography oct, the most commonly used imaging method in the retinal area for the diagnosis of amd, is usually interpreted by a clinician, and oct can help diagnose disease on the basis of the relevant diagnostic criteria, but these judgments can be somewhat subjective we propose an algorithm for the detection of amd based on a weakly supervised convolutional neural network cnn model to support computer-aided diagnosis cad system our main contributions are the following three things 1 we propose a concise cnn model for oct images, which outperforms the existing large cnn models using vgg16 and googlenet architectures 2 we propose an algorithm called expressive gradients eg that extends the existing integrated gradients ig algorithm so as to exploit not only the input-level attribution map, but also the high-level attribution maps due to enriched gradients, eg can highlight suspicious regions for diagnosis of amd better than the guided-backpropagation method and ig 3 our method provides two visualization options: overlay and top-k bounding boxes, which would be useful for cad through experimental evaluation using 10,100 clinical oct images from amd patients, we demonstrate that our eg algorithm outperforms the ig algorithm in terms of localization accuracy and also outperforms the existing object detection methods in terms of class accuracy   \n",
       "125116  superpixel classification based optic disc and optic cup segmentation for glaucoma screening glaucoma is a chronic eye disease that leads to vision loss as it cannot be cured, detecting the disease in time is important current tests using intraocular pressure iop are not sensitive enough for population based glaucoma screening optic nerve head assessment in retinal fundus images is both more promising and superior this paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening in optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc a self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation for optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance the proposed segmentation methods have been evaluated in a database of 650 images with optic disc and optic cup boundaries manually marked by trained professionals experimental results show an average overlapping error of 95% and 241% in optic disc and optic cup segmentation, respectively the results also show an increase in overlapping error as the reliability score is reduced, which justifies the effectiveness of the self-assessment the segmented optic disc and optic cup are then used to compute the cup to disc ratio for glaucoma screening our proposed method achieves areas under curve of 0800 and 0822 in two data sets, which is higher than other methods the methods can be used for segmentation and glaucoma screening the self-assessment will be used as an indicator of cases with large errors and enhance the clinical deployment of the automatic segmentation and screening   \n",
       "8084                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               multimodal machine learning using visual fields and peripapillary circular oct scans in detection of glaucomatous optic neuropathy to develop and validate a multimodal artificial intelligence algorithm, fusionnet, using the pattern deviation probability plots from visual field vf reports and circular peripapillary oct scans to detect glaucomatous optic neuropathy gon   \n",
       "36192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      detection of morphologic patterns of diabetic macular edema using a deep learning approach based on optical coherence tomography images to develop a deep learning dl model to detect morphologic patterns of diabetic macular edema dme based on optical coherence tomography oct images   \n",
       "130986                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    mrmr optimized classification for automatic glaucoma diagnosis min-redundancy max-relevance mrmr is a feature selection methodology based on information theory we explore the mrmr principle for automatic glaucoma diagnosis optimal candidate feature sets are acquired from a composition of clinical screening data and retinal fundus image data an mrmr optimized classifier is further trained using the candidate feature sets to find the optimized classifier we tested the proposed methodology on eye records of 650 subjects collected from singapore eye research institute the experimental results demonstrate that the new classifier is much compact by using less than ¼ of the initial feature set the ranked feature set also enables the clinicians to better access the diagnostic process of the algorithm the work is a further step towards the advancement of the automatic glaucoma diagnosis   \n",
       "79763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       optical coherence tomography biomarkers to distinguish diabetic macular edema from pseudophakic cystoid macular edema using machine learning algorithms in diabetic patients presenting with macular edema me shortly after cataract surgery, identifying the underlying pathology can be challenging and influence management our aim was to develop a simple clinical classifier able to confirm a diabetic etiology using few spectral domain optical coherence tomography parameters   \n",
       "40697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      artificial intelligence mapping of structure to function in glaucoma to develop an artificial intelligence ai-based structure-function sf map relating retinal nerve fiber layer rnfl damage on spectral domain optical coherence tomography sdoct to functional loss on standard automated perimetry sap   \n",
       "59898                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 detection of optical coherence tomography-defined thin-cap fibroatheroma in the coronary artery using deep learning the aim of this study was to develop a deep learning model for classifying frames with versus without optical coherence tomography oct-derived thin-cap fibroatheroma tcfa   \n",
       "26150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             predicting incremental and future visual change in neovascular age-related macular degeneration using deep learning to evaluate the predictive usefulness of quantitative imaging biomarkers, acquired automatically from oct scans, of cross-sectional and future visual outcomes of patients with neovascular age-related macular degeneration amd starting anti-vascular endothelial growth factor vegf therapy   \n",
       "79403                                                                                                                                                                                                                                                unsupervised identification of disease marker candidates in retinal oct imaging data the identification and quantification of markers in medical images is critical for diagnosis, prognosis, and disease management supervised machine learning enables the detection and exploitation of findings that are known a priori after annotation of training examples by experts however, supervision does not scale well, due to the amount of necessary training examples, and the limitation of the marker vocabulary to known entities in this proof-of-concept study, we propose unsupervised identification of anomalies as candidates for markers in retinal optical coherence tomography oct imaging data without a constraint to a priori definitions we identify and categorize marker candidates occurring frequently in the data and demonstrate that these markers show a predictive value in the task of detecting disease a careful qualitative analysis of the identified data driven markers reveals how their quantifiable occurrence aligns with our current understanding of disease course, in early- and late age-related macular degeneration amd patients a multi-scale deep denoising autoencoder is trained on healthy images, and a one-class support vector machine identifies anomalies in new data clustering in the anomalies identifies stable categories using these markers to classify healthy-, early amd- and late amd cases yields an accuracy of 8140% in a second binary classification experiment on a publicly available data set healthy versus intermediate amd, the model achieves an area under the roc curve of 0944   \n",
       "80177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          an automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color fundus photographs the goal of this study was to describe the development and validation of an artificial intelligence-based, deep learning algorithm dla for the detection of referable diabetic retinopathy dr   \n",
       "78333                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       exudate detection in fundus images using deeply-learnable features presence of exudates on a retina is an early sign of diabetic retinopathy, and automatic detection of these can improve the diagnosis of the disease convolutional neural networks cnns have been used for automatic exudate detection, but with poor performance this study has investigated different deep learning techniques to maximize the sensitivity and specificity we have compared multiple deep learning methods, and both supervised and unsupervised classifiers for improving the performance of automatic exudate detection, ie, cnns, pre-trained residual networks resnet-50 and discriminative restricted boltzmann machines the experiments were conducted on two publicly available databases: i diaretdb1 and ii e-ophtha the results show that resnet-50 with support vector machines outperformed other networks with an accuracy and sensitivity of 98% and 099, respectively this shows that resnet-50 can be used for the analysis of the fundus images to detect exudates   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "128682       0       0        0         0       0        0        0        0   \n",
       "82803        0       0        0         0       0        0        0        0   \n",
       "89683        0       0        0         0       0        0        0        0   \n",
       "61024        0       0        0         0       0        0        0        0   \n",
       "41599        0       0        0         0       0        0        0        0   \n",
       "29265        0       0        0         0       0        0        0        0   \n",
       "9743         0       0        0         0       0        0        0        0   \n",
       "51496        0       0        0         0       0        0        0        0   \n",
       "71646        0       0        0         0       0        0        0        0   \n",
       "125116       0       0        0         0       0        0        0        0   \n",
       "8084         0       1        0         0       0        0        0        0   \n",
       "36192        0       0        0         0       0        0        0        0   \n",
       "130986       0       0        0         0       0        0        0        0   \n",
       "79763        0       0        0         0       0        0        0        0   \n",
       "40697        0       0        0         0       0        0        0        0   \n",
       "59898        0       0        0         0       0        0        0        0   \n",
       "26150        0       1        0         0       0        0        0        0   \n",
       "79403        0       0        0         0       0        0        0        0   \n",
       "80177        0       0        0         0       0        0        0        0   \n",
       "78333        0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text  \n",
       "128682          0        1  \n",
       "82803           0        1  \n",
       "89683           0        1  \n",
       "61024           0        1  \n",
       "41599           0        1  \n",
       "29265           0        1  \n",
       "9743            0        1  \n",
       "51496           0        1  \n",
       "71646           0        1  \n",
       "125116          0        1  \n",
       "8084            0        1  \n",
       "36192           0        1  \n",
       "130986          0        1  \n",
       "79763           0        1  \n",
       "40697           0        1  \n",
       "59898           0        1  \n",
       "26150           0        1  \n",
       "79403           0        1  \n",
       "80177           0        1  \n",
       "78333           0        1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['oct_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da73cf19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33611, '1': 568})\n"
     ]
    }
   ],
   "source": [
    "## MAMMOGRAM\n",
    "\n",
    "## text\n",
    "feat['mamm_text'] = np.where(groups['text'].str.contains(\"mammog\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['mamm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e86ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33576, '1': 603})\n"
     ]
    }
   ],
   "source": [
    "## FIBREOPTIC ENDOSCOPY\n",
    "\n",
    "## text\n",
    "text = ['colonoscop', 'endoscop', 'bronchoscop', 'fiberoptic', 'fiber-optic', 'fiberscop', 'fibrescop',\n",
    "       'cystoscop', 'enteroscop', 'hysteroscop']\n",
    "\n",
    "feat['endo_text'] = np.where(groups['text'].str.contains('endoscopy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['endo_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['endo_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['endo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "563a4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33876, '1': 303})\n"
     ]
    }
   ],
   "source": [
    "## DERMATOLOGY IMAGES\n",
    "\n",
    "## text\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"dermoscop\"), \"1\", \"0\")\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"dermascop\"), \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"skin cancer\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"skin cancer\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"dermat\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"dermat\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"melanoma\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"melanoma\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"skin lesion\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"skin lesion\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"rash\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"rash\")) , \"1\", feat['derm_text'])\n",
    "\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"histo\"), \"0\", feat['derm_text']) # exclude histological studies\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"microsc\"), \"0\", feat['derm_text']) # exclude microscopy\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['derm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6aaf3f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "      <th>mamm_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>derm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45995</th>\n",
       "      <td>past and present of computer-assisted dermoscopic diagnosis: performance of a conventional image analyser versus a convolutional neural network in a prospective data set of 1,981 skin lesions convolutional neural networks cnns have shown a dermatologist-level performance in the classification of skin lesions we aimed to deliver a head-to-head comparison of a conventional image analyser cia, which depends on segmentation and weighting of handcrafted features, to a cnn trained by deep learning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129875</th>\n",
       "      <td>distribution quantification on dermoscopy images for computer-assisted diagnosis of cutaneous melanomas computerised analysis on skin lesion images has been reported to be helpful in achieving objective and reproducible diagnosis of melanoma in particular, asymmetry in shape, colour and structure reflects the irregular growth of melanin under the skin and is of great importance for diagnosing the malignancy of skin lesions this paper proposes a novel asymmetry analysis based on a newly developed pigmentation elevation model and the global point signatures gpss specifically, the pigmentation elevation model was first constructed by computer-based analysis of dermoscopy images, for the identification of melanin and haemoglobin asymmetry of skin lesions was then assessed through quantifying distributions of the pigmentation elevation model using the gpss, derived from a laplace-beltrami operator this new approach allows quantifying the shape and pigmentation distributions of cutaneous lesions simultaneously algorithm performance was tested on 351 dermoscopy images, including 88 malignant melanomas and 263 benign naevi, employing a support vector machine svm with tenfold cross-validation strategy competitive diagnostic results were achieved using the proposed asymmetry descriptor only, presenting 8636 % sensitivity, 8213 % specificity and overall 8343 % accuracy, respectively in addition, the proposed gps-based asymmetry analysis enables working on dermoscopy images from different databases and is approved to be inherently robust to the external imaging variations these advantages suggested that the proposed method has good potential for follow-up treatment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29294</th>\n",
       "      <td>a new deep learning approach integrated with clinical data for the dermoscopic differentiation of early melanomas from atypical nevi timely recognition of malignant melanoma mm is challenging for dermatologists worldwide and represents the main determinant for mortality dermoscopic examination is influenced by dermatologistsexperience and fails to achieve adequate accuracy and reproducibility in discriminating atypical nevi an from early melanomas em</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77077</th>\n",
       "      <td>radiomics of brain mri: utility in prediction of metastatic tumor type purpose to investigate the feasibility of tumor type prediction with mri radiomic image features of different brain metastases in a multiclass machine learning approach for patients with unknown primary lesion at the time of diagnosis materials and methods this single-center retrospective analysis included radiomic features of 658 brain metastases from t1-weighted contrast material-enhanced, t1-weighted nonenhanced, and fluid-attenuated inversion recovery flair images in 189 patients 101 women, 88 men; mean age, 61 years; age range, 32-85 years images were acquired over a 9-year period from september 2007 through december 2016 with different mri units, reflecting heterogeneous image data included metastases originated from breast cancer n = 143, small cell lung cancer n = 151, non-small cell lung cancer n = 225, gastrointestinal cancer n = 50, and melanoma n = 89 a total of 1423 quantitative image features and basic clinical data were evaluated by using random forest machine learning algorithms validation was performed with model-external fivefold cross validation comparative analysis of 10 randomly drawn cross-validation sets verified the stability of the results the classifier performance was compared with predictions from a respective conventional reading by two radiologists results areas under the receiver operating characteristic curve of the five-class problem ranged between 064 for non-small cell lung cancer and 082 for melanoma; all p values were less than 01 prediction performance of the classifier was superior to the radiologistsreadings highest differences were observed for melanoma, with a 17-percentage-point gain in sensitivity compared with the sensitivity of both readers; p values were less than 02 conclusion quantitative features of routine brain mr images used in a machine learning classifier provided high discriminatory accuracy in predicting the tumor type of brain metastases © rsna, 2018 online supplemental material is available for this article</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171279</th>\n",
       "      <td>computer-supported diagnosis of melanoma in profilometry laser profilometry offers new possibilities to improve non-invasive tumor diagnostics in dermatology in this paper, a new approach to computer-supported analysis and interpretation of high-resolution skin-surface profiles of melanomas and nevocellular nevi is presented image analysis methods are used to describe the profiles structures by texture parameters based on co-occurrence matrices, features extracted from the fourier power spectrum, and fractal features different feature selection strategies, including genetic algorithms, are applied to determine the best possible subsets of features for the classification task several architectures of multilayer perceptrons with error back-propagation as learning paradigm are trained for the automatic recognition of melanomas and nevi furthermore, network-pruning algorithms are applied to optimize the network topology in the study, the best neural classifier showed an error rate of 45% and was obtained after network pruning the smallest error rate in all, of 23%, was achieved with nearest neighbor classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17300</th>\n",
       "      <td>skin lesion classification using additional patient information in this paper, we describe our method for skin lesion classification the goal is to classify skin lesions based on dermoscopic images to several diagnosesclasses presented in the ham human against machine dataset: melanoma mel, melanocytic nevus nv, basal cell carcinoma bcc, actinic keratosis ak, benign keratosis bkl, dermatofibroma df, and vascular lesion vasc we propose a simplified solution which has a better accuracy than previous methods, but only predicted on a single model that is practical for a real-world scenario our results show that using a network with additional metadata as input achieves a better classification performance this metadata includes both the patient information and the extra information during the data augmentation process on the international skin imaging collaboration isic 2018 skin lesion classification challenge test set, our algorithm yields a balanced multiclass accuracy of 887% on a single model and 895% for the embedding solution, which makes it the currently first ranked algorithm on the live leaderboard to improve the inference accuracy test time augmentation tta is applied we also demonstrate how grad-cam is applied in tta therefore, tta and grad-cam can be integrated in heat map generation, which can be very helpful to assist the clinician for diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125522</th>\n",
       "      <td>an incremental approach to pigmented skin lesion segmentation with classification refinements in uncertain regions skin lesion segmentation in dermatoscopic images is difficult because there are large inter variations in shape, size, color, and texture between lesions and skin types hence, computational features learned from a training set of lesion images may not be applicable to other lesion images in this paper, we propose an incremental method for lesion segmentation it leverages the expectation-maximization algorithm to find an initial segmentation a new adaptive method is proposed to define two types of segmented regions: the high-confident and the low-confident we train a support vector machine, using computational features from the high-confident regions, to further refine segmentation and, hence, achieve improved results for the low-confident regions validation experiments of our proposed method are performed on 319 dermatoscopy images and we have achieved good results with precision and recall to be 0864 and 0875 respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105212</th>\n",
       "      <td>computer-aided diagnosis of psoriasis skin images with hos, texture and color features: a first comparative study of its kind psoriasis is an autoimmune skin disease with red and scaly plaques on skin and affecting about 125 million people worldwide currently, dermatologist use visual and haptic methods for diagnosis the disease severity this does not help them in stratification and risk assessment of the lesion stage and grade further, current methods add complexity during monitoring and follow-up phase the current diagnostic tools lead to subjectivity in decision making and are unreliable and laborious this paper presents a first comparative performance study of its kind using principal component analysis pca based cadx system for psoriasis risk stratification and image classification utilizing: i 11 higher order spectra hos features, ii 60 texture features, and iii 86 color feature sets and their seven combinations aggregate 540 image samples 270 healthy and 270 diseased from 30 psoriasis patients of indian ethnic origin are used in our database machine learning using pca is used for dominant feature selection which is then fed to support vector machine classifier svm to obtain optimized performance three different protocols are implemented using three kinds of feature sets reliability index of the cadx is computed among all feature combinations, the cadx system shows optimal performance of 100% accuracy, 100% sensitivity and specificity, when all three sets of feature are combined further, our experimental result with increasing data size shows that all feature combinations yield high reliability index throughout the pca-cutoffs except color feature set and combination of color and texture feature sets hos features are powerful in psoriasis disease classification and stratification even though, independently, all three set of features hos, texture, and color perform competitively, but when combined, the machine learning system performs the best the system is fully automated, reliable and accurate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94859</th>\n",
       "      <td>dermoscopic image segmentation via multistage fully convolutional networks segmentation of skin lesions is an important step in the automated computer aided diagnosis of melanoma however, existing segmentation methods have a tendency to over- or under-segment the lesions and perform poorly when the lesions have fuzzy boundaries, low contrast with the background, inhomogeneous textures, or contain artifacts furthermore, the performance of these methods are heavily reliant on the appropriate tuning of a large number of parameters as well as the use of effective preprocessing techniques, such as illumination correction and hair removal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88075</th>\n",
       "      <td>skin lesion analysis towards melanoma detection using deep learning network skin lesions are a severe disease globally early detection of melanoma in dermoscopy images significantly increases the survival rate however, the accurate recognition of melanoma is extremely challenging due to the following reasons: low contrast between lesions and skin, visual similarity between melanoma and non-melanoma lesions, etc hence, reliable automatic detection of skin tumors is very useful to increase the accuracy and efficiency of pathologists in this paper, we proposed two deep learning methods to address three main tasks emerging in the area of skin lesion image processing, ie, lesion segmentation task 1, lesion dermoscopic feature extraction task 2 and lesion classification task 3 a deep learning framework consisting of two fully convolutional residual networks fcrn is proposed to simultaneously produce the segmentation result and the coarse classification result a lesion index calculation unit licu is developed to refine the coarse classification results by calculating the distance heat-map a straight-forward cnn is proposed for the dermoscopic feature extraction task the proposed deep learning frameworks were evaluated on the isic 2017 dataset experimental results show the promising accuracies of our frameworks, ie, 0753 for task 1, 0848 for task 2 and 0912 for task 3 were achieved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97185</th>\n",
       "      <td>automatic detection of melanoma using broad extraction of features from digital images automatic and reliable diagnosis of skin cancer, as a smartphone application, is of great interest among different types of skin cancers, melanoma is the most dangerous one which causes most deaths meanwhile, melanoma is curable if it were diagnosed in its early stages in this paper we propose an efficient system for prescreening of pigmented skin lesions for malignancy using general-purpose digital cameras these images can be captured by a smartphone or a digital camera this could be beneficial in different applications, such as computer aided diagnosis and telemedicine applications it could assist dermatologists, or smartphone users, evaluate risk of suspicious moles the proposed method enhances borders and extracts a broad set of dermatologically important features these discriminative features allow classification of lesions into two groups of melanoma and benign this method is computationally appropriate as a smartphone application experimental results show that our proposed method is superior in diagnosis accuracy compared to state-of-the-art methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49424</th>\n",
       "      <td>segmentation methods for acne vulgaris images: proposal of a new methodology applied to fluorescence images acne vulgaris is one of the most common human pathologies worldwide its prevalence causes a high healthcare expenditure acne healthcare costs and effects on individualsquality of life lead to the need of analysing current acne evaluation, treatment and monitoring methods one of the most common ones is manual lesion counting by a dermatologist however, this technique has several limitations, such as time spent that is the reason why the development of new computer-assisted techniques is needed in order to automatically count the acne lesions</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84909</th>\n",
       "      <td>man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists deep learning convolutional neural networks cnn may facilitate melanoma detection, but data comparing a cnns diagnostic performance to larger groups of dermatologists are lacking</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81012</th>\n",
       "      <td>automated decision support in melanocytic lesion management an automated melanocytic lesion image-analysis algorithm is described that aims to reproduce the decision-making of a dermatologist the utility of the algorithm lies in its ability to identify lesions requiring excision from lesions not requiring excision using only wavelet coefficients as features, and testing three different machine learning algorithms, a cohort of 250 images of pigmented lesions is classified based on expert dermatologistsrecommendations of either excision 165 images or no excision 85 images it is shown that the best algorithm utilises the shannon4 wavelet coupled to the support vector machine, where the latter is used as the classifier in this case the algorithm, utilising only 22 othogonal features, achieves a 10-fold cross validation sensitivity and specificity of 096 and 087, resulting in a diagnostic-odds ratio of 261 the advantages of this method over diagnostic algorithms-which make a melanoma/no melanoma decision-are twofold: first, by reproducing the decision-making of a dermatologist, the average number of lesions excised per melanoma among practioners in general can be reduced without compromising the detection of melanoma; and second, the intractable problem of clinically differentiating between many atypical dysplastic naevi and melanoma is avoided since many atypical naevi that require excision on clinical grounds will not be melanoma, the algorithm-in contrast to diagnostic algorithms-can aim for perfect specificities without clinical concerns, thus lowering the excision rate of non-melanoma finally, the algorithm has been implemented as a smart phone application to investigate its utility in clinical practice and to streamline the assimilation of hitherto unseen tested images into the training set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64100</th>\n",
       "      <td>development and accuracy of an artificial intelligence algorithm for acne grading from smartphone photographs we developed an artificial intelligence algorithm aia for smartphones to determine the severity of facial acne using the gea scale and to identify different types of acne lesion comedonal, inflammatory and postinflammatory hyperpigmentation pihp or residual hyperpigmentation overall, 5972 images face, right and left profiles obtained with smartphones ios and/or android from 1072 acne patients were collected three trained dermatologists assessed the acne severity for each patient one acne severity grade per patient grade given by the majority of the three dermatologists from the two sets of three images was used to train the algorithm acne lesion identification was performed from a subgroup of 348 images using a tagging tool; tagged images served to train the algorithm the algorithm evolved and was adjusted for sensibility, specificity and correlation using new images the correlation between the gea grade and the quantification and qualification of acne lesions both by the aia and the experts for each image were evaluated for all aia versions at final version 6, the gea grading provided by aia reached 68% and was similar to that provided by the dermatologists between version 4 and version 6, aia improved precision results multiplied by 15 for inflammatory lesions, 25 for non-inflammatory lesions and by 2 for pihp; recall was improved by 26, 16 and 27 the weighted average of precision and recall or f1 score was 84% for inflammatory lesions, 61% for non-inflammatory lesions and 72% for pihp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138072</th>\n",
       "      <td>a support vector machine for decision support in melanoma recognition the early diagnosis of melanoma is critical to achieving reduced mortality and increased survival although clinical examination is currently the method of choice for melanocytic lesion assessment, difficulties may arise in the diagnosis of atypical lesions from both the naked eye and dermoscopic perspective, dysplastic naevi often exhibit a prominent heterogeneity of structure that renders their clinical distinction from melanoma difficult to address these problems in diagnosis, there exists a heightened interest among researchers regarding the utility of machine learning techniques in computerised image analysis here we report on the utility, for dermatologists, of support vector machine svm technology in melanoma diagnosis, using an archive of 199 digital dermoscopic images of excised atypical melanocytic lesions our best validation models achieved an average sensitivity and specificity for melanoma diagnosis of 086 and 072, respectively applying the best model to our test set yielded a sensitivity of 089, a diagnostic odds ratio of 1409 and an area under the receiver operated characteristic curve auc of 076 advantages of the procedure implemented are the simplicity of feature extraction and the computationally cheap and efficient nature of svms the derived model generalises well with outcomes that compare favourably with competing algorithms and expert assessment in line with the concept of the utility of decision support systems in clinical practice, we propose that to reduce the risk of missed melanomas, both the dermatologistsassessment and the svm diagnosis be incorporated into the clinical decision-making process</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171114</th>\n",
       "      <td>feature selection for optimized skin tumor recognition using genetic algorithms in this paper, a new approach to computer supported diagnosis of skin tumors in dermatology is presented high resolution skin surface profiles are analyzed to recognize malignant melanomas and nevocytic nevi moles, automatically in the first step, several types of features are extracted by 2d image analysis methods characterizing the structure of skin surface profiles: texture features based on cooccurrence matrices, fourier features and fractal features then, feature selection algorithms are applied to determine suitable feature subsets for the recognition process feature selection is described as an optimization problem and several approaches including heuristic strategies, greedy and genetic algorithms are compared as quality measure for feature subsets, the classification rate of the nearest neighbor classifier computed with the leaving-one-out method is used genetic algorithms show the best results finally, neural networks with error back-propagation as learning paradigm are trained using the selected feature sets different network topologies, learning parameters and pruning algorithms are investigated to optimize the classification performance of the neural classifiers with the optimized recognition system a classification performance of 977% is achieved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91486</th>\n",
       "      <td>a feature fusion system for basal cell carcinoma detection through data-driven feature learning and patient profile basal cell carcinoma bcc is the most common skin cancer, which is highly damaging in its advanced stages computer-aided techniques provide a feasible option for early detection of bcc however, automated bcc detection techniques immensely rely on handcrafting high-level precise features such features are not only computationally complex to design but can also represent a very limited aspect of the lesion characteristics this paper proposes an automated bcc detection technique that directly learns the features from image data, eliminating the need for handcrafted feature design</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63047</th>\n",
       "      <td>superior skin cancer classification by the combination of human and artificial intelligence in recent studies, convolutional neural networks cnns outperformed dermatologists in distinguishing dermoscopic images of melanoma and nevi in these studies, dermatologists and artificial intelligence were considered as opponents however, the combination of classifiers frequently yields superior results, both in machine learning and among humans in this study, we investigated the potential benefit of combining human and artificial intelligence for skin cancer classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127969</th>\n",
       "      <td>computer-aided diagnosis of melanoma using border and wavelet-based texture analysis this paper presents a novel computer-aided diagnosis system for melanoma the novelty lies in the optimised selection and integration of features derived from textural, borderbased and geometrical properties of the melanoma lesion the texture features are derived from using wavelet-decomposition, the border features are derived from constructing a boundaryseries model of the lesion border and analysing it in spatial and frequency domains, and the geometry features are derived from shape indexes the optimised selection of features is achieved by using the gain-ratio method, which is shown to be computationally efficient for melanoma diagnosis application classification is done through the use of four classifiers; namely, support vector machine, random forest, logistic model tree and hidden naive bayes the proposed diagnostic system is applied on a set of 289 dermoscopy images 114 malignant, 175 benign partitioned into train, validation and test image sets the system achieves and accuracy of 9126% and auc value of 0937, when 23 features are used other important findings include i the clear advantage gained in complementing texture with border and geometry features, compared to using texture information only, and ii higher contribution of texture features than border-based features in the optimised feature set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "45995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         past and present of computer-assisted dermoscopic diagnosis: performance of a conventional image analyser versus a convolutional neural network in a prospective data set of 1,981 skin lesions convolutional neural networks cnns have shown a dermatologist-level performance in the classification of skin lesions we aimed to deliver a head-to-head comparison of a conventional image analyser cia, which depends on segmentation and weighting of handcrafted features, to a cnn trained by deep learning   \n",
       "129875                                                                                                                                                                                                                                                                                                                                                                                                       distribution quantification on dermoscopy images for computer-assisted diagnosis of cutaneous melanomas computerised analysis on skin lesion images has been reported to be helpful in achieving objective and reproducible diagnosis of melanoma in particular, asymmetry in shape, colour and structure reflects the irregular growth of melanin under the skin and is of great importance for diagnosing the malignancy of skin lesions this paper proposes a novel asymmetry analysis based on a newly developed pigmentation elevation model and the global point signatures gpss specifically, the pigmentation elevation model was first constructed by computer-based analysis of dermoscopy images, for the identification of melanin and haemoglobin asymmetry of skin lesions was then assessed through quantifying distributions of the pigmentation elevation model using the gpss, derived from a laplace-beltrami operator this new approach allows quantifying the shape and pigmentation distributions of cutaneous lesions simultaneously algorithm performance was tested on 351 dermoscopy images, including 88 malignant melanomas and 263 benign naevi, employing a support vector machine svm with tenfold cross-validation strategy competitive diagnostic results were achieved using the proposed asymmetry descriptor only, presenting 8636 % sensitivity, 8213 % specificity and overall 8343 % accuracy, respectively in addition, the proposed gps-based asymmetry analysis enables working on dermoscopy images from different databases and is approved to be inherently robust to the external imaging variations these advantages suggested that the proposed method has good potential for follow-up treatment   \n",
       "29294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   a new deep learning approach integrated with clinical data for the dermoscopic differentiation of early melanomas from atypical nevi timely recognition of malignant melanoma mm is challenging for dermatologists worldwide and represents the main determinant for mortality dermoscopic examination is influenced by dermatologistsexperience and fails to achieve adequate accuracy and reproducibility in discriminating atypical nevi an from early melanomas em   \n",
       "77077   radiomics of brain mri: utility in prediction of metastatic tumor type purpose to investigate the feasibility of tumor type prediction with mri radiomic image features of different brain metastases in a multiclass machine learning approach for patients with unknown primary lesion at the time of diagnosis materials and methods this single-center retrospective analysis included radiomic features of 658 brain metastases from t1-weighted contrast material-enhanced, t1-weighted nonenhanced, and fluid-attenuated inversion recovery flair images in 189 patients 101 women, 88 men; mean age, 61 years; age range, 32-85 years images were acquired over a 9-year period from september 2007 through december 2016 with different mri units, reflecting heterogeneous image data included metastases originated from breast cancer n = 143, small cell lung cancer n = 151, non-small cell lung cancer n = 225, gastrointestinal cancer n = 50, and melanoma n = 89 a total of 1423 quantitative image features and basic clinical data were evaluated by using random forest machine learning algorithms validation was performed with model-external fivefold cross validation comparative analysis of 10 randomly drawn cross-validation sets verified the stability of the results the classifier performance was compared with predictions from a respective conventional reading by two radiologists results areas under the receiver operating characteristic curve of the five-class problem ranged between 064 for non-small cell lung cancer and 082 for melanoma; all p values were less than 01 prediction performance of the classifier was superior to the radiologistsreadings highest differences were observed for melanoma, with a 17-percentage-point gain in sensitivity compared with the sensitivity of both readers; p values were less than 02 conclusion quantitative features of routine brain mr images used in a machine learning classifier provided high discriminatory accuracy in predicting the tumor type of brain metastases © rsna, 2018 online supplemental material is available for this article   \n",
       "171279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 computer-supported diagnosis of melanoma in profilometry laser profilometry offers new possibilities to improve non-invasive tumor diagnostics in dermatology in this paper, a new approach to computer-supported analysis and interpretation of high-resolution skin-surface profiles of melanomas and nevocellular nevi is presented image analysis methods are used to describe the profiles structures by texture parameters based on co-occurrence matrices, features extracted from the fourier power spectrum, and fractal features different feature selection strategies, including genetic algorithms, are applied to determine the best possible subsets of features for the classification task several architectures of multilayer perceptrons with error back-propagation as learning paradigm are trained for the automatic recognition of melanomas and nevi furthermore, network-pruning algorithms are applied to optimize the network topology in the study, the best neural classifier showed an error rate of 45% and was obtained after network pruning the smallest error rate in all, of 23%, was achieved with nearest neighbor classification   \n",
       "17300                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        skin lesion classification using additional patient information in this paper, we describe our method for skin lesion classification the goal is to classify skin lesions based on dermoscopic images to several diagnosesclasses presented in the ham human against machine dataset: melanoma mel, melanocytic nevus nv, basal cell carcinoma bcc, actinic keratosis ak, benign keratosis bkl, dermatofibroma df, and vascular lesion vasc we propose a simplified solution which has a better accuracy than previous methods, but only predicted on a single model that is practical for a real-world scenario our results show that using a network with additional metadata as input achieves a better classification performance this metadata includes both the patient information and the extra information during the data augmentation process on the international skin imaging collaboration isic 2018 skin lesion classification challenge test set, our algorithm yields a balanced multiclass accuracy of 887% on a single model and 895% for the embedding solution, which makes it the currently first ranked algorithm on the live leaderboard to improve the inference accuracy test time augmentation tta is applied we also demonstrate how grad-cam is applied in tta therefore, tta and grad-cam can be integrated in heat map generation, which can be very helpful to assist the clinician for diagnosis   \n",
       "125522                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             an incremental approach to pigmented skin lesion segmentation with classification refinements in uncertain regions skin lesion segmentation in dermatoscopic images is difficult because there are large inter variations in shape, size, color, and texture between lesions and skin types hence, computational features learned from a training set of lesion images may not be applicable to other lesion images in this paper, we propose an incremental method for lesion segmentation it leverages the expectation-maximization algorithm to find an initial segmentation a new adaptive method is proposed to define two types of segmented regions: the high-confident and the low-confident we train a support vector machine, using computational features from the high-confident regions, to further refine segmentation and, hence, achieve improved results for the low-confident regions validation experiments of our proposed method are performed on 319 dermatoscopy images and we have achieved good results with precision and recall to be 0864 and 0875 respectively   \n",
       "105212                                    computer-aided diagnosis of psoriasis skin images with hos, texture and color features: a first comparative study of its kind psoriasis is an autoimmune skin disease with red and scaly plaques on skin and affecting about 125 million people worldwide currently, dermatologist use visual and haptic methods for diagnosis the disease severity this does not help them in stratification and risk assessment of the lesion stage and grade further, current methods add complexity during monitoring and follow-up phase the current diagnostic tools lead to subjectivity in decision making and are unreliable and laborious this paper presents a first comparative performance study of its kind using principal component analysis pca based cadx system for psoriasis risk stratification and image classification utilizing: i 11 higher order spectra hos features, ii 60 texture features, and iii 86 color feature sets and their seven combinations aggregate 540 image samples 270 healthy and 270 diseased from 30 psoriasis patients of indian ethnic origin are used in our database machine learning using pca is used for dominant feature selection which is then fed to support vector machine classifier svm to obtain optimized performance three different protocols are implemented using three kinds of feature sets reliability index of the cadx is computed among all feature combinations, the cadx system shows optimal performance of 100% accuracy, 100% sensitivity and specificity, when all three sets of feature are combined further, our experimental result with increasing data size shows that all feature combinations yield high reliability index throughout the pca-cutoffs except color feature set and combination of color and texture feature sets hos features are powerful in psoriasis disease classification and stratification even though, independently, all three set of features hos, texture, and color perform competitively, but when combined, the machine learning system performs the best the system is fully automated, reliable and accurate    \n",
       "94859                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         dermoscopic image segmentation via multistage fully convolutional networks segmentation of skin lesions is an important step in the automated computer aided diagnosis of melanoma however, existing segmentation methods have a tendency to over- or under-segment the lesions and perform poorly when the lesions have fuzzy boundaries, low contrast with the background, inhomogeneous textures, or contain artifacts furthermore, the performance of these methods are heavily reliant on the appropriate tuning of a large number of parameters as well as the use of effective preprocessing techniques, such as illumination correction and hair removal   \n",
       "88075                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     skin lesion analysis towards melanoma detection using deep learning network skin lesions are a severe disease globally early detection of melanoma in dermoscopy images significantly increases the survival rate however, the accurate recognition of melanoma is extremely challenging due to the following reasons: low contrast between lesions and skin, visual similarity between melanoma and non-melanoma lesions, etc hence, reliable automatic detection of skin tumors is very useful to increase the accuracy and efficiency of pathologists in this paper, we proposed two deep learning methods to address three main tasks emerging in the area of skin lesion image processing, ie, lesion segmentation task 1, lesion dermoscopic feature extraction task 2 and lesion classification task 3 a deep learning framework consisting of two fully convolutional residual networks fcrn is proposed to simultaneously produce the segmentation result and the coarse classification result a lesion index calculation unit licu is developed to refine the coarse classification results by calculating the distance heat-map a straight-forward cnn is proposed for the dermoscopic feature extraction task the proposed deep learning frameworks were evaluated on the isic 2017 dataset experimental results show the promising accuracies of our frameworks, ie, 0753 for task 1, 0848 for task 2 and 0912 for task 3 were achieved   \n",
       "97185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  automatic detection of melanoma using broad extraction of features from digital images automatic and reliable diagnosis of skin cancer, as a smartphone application, is of great interest among different types of skin cancers, melanoma is the most dangerous one which causes most deaths meanwhile, melanoma is curable if it were diagnosed in its early stages in this paper we propose an efficient system for prescreening of pigmented skin lesions for malignancy using general-purpose digital cameras these images can be captured by a smartphone or a digital camera this could be beneficial in different applications, such as computer aided diagnosis and telemedicine applications it could assist dermatologists, or smartphone users, evaluate risk of suspicious moles the proposed method enhances borders and extracts a broad set of dermatologically important features these discriminative features allow classification of lesions into two groups of melanoma and benign this method is computationally appropriate as a smartphone application experimental results show that our proposed method is superior in diagnosis accuracy compared to state-of-the-art methods   \n",
       "49424                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           segmentation methods for acne vulgaris images: proposal of a new methodology applied to fluorescence images acne vulgaris is one of the most common human pathologies worldwide its prevalence causes a high healthcare expenditure acne healthcare costs and effects on individualsquality of life lead to the need of analysing current acne evaluation, treatment and monitoring methods one of the most common ones is manual lesion counting by a dermatologist however, this technique has several limitations, such as time spent that is the reason why the development of new computer-assisted techniques is needed in order to automatically count the acne lesions   \n",
       "84909                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists deep learning convolutional neural networks cnn may facilitate melanoma detection, but data comparing a cnns diagnostic performance to larger groups of dermatologists are lacking   \n",
       "81012                                                                                                                                                                                                                                                           automated decision support in melanocytic lesion management an automated melanocytic lesion image-analysis algorithm is described that aims to reproduce the decision-making of a dermatologist the utility of the algorithm lies in its ability to identify lesions requiring excision from lesions not requiring excision using only wavelet coefficients as features, and testing three different machine learning algorithms, a cohort of 250 images of pigmented lesions is classified based on expert dermatologistsrecommendations of either excision 165 images or no excision 85 images it is shown that the best algorithm utilises the shannon4 wavelet coupled to the support vector machine, where the latter is used as the classifier in this case the algorithm, utilising only 22 othogonal features, achieves a 10-fold cross validation sensitivity and specificity of 096 and 087, resulting in a diagnostic-odds ratio of 261 the advantages of this method over diagnostic algorithms-which make a melanoma/no melanoma decision-are twofold: first, by reproducing the decision-making of a dermatologist, the average number of lesions excised per melanoma among practioners in general can be reduced without compromising the detection of melanoma; and second, the intractable problem of clinically differentiating between many atypical dysplastic naevi and melanoma is avoided since many atypical naevi that require excision on clinical grounds will not be melanoma, the algorithm-in contrast to diagnostic algorithms-can aim for perfect specificities without clinical concerns, thus lowering the excision rate of non-melanoma finally, the algorithm has been implemented as a smart phone application to investigate its utility in clinical practice and to streamline the assimilation of hitherto unseen tested images into the training set   \n",
       "64100                                                                                                                                                                                                                                                                                                                                                                                                                                                                   development and accuracy of an artificial intelligence algorithm for acne grading from smartphone photographs we developed an artificial intelligence algorithm aia for smartphones to determine the severity of facial acne using the gea scale and to identify different types of acne lesion comedonal, inflammatory and postinflammatory hyperpigmentation pihp or residual hyperpigmentation overall, 5972 images face, right and left profiles obtained with smartphones ios and/or android from 1072 acne patients were collected three trained dermatologists assessed the acne severity for each patient one acne severity grade per patient grade given by the majority of the three dermatologists from the two sets of three images was used to train the algorithm acne lesion identification was performed from a subgroup of 348 images using a tagging tool; tagged images served to train the algorithm the algorithm evolved and was adjusted for sensibility, specificity and correlation using new images the correlation between the gea grade and the quantification and qualification of acne lesions both by the aia and the experts for each image were evaluated for all aia versions at final version 6, the gea grading provided by aia reached 68% and was similar to that provided by the dermatologists between version 4 and version 6, aia improved precision results multiplied by 15 for inflammatory lesions, 25 for non-inflammatory lesions and by 2 for pihp; recall was improved by 26, 16 and 27 the weighted average of precision and recall or f1 score was 84% for inflammatory lesions, 61% for non-inflammatory lesions and 72% for pihp   \n",
       "138072                                                                                                                                                                                                                                                                                                                                                                  a support vector machine for decision support in melanoma recognition the early diagnosis of melanoma is critical to achieving reduced mortality and increased survival although clinical examination is currently the method of choice for melanocytic lesion assessment, difficulties may arise in the diagnosis of atypical lesions from both the naked eye and dermoscopic perspective, dysplastic naevi often exhibit a prominent heterogeneity of structure that renders their clinical distinction from melanoma difficult to address these problems in diagnosis, there exists a heightened interest among researchers regarding the utility of machine learning techniques in computerised image analysis here we report on the utility, for dermatologists, of support vector machine svm technology in melanoma diagnosis, using an archive of 199 digital dermoscopic images of excised atypical melanocytic lesions our best validation models achieved an average sensitivity and specificity for melanoma diagnosis of 086 and 072, respectively applying the best model to our test set yielded a sensitivity of 089, a diagnostic odds ratio of 1409 and an area under the receiver operated characteristic curve auc of 076 advantages of the procedure implemented are the simplicity of feature extraction and the computationally cheap and efficient nature of svms the derived model generalises well with outcomes that compare favourably with competing algorithms and expert assessment in line with the concept of the utility of decision support systems in clinical practice, we propose that to reduce the risk of missed melanomas, both the dermatologistsassessment and the svm diagnosis be incorporated into the clinical decision-making process   \n",
       "171114                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        feature selection for optimized skin tumor recognition using genetic algorithms in this paper, a new approach to computer supported diagnosis of skin tumors in dermatology is presented high resolution skin surface profiles are analyzed to recognize malignant melanomas and nevocytic nevi moles, automatically in the first step, several types of features are extracted by 2d image analysis methods characterizing the structure of skin surface profiles: texture features based on cooccurrence matrices, fourier features and fractal features then, feature selection algorithms are applied to determine suitable feature subsets for the recognition process feature selection is described as an optimization problem and several approaches including heuristic strategies, greedy and genetic algorithms are compared as quality measure for feature subsets, the classification rate of the nearest neighbor classifier computed with the leaving-one-out method is used genetic algorithms show the best results finally, neural networks with error back-propagation as learning paradigm are trained using the selected feature sets different network topologies, learning parameters and pruning algorithms are investigated to optimize the classification performance of the neural classifiers with the optimized recognition system a classification performance of 977% is achieved   \n",
       "91486                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               a feature fusion system for basal cell carcinoma detection through data-driven feature learning and patient profile basal cell carcinoma bcc is the most common skin cancer, which is highly damaging in its advanced stages computer-aided techniques provide a feasible option for early detection of bcc however, automated bcc detection techniques immensely rely on handcrafting high-level precise features such features are not only computationally complex to design but can also represent a very limited aspect of the lesion characteristics this paper proposes an automated bcc detection technique that directly learns the features from image data, eliminating the need for handcrafted feature design   \n",
       "63047                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               superior skin cancer classification by the combination of human and artificial intelligence in recent studies, convolutional neural networks cnns outperformed dermatologists in distinguishing dermoscopic images of melanoma and nevi in these studies, dermatologists and artificial intelligence were considered as opponents however, the combination of classifiers frequently yields superior results, both in machine learning and among humans in this study, we investigated the potential benefit of combining human and artificial intelligence for skin cancer classification   \n",
       "127969                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   computer-aided diagnosis of melanoma using border and wavelet-based texture analysis this paper presents a novel computer-aided diagnosis system for melanoma the novelty lies in the optimised selection and integration of features derived from textural, borderbased and geometrical properties of the melanoma lesion the texture features are derived from using wavelet-decomposition, the border features are derived from constructing a boundaryseries model of the lesion border and analysing it in spatial and frequency domains, and the geometry features are derived from shape indexes the optimised selection of features is achieved by using the gain-ratio method, which is shown to be computationally efficient for melanoma diagnosis application classification is done through the use of four classifiers; namely, support vector machine, random forest, logistic model tree and hidden naive bayes the proposed diagnostic system is applied on a set of 289 dermoscopy images 114 malignant, 175 benign partitioned into train, validation and test image sets the system achieves and accuracy of 9126% and auc value of 0937, when 23 features are used other important findings include i the clear advantage gained in complementing texture with border and geometry features, compared to using texture information only, and ii higher contribution of texture features than border-based features in the optimised feature set    \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "45995        0       0        0         0       0        0        0        0   \n",
       "129875       0       0        0         0       0        0        0        0   \n",
       "29294        0       0        0         0       0        0        0        0   \n",
       "77077        0       0        1         0       0        0        0        0   \n",
       "171279       0       0        0         0       0        0        0        0   \n",
       "17300        0       0        0         0       0        0        0        0   \n",
       "125522       0       0        0         0       0        0        0        0   \n",
       "105212       0       0        0         0       0        0        0        0   \n",
       "94859        0       0        0         0       0        0        0        0   \n",
       "88075        0       0        0         0       0        0        0        0   \n",
       "97185        0       0        0         0       0        0        0        0   \n",
       "49424        0       0        0         0       0        0        0        0   \n",
       "84909        0       0        0         0       0        0        0        0   \n",
       "81012        0       0        0         0       0        0        0        0   \n",
       "64100        0       0        0         0       0        0        0        0   \n",
       "138072       0       0        0         0       0        0        0        0   \n",
       "171114       0       0        0         0       0        0        0        0   \n",
       "91486        0       0        0         0       0        0        0        0   \n",
       "63047        0       0        0         0       0        0        0        0   \n",
       "127969       0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text mamm_text endo_text derm_text  \n",
       "45995           0        0         0         0         1  \n",
       "129875          0        0         0         0         1  \n",
       "29294           0        0         0         0         1  \n",
       "77077           0        0         0         0         1  \n",
       "171279          0        0         0         0         1  \n",
       "17300           0        0         0         0         1  \n",
       "125522          0        0         0         0         1  \n",
       "105212          0        0         0         0         1  \n",
       "94859           0        0         0         0         1  \n",
       "88075           0        0         0         0         1  \n",
       "97185           0        0         0         0         1  \n",
       "49424           0        0         0         0         1  \n",
       "84909           0        0         0         0         1  \n",
       "81012           0        0         0         0         1  \n",
       "64100           0        0         0         0         1  \n",
       "138072          0        0         0         0         1  \n",
       "171114          0        0         0         0         1  \n",
       "91486           0        0         0         0         1  \n",
       "63047           0        0         0         0         1  \n",
       "127969          0        0         0         0         1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['derm_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8503ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32439, '1': 1740})\n"
     ]
    }
   ],
   "source": [
    "## GENOMIC\n",
    "\n",
    "## text\n",
    "text = ['candidate gene', 'prognostic gene', ' gene ', ' genes ', ' dna ', ' rna ']\n",
    "\n",
    "feat['gene_text'] = np.where(groups['text'].str.contains('genomic'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['gene_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['gene_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['gene_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a231099f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "      <th>mamm_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>gene_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25404</th>\n",
       "      <td>machine learning-driven and smartphone-based fluorescence detection for crispr diagnostic of sars-cov-2 rapid, accurate, and low-cost detection of sars-cov-2 is crucial to contain the transmission of covid-19 here, we present a cost-effective smartphone-based device coupled with machine learning-driven software that evaluates the fluorescence signals of the crispr diagnostic of sars-cov-2 the device consists of a three-dimensional 3d-printed housing and low-cost optic components that allow excitation of fluorescent reporters and selective transmission of the fluorescence emission to a smartphone custom software equipped with a binary classification model has been developed to quantify the acquired fluorescence images and determine the presence of the virus our detection system has a limit of detection lod of 625 rna copies/μl on laboratory samples and produces a test accuracy of 95% and sensitivity of 97% on 96 nasopharyngeal swab samples with transmissible viral loads our quantitative fluorescence score shows a strong correlation with the quantitative reverse transcription polymerase chain reaction rt-qpcr ct values, offering valuable information of the viral load and, therefore, presenting an important advantage over nonquantitative readouts</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86153</th>\n",
       "      <td>predicting clinical outcomes in colorectal cancer using machine learning using gene markers and other patient features to predict clinical outcomes plays a vital role in enhancing clinical decision making and improving prognostic accuracy this work uses a large set of colorectal cancer patient data to train predictive models using machine learning methods such as random forest, general linear model, and neural network for clinically relevant outcomes including disease free survival, survival, radio-chemotherapy response rct-r and relapse the most successful predictive models were created for dichotomous outcomes like relapse and rct-r with accuracies of 071 and 070 on blinded test data respectively the best prediction models regarding overall survival and disease-free survival had c-index scores of 086 and 076 respectively these models could be used in the future to aid a decision for or against chemotherapy and improve survival prognosis we propose that future work should focus on creating reusable frameworks and infrastructure for training and delivering predictive models to physicians, so that they could be readily applied to other diseases in practice and be continuously developed integrating new data</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145485</th>\n",
       "      <td>recursive fuzzy granulation for gene subsets extraction and cancer classification a typical microarray gene expression dataset is usually both extremely sparse and imbalanced to select multiple highly informative gene subsets for cancer classification and diagnosis, a new fuzzy granular support vector machine---recursive feature elimination algorithm fgsvm-rfe is designed in this paper as a hybrid algorithm of statistical learning, fuzzy clustering, and granular computing, the fgsvm-rfe separately eliminates irrelevant, redundant, or noisy genes in different granules at different stages and selects highly informative genes with potentially different biological functions in balance empirical studies on three public datasets demonstrate that the fgsvm-rfe outperforms state-of-the-art approaches moreover, the fgsvm-rfe can extract multiple gene subsets on each of which a classifier can be modeled with 100% accuracy specifically, the independent testing accuracy for the prostate cancer dataset is significantly improved the previous best result is 86% with 16 genes and our best result is 100% with only eight genes the identified genes are annotated by onto-express to be biologically meaningful</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20257</th>\n",
       "      <td>classification of molecular subtypes of high-grade serous ovarian cancer by maldi-imaging despite the correlation of clinical outcome and molecular subtypes of high-grade serous ovarian cancer hgsoc, contemporary gene expression signatures have not been implemented in clinical practice to stratify patients for targeted therapy hence, we aimed to examine the potential of unsupervised matrix-assisted laser desorption/ionization imaging mass spectrometry maldi-ims to stratify patients who might benefit from targeted therapeutic strategies molecular subtyping of paraffin-embedded tissue samples from 279 hgsoc patients was performed by nanostring analysis ground truth labeling next, we applied maldi-ims paired with machine-learning algorithms to identify distinct mass profiles on the same paraffin-embedded tissue sections and distinguish hgsoc subtypes by proteomic signature finally, we devised a novel approach to annotate spectra of stromal origin we elucidated a maldi-derived proteomic signature 135 peptides able to classify hgsoc subtypes random forest classifiers achieved an area under the curve auc of 0983 furthermore, we demonstrated that the exclusion of stroma-associated spectra provides tangible improvements to classification quality auc = 0988 moreover, novel maldi-based stroma annotation achieved near-perfect classifications auc = 0999 here, we present a concept integrating maldi-ims with machine-learning algorithms to classify patients according to distinct molecular subtypes of hgsoc this has great potential to assign patients for personalized treatment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83442</th>\n",
       "      <td>early diagnosis of alzheimers disease based on resting-state brain networks and deep learning computerized healthcare has undergone rapid development thanks to the advances in medical imaging and machine learning technologies especially, recent progress on deep learning opens a new era for multimedia based clinical decision support in this paper, we use deep learning with brain network and clinical relevant text information to make early diagnosis of alzheimers disease ad the clinical relevant text information includes age, gender, and apoe gene of the subject the brain network is constructed by computing the functional connectivity of brain regions using resting-state functional magnetic resonance imaging r-fmri data a targeted autoencoder network is built to distinguish normal aging from mild cognitive impairment, an early stage of ad the proposed method reveals discriminative brain network features effectively and provides a reliable classifier for ad detection compared to traditional classifiers based on r-fmri time series data, about 3121 percent improvement of the prediction accuracy is achieved by the proposed deep learning method, and the standard deviation reduces by 5123 percent in the best case that means our prediction model is more stable and reliable compared to the traditional methods our work excavates deep learnings advantages of classifying high-dimensional multimedia data in medical services, and could help predict and prevent ad at an early stage</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110352</th>\n",
       "      <td>multi-class bcga-elm based classifier that identifies biomarkers associated with hallmarks of cancer traditional cancer treatments have centered on cytotoxic drugs and general purpose chemotherapy that may not be tailored to treat specific cancers identification of molecular markers that are related to different types of cancers might lead to discovery of drugs that are patient and disease specific this study aims to use microarray gene expression cancer data to identify biomarkers that are indicative of different types of cancers our aim is to provide a multi-class cancer classifier that can simultaneously differentiate between cancers and identify type-specific biomarkers, through the application of the binary coded genetic algorithm bcga and a neural network based extreme learning machine elm algorithm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13461</th>\n",
       "      <td>the application of artificial intelligence methods to gene expression data for differentiation of uncomplicated and complicated appendicitis in children and adolescents - a proof of concept study genome wide gene expression analysis has revealed hints for independent immunological pathways underlying the pathophysiologies of phlegmonous pa and gangrenous appendicitis ga methods of artificial intelligence ai have successfully been applied to routine laboratory and sonographic parameters for differentiation of the inflammatory manifestations in this study we aimed to apply ai methods to gene expression data to provide evidence for feasibility</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165383</th>\n",
       "      <td>assessing optimal neural network architecture for identifying disease-associated multi-marker genotypes using a permutation test, and application to calpain 10 polymorphisms associated with diabetes biallelic markers, such as single nucleotide polymorphisms snps, provide greater information for localising disease loci when treated as multilocus haplotypes, but often haplotypes are not immediately available from multilocus genotypes in case-control studies an artificial neural network allows investigation of association between disease phenotype and tightly linked markers without requiring haplotype phase and without modelling any evolutionary history for the disease-related haplotypes the network assesses whether marker haplotypes differ between cases and controls to the extent that classification of disease status based on multi-marker genotypes is achievable the network is trained to recognise affection status based on supplied marker genotypes, and then for each multi-marker genotype it produces outputs which aim to approximate the associated affection status next, the genotypes are permuted relative to affection status to produce many random datasets and the process of training and recording of outputs is repeated the extent to which the ability to predict affection for the real dataset exceeds that for the random datasets measures the statistical significance of the association between multi-marker genotype and affection this permutation test performs well with simulated case-control datasets, particularly when major gene effects are present we have explored the effects of systematically varying different network parameters in order to identify their optimal values we have applied the permutation test to 4 snps of the calpain 10 capn10 gene typed in a case-control sample of subjects with type 2 diabetes, impaired glucose tolerance, and controls we show that the neural network produces more highly significant evidence for association than do single marker tests corrected for the number of markers genotyped the use of a permutation test could potentially allow conditional analyses which could incorporate known risk factors alongside marker genotypes permuting only the marker genotypes relative to affection status and these risk factors would allow the contribution of the markers to disease risk to be independently assessed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>identification of prognostic biomarker candidates associated with melanoma using high-dimensional genomic data survival of patients with metastatic melanoma varies widely melanoma is a highly proliferative, chemo-resistant disease with the recent availability of immunotherapies such as checkpoint inhibitors, durable response rates have improved but are often still limited to 2-3 years response rates to treatment range from 30 to 45% with combination therapy however no improvement in overall survival is frequently observed of the available therapies, many have targeted the brafv600e mutation that results in abnormal mapk pathway activation which is important for regulating cell proliferation immune checkpoint inhibitors such as anti-pd-1 and anti-pd-l1 offer better success but response rates are still low identifying biomarkers to better target those who will respond and identify the right combination of treatment is the best approach in this study, we utilize data from the cancer cell line encyclopedia ccle, including 62 samples, to examine features of gene expression 19k+ and copy number 20k+ in the melanoma cell lines we perform a clustering analysis on the feature set to assess genetically similarity among the cell lines we then discover which specific genes and combinations thereof maximize cluster density we design a feature selection approach for high-dimensional datasets that integrates multiple disparate machine learning techniques into one cohesive pipeline our approach provides a small subset of genes that can accurately distinguish between the clusters of melanoma cell lines across multiple types of classifiers in particular, we find only the 15 highest ranked genes among the original 19 k are necessary to achieve perfect or near-perfect test split classification performance of these 15 genes, some are known to be linked to melanoma or other cancer progressions, while others have not previously been linked to melanoma and are of interest for further examination</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37927</th>\n",
       "      <td>development of prognostic indicator based on autophagy-related lncrna analysis in colon adenocarcinoma there were no systematic researches about autophagy-related long noncoding rna lncrna signatures to predict the survival of patients with colon adenocarcinoma it was necessary to set up corresponding autophagy-related lncrna signatures the expression profiles of lncrnas which contained 480 colon adenocarcinoma samples were obtained from the cancer genome atlas tcga database the coexpression network of lncrnas and autophagy-related genes was utilized to select autophagy-related lncrnas the lncrnas were further screened using univariate cox regression in addition, lasso regression and multivariate cox regression were used to develop an autophagy-related lncrna signature a risk score based on the signature was established, and cox regression was used to test whether it was an independent prognostic factor the functional enrichment of autophagy-related lncrnas was visualized using gene ontology and kyoto encyclopedia of genes and genomes ten prognostic autophagy-related lncrnas ac0273072, ac0685803, al1387561, cd27-as1, eif3j-dt, linc01011, linc01063, linc02381, ac0738963, and snhg16 were identified to be significantly different, which made up an autophagy-related lncrna signature the signature divided patients with colon adenocarcinoma into the low-risk group and the high-risk group a risk score based on the signature was a significantly independent factor for the patients with colon adenocarcinoma hr = 1088, 95%ci = 1057 - 1120; &lt;i&gt;p&lt;/i&gt; &lt; 0001 additionally, the ten lncrnas were significantly enriched in autophagy process, metabolism, and tumor classical pathways in conclusion, the ten autophagy-related lncrnas and their signature might be molecular biomarkers and therapeutic targets for the patients with colon adenocarcinoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107342</th>\n",
       "      <td>kir genes and patterns given by the a priori algorithm: immunity for haematological malignancies killer-cell immunoglobulin-like receptors kirs are membrane proteins expressed by cells of innate and adaptive immunity the kir system consists of 17 genes and 614 alleles arranged into different haplotypes kir genes modulate susceptibility to haematological malignancies, viral infections, and autoimmune diseases molecular epidemiology studies rely on traditional statistical methods to identify associations between kir genes and disease we have previously described our results by applying support vector machines to identify associations between kir genes and disease however, rules specifying which haplotypes are associated with greater susceptibility to malignancies are lacking here we present the results of our investigation into the rules governing haematological malignancy susceptibility we have studied the different haplotypic combinations of 17 kir genes in 300 healthy individuals and 43 patients with haematological malignancies 25 with leukaemia and 18 with lymphomas we compare two machine learning algorithms against traditional statistical analysis and show that the a priori algorithm is capable of discovering patterns unrevealed by previous algorithms and statistical approaches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154334</th>\n",
       "      <td>diagnosis of the small round blue cell tumors using multiplex polymerase chain reaction the small round blue cell tumors of childhood, which include neuroblastoma, rhabdomyosarcoma, non-hodgkins lymphoma, and the ewings family of tumors, are so called because of their similar appearance on routine histology using cdna microarray gene expression profiles and artificial neural networks anns, we previously identified 93 genes capable of diagnosing these cancers using a subset of these, together with some additional genes total 39, we developed a multiplex polymerase chain reaction pcr assay to diagnose these cancer types blinded testing of 96 new samples 26 ewings family of tumors, 29 rhabdomyosarcomas, 24 neuroblastomas, and 17 lymphomas using anns in a complete leave-one-out analysis demonstrated that all except one sample were accurately diagnosed as their respective category moreover, using an ann-based gene minimization strategy in a separate analysis, we found that the top 31 genes could correctly diagnose all 96 tumors our results suggest that this molecular test based on a multiplex pcr reaction may assist the physician in the rapid confirmation of the diagnosis of these cancers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54427</th>\n",
       "      <td>development of a long non-coding rna signature for prediction of response to neoadjuvant chemoradiotherapy in locally advanced rectal adenocarcinoma standard treatment for locally advanced rectal adenocarcinoma larc includes a combination of chemotherapy with pyrimidine analogues, such as capecitabine, and radiation therapy, followed by surgery currently no clinically useful genomic predictors of benefit from neoadjuvant chemoradiotherapy ncrt exist for larc in this study we assessed the expression of 8,127 long noncoding rnas lncrnas, poorly studied in larc, to infer their ability in classifying patientspathological complete response pcr we collected and analyzed, using lncrna-specific agilent microarrays a consecutive series of 61 larc cases undergoing ncrt potential lncrna predictors in responders and non-responders to ncrt were identified with lasso regression, and a model was optimized using k-fold cross-validation after selection of the three most informative lncrna 11 lncrnas were differentially expressed with false discovery rate &lt; 001 between responders and non-responders to nact we identified lnc-klf7-1, lnc-mab21l2-1, and linc00324 as the most promising variable subset for classification building overall sensitivity and specificity were 091 and 094 respectively, with an auc of our roc curve = 093 our study shows for the first time that lncrnas can accurately predict response in larc undergoing ncrt our three-lncrna based signature must be independently validated and further analyses must be conducted to fully understand the biological role of the identified signature, but our results suggest lncrnas may be an ideal biomarker for response prediction in the studied setting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110603</th>\n",
       "      <td>application of artificial neural networks to link genetic and environmental factors to dna methylation in colorectal cancer we applied artificial neural networks anns to understand the connections among polymorphisms of genes involved in folate metabolism, clinico-pathological features and promoter methylation levels of mlh1, apc, cdkn2aink4a, mgmt and rassf1a in 83 sporadic colorectal cancer crc tissues, and to link dietary and lifestyle factors with gene promoter methylation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71878</th>\n",
       "      <td>application of single-nucleotide polymorphisms in the diagnosis of autism spectrum disorders: a preliminary study with artificial neural networks autism spectrum disorder asd includes different neurodevelopmental disorders characterized by deficits in social communication, and restricted, repetitive patterns of behavior, interests or activities based on the importance of early diagnosis for effective therapeutic intervention, several strategies have been employed for detection of the disorder the artificial neural network ann as a type of machine learning method is a common strategy in the current study, we extracted genomic data for 487 asd patients and 455 healthy individuals all individuals were genotyped in certain single-nucleotide polymorphisms within retinoic acid-related orphan receptor alpha rora, gamma-aminobutyric acid type a receptor beta3 subunit gabrb3, synaptosomal-associated protein 25 snap25 and metabotropic glutamate receptor 7 grm7 genes subsequently, we used the keras package to create and train the ann model for cross-validation, samples were divided into ten folds in the training process, initially, the first fold was preserved for validation and the other folds were used to train the model the validation fold was then used to evaluate model performance the k-fold cross-validation method was used to ensure model generalizability and to prevent overfitting local interpretable model-agnostic explanations lime were applied to explain model predictions at the data sample level the output of loss function was evaluated in the training process for each fold in the k-fold cross-validation model finally, the number of losses was reduced to less than 06 after 200 epochs except in two cases the accuracy, sensitivity and specificity of our model were 7367%, 8275% and 6395%, respectively the area under the curve auc was 8059 consequently, in the current study, we propose an ann-based method for differentiating asd status from healthy status with adequate power</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52173</th>\n",
       "      <td>a multitask multiple kernel learning formulation for discriminating early- and late-stage cancers genomic information is increasingly being used in diagnosis, prognosis and treatment of cancer the severity of the disease is usually measured by the tumor stage therefore, identifying pathways playing an important role in progression of the disease stage is of great interest given that there are similarities in the underlying mechanisms of different cancers, in addition to the considerable correlation in the genomic data, there is a need for machine learning methods that can take these aspects of genomic data into account furthermore, using machine learning for studying multiple cancer cohorts together with a collection of molecular pathways creates an opportunity for knowledge extraction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6434</th>\n",
       "      <td>identification of biomarkers for acute leukemia via machine learning-based stemness index traditional methods to understand leukemia stem cell lscs biological characteristics include constructing lsc-like cells and mouse models by transgenic or knock-in methods however, there are some potential pitfalls in using this method, such as retroviral insertion mutagenesis, non-physiological level gene expression, non-physiological expansion, and difficulty to construct the mrnasi index for each sample of the cancer genome atlas tcga could avoid these potential pitfalls by machine learning in this work, we aimed to construct a network of lsc genes utilizing the mrnasi first, mrnasi value was analyzed with expressions distributions, survival analysis, age, and gender in acute myeloid leukemia aml samples then, we used the weighted gene co-expression network analysis wgcna to construct modules of stemness genes the correlation of the lsc genes transcription and interplay among lsc proteins was analyzed we performed functional and pathway enrichment analysis to annotate stemness genes survival analysis further identified prognostic biomarkers by clinical data of tcga and the gene expression omnibus geo database we found that the result of mrnasi overall survival is not significant, which may be due to the heterogeneity of aml in the stage of myeloid differentiation, french-american-british fab classification systems enrichment analysis indicated that the stemness genes were biologically clustered as a group and mainly associated with cell cycle and mitosis moreover, 10 key genes snrnp40, rfc4, rfc5, cdc6, hspe1, pa2g4, snap23p, dars2, mis18a, and hprt1 were screened by survival analysis with the data from tcga and geo among them, rfc4 and rfc5 were the distinguished biomarkers for their double-validated prognostic value in both databases additionally, the expression of rfc4 and rfc5 had the same trend as mrnasi score in fab subtypes in conclusion, our result demonstrated that mrnasi based lsc-related genes were found to have strong interactions as a cluster these genes, especially rfc4 and rfc5, could be the therapeutic targets for inhibiting the stemness characteristics of aml this work is also a comprehensive pipeline for future cancer stem cell studies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22329</th>\n",
       "      <td>placental dna methylation profiles in opioid-exposed pregnancies and associations with the neonatal opioid withdrawal syndrome opioid abuse during pregnancy can result in neonatal opioid withdrawal syndrome nows we investigated genome-wide methylation analyses of 96 placental tissue samples, including 32 prenatally opioid-exposed infants with nows who needed therapy +opioids/+nows, 32 prenatally opioid-exposed infants with nows who did not require treatment +opioids/-nows, and 32 prenatally unexposed controls -opioids/-nows, control statistics, bioinformatics, artificial intelligence ai, including deep learning dl, and ingenuity pathway analyses ipa were performed we identified 17 dysregulated pathways thought to be important in the pathophysiology of nows and reported accurate ai prediction of nows diagnoses the dl had an auc 95% ci =098 095-10 with a sensitivity and specificity of 100% for distinguishing nows from the +opioids/-nows group and aucs 95% ci =100 10-10 with a sensitivity and specificity of 100% for distinguishing nows versus control and + opioids/-nows group versus controls this study provides strong evidence of methylation dysregulation of placental tissue in nows development</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>large-scale machine-learning-based phenotyping significantly improves genomic discovery for optic nerve head morphology genome-wide association studies gwass require accurate cohort phenotyping, but expert labeling can be costly, time intensive, and variable here, we develop a machine learning ml model to predict glaucomatous optic nerve head features from color fundus photographs we used the model to predict vertical cup-to-disc ratio vcdr, a diagnostic parameter and cardinal endophenotype for glaucoma, in 65,680 europeans in the uk biobank ukb a gwas of ml-based vcdr identified 299 independent genome-wide significant gws; p ≤ 5 × 10&lt;sup&gt;-8&lt;/sup&gt; hits in 156 loci the ml-based gwas replicated 62 of 65 gws loci from a recent vcdr gwas in the ukb for which two ophthalmologists manually labeled images for 67,040 europeans the ml-based gwas also identified 93 novel loci, significantly expanding our understanding of the genetic etiologies of glaucoma and vcdr pathway analyses support the biological significance of the novel hits to vcdr: select loci near genes involved in neuronal and synaptic biology or harboring variants are known to cause severe mendelian ophthalmic disease finally, the ml-based gwas results significantly improve polygenic prediction of vcdr and primary open-angle glaucoma in the independent epic-norfolk cohort</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45661</th>\n",
       "      <td>combination of variations in inflammation- and endoplasmic reticulum-associated genes as putative biomarker for bevacizumab response in kras wild-type colorectal cancer chemotherapy combined with the angiogenesis inhibitor bevacizumab bvz is approved as a first-line treatment in metastatic colorectal cancer mcrc limited clinical benefit underpins the need for improved understanding of resistance mechanisms and the elucidation of novel predictive biomarkers we assessed germline single-nucleotide polymorphisms snps in 180 mcrc patients angiopredict apd cohort treated with combined bvz + chemotherapy and investigated previously reported predictive snps we further employed a machine learning approach to identify novel associations in the apd cohort il8 rs4073 any a carriers, compared to tt carriers, were associated with worse progression-free survival pfs hr = 151, 95% ci:103-222, p-value = 0037 and tbk1 rs7486100 tt carriers, compared to any a carriers, were associated with worse pfs in kras wild-type wt patients hr = 194, 95% ci:104-361, p-value = 0037, replicating previous findings machine learning identified novel associations in genes encoding the inflammasome protein nlrp1 and the er protein sarcalumenin srl a negative association between pfs and carriers of any a at nlrp1 rs12150220 and aa for srl rs13334970 in apd kras wild-type patients hr = 444, 95% ci:123-1613, p-value = 0005, which validated in two independent clinical cohorts involving bvz, mavericc and tribe our findings highlight a key role for inflammation and er signalling underpinning bvz + chemotherapy responsiveness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "25404                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   machine learning-driven and smartphone-based fluorescence detection for crispr diagnostic of sars-cov-2 rapid, accurate, and low-cost detection of sars-cov-2 is crucial to contain the transmission of covid-19 here, we present a cost-effective smartphone-based device coupled with machine learning-driven software that evaluates the fluorescence signals of the crispr diagnostic of sars-cov-2 the device consists of a three-dimensional 3d-printed housing and low-cost optic components that allow excitation of fluorescent reporters and selective transmission of the fluorescence emission to a smartphone custom software equipped with a binary classification model has been developed to quantify the acquired fluorescence images and determine the presence of the virus our detection system has a limit of detection lod of 625 rna copies/μl on laboratory samples and produces a test accuracy of 95% and sensitivity of 97% on 96 nasopharyngeal swab samples with transmissible viral loads our quantitative fluorescence score shows a strong correlation with the quantitative reverse transcription polymerase chain reaction rt-qpcr ct values, offering valuable information of the viral load and, therefore, presenting an important advantage over nonquantitative readouts   \n",
       "86153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          predicting clinical outcomes in colorectal cancer using machine learning using gene markers and other patient features to predict clinical outcomes plays a vital role in enhancing clinical decision making and improving prognostic accuracy this work uses a large set of colorectal cancer patient data to train predictive models using machine learning methods such as random forest, general linear model, and neural network for clinically relevant outcomes including disease free survival, survival, radio-chemotherapy response rct-r and relapse the most successful predictive models were created for dichotomous outcomes like relapse and rct-r with accuracies of 071 and 070 on blinded test data respectively the best prediction models regarding overall survival and disease-free survival had c-index scores of 086 and 076 respectively these models could be used in the future to aid a decision for or against chemotherapy and improve survival prognosis we propose that future work should focus on creating reusable frameworks and infrastructure for training and delivering predictive models to physicians, so that they could be readily applied to other diseases in practice and be continuously developed integrating new data   \n",
       "145485                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          recursive fuzzy granulation for gene subsets extraction and cancer classification a typical microarray gene expression dataset is usually both extremely sparse and imbalanced to select multiple highly informative gene subsets for cancer classification and diagnosis, a new fuzzy granular support vector machine---recursive feature elimination algorithm fgsvm-rfe is designed in this paper as a hybrid algorithm of statistical learning, fuzzy clustering, and granular computing, the fgsvm-rfe separately eliminates irrelevant, redundant, or noisy genes in different granules at different stages and selects highly informative genes with potentially different biological functions in balance empirical studies on three public datasets demonstrate that the fgsvm-rfe outperforms state-of-the-art approaches moreover, the fgsvm-rfe can extract multiple gene subsets on each of which a classifier can be modeled with 100% accuracy specifically, the independent testing accuracy for the prostate cancer dataset is significantly improved the previous best result is 86% with 16 genes and our best result is 100% with only eight genes the identified genes are annotated by onto-express to be biologically meaningful   \n",
       "20257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               classification of molecular subtypes of high-grade serous ovarian cancer by maldi-imaging despite the correlation of clinical outcome and molecular subtypes of high-grade serous ovarian cancer hgsoc, contemporary gene expression signatures have not been implemented in clinical practice to stratify patients for targeted therapy hence, we aimed to examine the potential of unsupervised matrix-assisted laser desorption/ionization imaging mass spectrometry maldi-ims to stratify patients who might benefit from targeted therapeutic strategies molecular subtyping of paraffin-embedded tissue samples from 279 hgsoc patients was performed by nanostring analysis ground truth labeling next, we applied maldi-ims paired with machine-learning algorithms to identify distinct mass profiles on the same paraffin-embedded tissue sections and distinguish hgsoc subtypes by proteomic signature finally, we devised a novel approach to annotate spectra of stromal origin we elucidated a maldi-derived proteomic signature 135 peptides able to classify hgsoc subtypes random forest classifiers achieved an area under the curve auc of 0983 furthermore, we demonstrated that the exclusion of stroma-associated spectra provides tangible improvements to classification quality auc = 0988 moreover, novel maldi-based stroma annotation achieved near-perfect classifications auc = 0999 here, we present a concept integrating maldi-ims with machine-learning algorithms to classify patients according to distinct molecular subtypes of hgsoc this has great potential to assign patients for personalized treatment   \n",
       "83442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                early diagnosis of alzheimers disease based on resting-state brain networks and deep learning computerized healthcare has undergone rapid development thanks to the advances in medical imaging and machine learning technologies especially, recent progress on deep learning opens a new era for multimedia based clinical decision support in this paper, we use deep learning with brain network and clinical relevant text information to make early diagnosis of alzheimers disease ad the clinical relevant text information includes age, gender, and apoe gene of the subject the brain network is constructed by computing the functional connectivity of brain regions using resting-state functional magnetic resonance imaging r-fmri data a targeted autoencoder network is built to distinguish normal aging from mild cognitive impairment, an early stage of ad the proposed method reveals discriminative brain network features effectively and provides a reliable classifier for ad detection compared to traditional classifiers based on r-fmri time series data, about 3121 percent improvement of the prediction accuracy is achieved by the proposed deep learning method, and the standard deviation reduces by 5123 percent in the best case that means our prediction model is more stable and reliable compared to the traditional methods our work excavates deep learnings advantages of classifying high-dimensional multimedia data in medical services, and could help predict and prevent ad at an early stage   \n",
       "110352                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 multi-class bcga-elm based classifier that identifies biomarkers associated with hallmarks of cancer traditional cancer treatments have centered on cytotoxic drugs and general purpose chemotherapy that may not be tailored to treat specific cancers identification of molecular markers that are related to different types of cancers might lead to discovery of drugs that are patient and disease specific this study aims to use microarray gene expression cancer data to identify biomarkers that are indicative of different types of cancers our aim is to provide a multi-class cancer classifier that can simultaneously differentiate between cancers and identify type-specific biomarkers, through the application of the binary coded genetic algorithm bcga and a neural network based extreme learning machine elm algorithm   \n",
       "13461                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          the application of artificial intelligence methods to gene expression data for differentiation of uncomplicated and complicated appendicitis in children and adolescents - a proof of concept study genome wide gene expression analysis has revealed hints for independent immunological pathways underlying the pathophysiologies of phlegmonous pa and gangrenous appendicitis ga methods of artificial intelligence ai have successfully been applied to routine laboratory and sonographic parameters for differentiation of the inflammatory manifestations in this study we aimed to apply ai methods to gene expression data to provide evidence for feasibility   \n",
       "165383  assessing optimal neural network architecture for identifying disease-associated multi-marker genotypes using a permutation test, and application to calpain 10 polymorphisms associated with diabetes biallelic markers, such as single nucleotide polymorphisms snps, provide greater information for localising disease loci when treated as multilocus haplotypes, but often haplotypes are not immediately available from multilocus genotypes in case-control studies an artificial neural network allows investigation of association between disease phenotype and tightly linked markers without requiring haplotype phase and without modelling any evolutionary history for the disease-related haplotypes the network assesses whether marker haplotypes differ between cases and controls to the extent that classification of disease status based on multi-marker genotypes is achievable the network is trained to recognise affection status based on supplied marker genotypes, and then for each multi-marker genotype it produces outputs which aim to approximate the associated affection status next, the genotypes are permuted relative to affection status to produce many random datasets and the process of training and recording of outputs is repeated the extent to which the ability to predict affection for the real dataset exceeds that for the random datasets measures the statistical significance of the association between multi-marker genotype and affection this permutation test performs well with simulated case-control datasets, particularly when major gene effects are present we have explored the effects of systematically varying different network parameters in order to identify their optimal values we have applied the permutation test to 4 snps of the calpain 10 capn10 gene typed in a case-control sample of subjects with type 2 diabetes, impaired glucose tolerance, and controls we show that the neural network produces more highly significant evidence for association than do single marker tests corrected for the number of markers genotyped the use of a permutation test could potentially allow conditional analyses which could incorporate known risk factors alongside marker genotypes permuting only the marker genotypes relative to affection status and these risk factors would allow the contribution of the markers to disease risk to be independently assessed   \n",
       "2149                                                                                                                                                                                                                                                                                                                                                                             identification of prognostic biomarker candidates associated with melanoma using high-dimensional genomic data survival of patients with metastatic melanoma varies widely melanoma is a highly proliferative, chemo-resistant disease with the recent availability of immunotherapies such as checkpoint inhibitors, durable response rates have improved but are often still limited to 2-3 years response rates to treatment range from 30 to 45% with combination therapy however no improvement in overall survival is frequently observed of the available therapies, many have targeted the brafv600e mutation that results in abnormal mapk pathway activation which is important for regulating cell proliferation immune checkpoint inhibitors such as anti-pd-1 and anti-pd-l1 offer better success but response rates are still low identifying biomarkers to better target those who will respond and identify the right combination of treatment is the best approach in this study, we utilize data from the cancer cell line encyclopedia ccle, including 62 samples, to examine features of gene expression 19k+ and copy number 20k+ in the melanoma cell lines we perform a clustering analysis on the feature set to assess genetically similarity among the cell lines we then discover which specific genes and combinations thereof maximize cluster density we design a feature selection approach for high-dimensional datasets that integrates multiple disparate machine learning techniques into one cohesive pipeline our approach provides a small subset of genes that can accurately distinguish between the clusters of melanoma cell lines across multiple types of classifiers in particular, we find only the 15 highest ranked genes among the original 19 k are necessary to achieve perfect or near-perfect test split classification performance of these 15 genes, some are known to be linked to melanoma or other cancer progressions, while others have not previously been linked to melanoma and are of interest for further examination   \n",
       "37927                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  development of prognostic indicator based on autophagy-related lncrna analysis in colon adenocarcinoma there were no systematic researches about autophagy-related long noncoding rna lncrna signatures to predict the survival of patients with colon adenocarcinoma it was necessary to set up corresponding autophagy-related lncrna signatures the expression profiles of lncrnas which contained 480 colon adenocarcinoma samples were obtained from the cancer genome atlas tcga database the coexpression network of lncrnas and autophagy-related genes was utilized to select autophagy-related lncrnas the lncrnas were further screened using univariate cox regression in addition, lasso regression and multivariate cox regression were used to develop an autophagy-related lncrna signature a risk score based on the signature was established, and cox regression was used to test whether it was an independent prognostic factor the functional enrichment of autophagy-related lncrnas was visualized using gene ontology and kyoto encyclopedia of genes and genomes ten prognostic autophagy-related lncrnas ac0273072, ac0685803, al1387561, cd27-as1, eif3j-dt, linc01011, linc01063, linc02381, ac0738963, and snhg16 were identified to be significantly different, which made up an autophagy-related lncrna signature the signature divided patients with colon adenocarcinoma into the low-risk group and the high-risk group a risk score based on the signature was a significantly independent factor for the patients with colon adenocarcinoma hr = 1088, 95%ci = 1057 - 1120; <i>p</i> < 0001 additionally, the ten lncrnas were significantly enriched in autophagy process, metabolism, and tumor classical pathways in conclusion, the ten autophagy-related lncrnas and their signature might be molecular biomarkers and therapeutic targets for the patients with colon adenocarcinoma   \n",
       "107342                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           kir genes and patterns given by the a priori algorithm: immunity for haematological malignancies killer-cell immunoglobulin-like receptors kirs are membrane proteins expressed by cells of innate and adaptive immunity the kir system consists of 17 genes and 614 alleles arranged into different haplotypes kir genes modulate susceptibility to haematological malignancies, viral infections, and autoimmune diseases molecular epidemiology studies rely on traditional statistical methods to identify associations between kir genes and disease we have previously described our results by applying support vector machines to identify associations between kir genes and disease however, rules specifying which haplotypes are associated with greater susceptibility to malignancies are lacking here we present the results of our investigation into the rules governing haematological malignancy susceptibility we have studied the different haplotypic combinations of 17 kir genes in 300 healthy individuals and 43 patients with haematological malignancies 25 with leukaemia and 18 with lymphomas we compare two machine learning algorithms against traditional statistical analysis and show that the a priori algorithm is capable of discovering patterns unrevealed by previous algorithms and statistical approaches    \n",
       "154334                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               diagnosis of the small round blue cell tumors using multiplex polymerase chain reaction the small round blue cell tumors of childhood, which include neuroblastoma, rhabdomyosarcoma, non-hodgkins lymphoma, and the ewings family of tumors, are so called because of their similar appearance on routine histology using cdna microarray gene expression profiles and artificial neural networks anns, we previously identified 93 genes capable of diagnosing these cancers using a subset of these, together with some additional genes total 39, we developed a multiplex polymerase chain reaction pcr assay to diagnose these cancer types blinded testing of 96 new samples 26 ewings family of tumors, 29 rhabdomyosarcomas, 24 neuroblastomas, and 17 lymphomas using anns in a complete leave-one-out analysis demonstrated that all except one sample were accurately diagnosed as their respective category moreover, using an ann-based gene minimization strategy in a separate analysis, we found that the top 31 genes could correctly diagnose all 96 tumors our results suggest that this molecular test based on a multiplex pcr reaction may assist the physician in the rapid confirmation of the diagnosis of these cancers   \n",
       "54427                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    development of a long non-coding rna signature for prediction of response to neoadjuvant chemoradiotherapy in locally advanced rectal adenocarcinoma standard treatment for locally advanced rectal adenocarcinoma larc includes a combination of chemotherapy with pyrimidine analogues, such as capecitabine, and radiation therapy, followed by surgery currently no clinically useful genomic predictors of benefit from neoadjuvant chemoradiotherapy ncrt exist for larc in this study we assessed the expression of 8,127 long noncoding rnas lncrnas, poorly studied in larc, to infer their ability in classifying patientspathological complete response pcr we collected and analyzed, using lncrna-specific agilent microarrays a consecutive series of 61 larc cases undergoing ncrt potential lncrna predictors in responders and non-responders to ncrt were identified with lasso regression, and a model was optimized using k-fold cross-validation after selection of the three most informative lncrna 11 lncrnas were differentially expressed with false discovery rate < 001 between responders and non-responders to nact we identified lnc-klf7-1, lnc-mab21l2-1, and linc00324 as the most promising variable subset for classification building overall sensitivity and specificity were 091 and 094 respectively, with an auc of our roc curve = 093 our study shows for the first time that lncrnas can accurately predict response in larc undergoing ncrt our three-lncrna based signature must be independently validated and further analyses must be conducted to fully understand the biological role of the identified signature, but our results suggest lncrnas may be an ideal biomarker for response prediction in the studied setting   \n",
       "110603                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                application of artificial neural networks to link genetic and environmental factors to dna methylation in colorectal cancer we applied artificial neural networks anns to understand the connections among polymorphisms of genes involved in folate metabolism, clinico-pathological features and promoter methylation levels of mlh1, apc, cdkn2aink4a, mgmt and rassf1a in 83 sporadic colorectal cancer crc tissues, and to link dietary and lifestyle factors with gene promoter methylation   \n",
       "71878                                                                                                                                                                                                                                                                                                                                                                              application of single-nucleotide polymorphisms in the diagnosis of autism spectrum disorders: a preliminary study with artificial neural networks autism spectrum disorder asd includes different neurodevelopmental disorders characterized by deficits in social communication, and restricted, repetitive patterns of behavior, interests or activities based on the importance of early diagnosis for effective therapeutic intervention, several strategies have been employed for detection of the disorder the artificial neural network ann as a type of machine learning method is a common strategy in the current study, we extracted genomic data for 487 asd patients and 455 healthy individuals all individuals were genotyped in certain single-nucleotide polymorphisms within retinoic acid-related orphan receptor alpha rora, gamma-aminobutyric acid type a receptor beta3 subunit gabrb3, synaptosomal-associated protein 25 snap25 and metabotropic glutamate receptor 7 grm7 genes subsequently, we used the keras package to create and train the ann model for cross-validation, samples were divided into ten folds in the training process, initially, the first fold was preserved for validation and the other folds were used to train the model the validation fold was then used to evaluate model performance the k-fold cross-validation method was used to ensure model generalizability and to prevent overfitting local interpretable model-agnostic explanations lime were applied to explain model predictions at the data sample level the output of loss function was evaluated in the training process for each fold in the k-fold cross-validation model finally, the number of losses was reduced to less than 06 after 200 epochs except in two cases the accuracy, sensitivity and specificity of our model were 7367%, 8275% and 6395%, respectively the area under the curve auc was 8059 consequently, in the current study, we propose an ann-based method for differentiating asd status from healthy status with adequate power   \n",
       "52173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      a multitask multiple kernel learning formulation for discriminating early- and late-stage cancers genomic information is increasingly being used in diagnosis, prognosis and treatment of cancer the severity of the disease is usually measured by the tumor stage therefore, identifying pathways playing an important role in progression of the disease stage is of great interest given that there are similarities in the underlying mechanisms of different cancers, in addition to the considerable correlation in the genomic data, there is a need for machine learning methods that can take these aspects of genomic data into account furthermore, using machine learning for studying multiple cancer cohorts together with a collection of molecular pathways creates an opportunity for knowledge extraction   \n",
       "6434                                                                                       identification of biomarkers for acute leukemia via machine learning-based stemness index traditional methods to understand leukemia stem cell lscs biological characteristics include constructing lsc-like cells and mouse models by transgenic or knock-in methods however, there are some potential pitfalls in using this method, such as retroviral insertion mutagenesis, non-physiological level gene expression, non-physiological expansion, and difficulty to construct the mrnasi index for each sample of the cancer genome atlas tcga could avoid these potential pitfalls by machine learning in this work, we aimed to construct a network of lsc genes utilizing the mrnasi first, mrnasi value was analyzed with expressions distributions, survival analysis, age, and gender in acute myeloid leukemia aml samples then, we used the weighted gene co-expression network analysis wgcna to construct modules of stemness genes the correlation of the lsc genes transcription and interplay among lsc proteins was analyzed we performed functional and pathway enrichment analysis to annotate stemness genes survival analysis further identified prognostic biomarkers by clinical data of tcga and the gene expression omnibus geo database we found that the result of mrnasi overall survival is not significant, which may be due to the heterogeneity of aml in the stage of myeloid differentiation, french-american-british fab classification systems enrichment analysis indicated that the stemness genes were biologically clustered as a group and mainly associated with cell cycle and mitosis moreover, 10 key genes snrnp40, rfc4, rfc5, cdc6, hspe1, pa2g4, snap23p, dars2, mis18a, and hprt1 were screened by survival analysis with the data from tcga and geo among them, rfc4 and rfc5 were the distinguished biomarkers for their double-validated prognostic value in both databases additionally, the expression of rfc4 and rfc5 had the same trend as mrnasi score in fab subtypes in conclusion, our result demonstrated that mrnasi based lsc-related genes were found to have strong interactions as a cluster these genes, especially rfc4 and rfc5, could be the therapeutic targets for inhibiting the stemness characteristics of aml this work is also a comprehensive pipeline for future cancer stem cell studies   \n",
       "22329                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        placental dna methylation profiles in opioid-exposed pregnancies and associations with the neonatal opioid withdrawal syndrome opioid abuse during pregnancy can result in neonatal opioid withdrawal syndrome nows we investigated genome-wide methylation analyses of 96 placental tissue samples, including 32 prenatally opioid-exposed infants with nows who needed therapy +opioids/+nows, 32 prenatally opioid-exposed infants with nows who did not require treatment +opioids/-nows, and 32 prenatally unexposed controls -opioids/-nows, control statistics, bioinformatics, artificial intelligence ai, including deep learning dl, and ingenuity pathway analyses ipa were performed we identified 17 dysregulated pathways thought to be important in the pathophysiology of nows and reported accurate ai prediction of nows diagnoses the dl had an auc 95% ci =098 095-10 with a sensitivity and specificity of 100% for distinguishing nows from the +opioids/-nows group and aucs 95% ci =100 10-10 with a sensitivity and specificity of 100% for distinguishing nows versus control and + opioids/-nows group versus controls this study provides strong evidence of methylation dysregulation of placental tissue in nows development   \n",
       "14018                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               large-scale machine-learning-based phenotyping significantly improves genomic discovery for optic nerve head morphology genome-wide association studies gwass require accurate cohort phenotyping, but expert labeling can be costly, time intensive, and variable here, we develop a machine learning ml model to predict glaucomatous optic nerve head features from color fundus photographs we used the model to predict vertical cup-to-disc ratio vcdr, a diagnostic parameter and cardinal endophenotype for glaucoma, in 65,680 europeans in the uk biobank ukb a gwas of ml-based vcdr identified 299 independent genome-wide significant gws; p ≤ 5 × 10<sup>-8</sup> hits in 156 loci the ml-based gwas replicated 62 of 65 gws loci from a recent vcdr gwas in the ukb for which two ophthalmologists manually labeled images for 67,040 europeans the ml-based gwas also identified 93 novel loci, significantly expanding our understanding of the genetic etiologies of glaucoma and vcdr pathway analyses support the biological significance of the novel hits to vcdr: select loci near genes involved in neuronal and synaptic biology or harboring variants are known to cause severe mendelian ophthalmic disease finally, the ml-based gwas results significantly improve polygenic prediction of vcdr and primary open-angle glaucoma in the independent epic-norfolk cohort   \n",
       "45661                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          combination of variations in inflammation- and endoplasmic reticulum-associated genes as putative biomarker for bevacizumab response in kras wild-type colorectal cancer chemotherapy combined with the angiogenesis inhibitor bevacizumab bvz is approved as a first-line treatment in metastatic colorectal cancer mcrc limited clinical benefit underpins the need for improved understanding of resistance mechanisms and the elucidation of novel predictive biomarkers we assessed germline single-nucleotide polymorphisms snps in 180 mcrc patients angiopredict apd cohort treated with combined bvz + chemotherapy and investigated previously reported predictive snps we further employed a machine learning approach to identify novel associations in the apd cohort il8 rs4073 any a carriers, compared to tt carriers, were associated with worse progression-free survival pfs hr = 151, 95% ci:103-222, p-value = 0037 and tbk1 rs7486100 tt carriers, compared to any a carriers, were associated with worse pfs in kras wild-type wt patients hr = 194, 95% ci:104-361, p-value = 0037, replicating previous findings machine learning identified novel associations in genes encoding the inflammasome protein nlrp1 and the er protein sarcalumenin srl a negative association between pfs and carriers of any a at nlrp1 rs12150220 and aa for srl rs13334970 in apd kras wild-type patients hr = 444, 95% ci:123-1613, p-value = 0005, which validated in two independent clinical cohorts involving bvz, mavericc and tribe our findings highlight a key role for inflammation and er signalling underpinning bvz + chemotherapy responsiveness   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "25404        0       1        0         0       0        0        0        0   \n",
       "86153        0       0        0         0       0        0        0        0   \n",
       "145485       0       0        0         0       0        0        0        0   \n",
       "20257        0       0        0         0       0        0        0        0   \n",
       "83442        0       0        1         0       0        0        0        0   \n",
       "110352       0       0        0         0       0        0        0        0   \n",
       "13461        0       0        0         0       0        0        0        0   \n",
       "165383       0       0        0         0       0        0        0        0   \n",
       "2149         0       0        0         0       0        0        0        0   \n",
       "37927        0       0        0         0       0        0        0        0   \n",
       "107342       0       0        0         0       0        0        0        0   \n",
       "154334       0       0        0         0       0        0        0        0   \n",
       "54427        0       0        0         0       0        0        0        0   \n",
       "110603       0       0        0         0       0        0        0        0   \n",
       "71878        0       0        0         0       0        0        0        0   \n",
       "52173        0       0        0         0       0        0        0        0   \n",
       "6434         0       0        0         0       0        0        0        0   \n",
       "22329        0       0        0         0       0        0        0        0   \n",
       "14018        0       0        0         0       0        0        0        0   \n",
       "45661        0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text mamm_text endo_text derm_text gene_text  \n",
       "25404           0        0         0         0         0         1  \n",
       "86153           0        0         0         0         0         1  \n",
       "145485          0        0         0         0         0         1  \n",
       "20257           1        0         0         0         0         1  \n",
       "83442           0        0         0         0         0         1  \n",
       "110352          0        0         0         0         0         1  \n",
       "13461           0        0         0         0         0         1  \n",
       "165383          0        0         0         0         0         1  \n",
       "2149            0        0         0         0         0         1  \n",
       "37927           0        0         0         0         0         1  \n",
       "107342          0        0         0         0         0         1  \n",
       "154334          1        0         0         0         0         1  \n",
       "54427           0        0         0         0         0         1  \n",
       "110603          0        0         0         0         0         1  \n",
       "71878           0        0         0         0         0         1  \n",
       "52173           0        0         0         0         0         1  \n",
       "6434            0        0         0         0         0         1  \n",
       "22329           1        0         0         0         0         1  \n",
       "14018           0        1         0         0         0         1  \n",
       "45661           0        0         0         0         0         1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['gene_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4da41bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31589, '1': 2590})\n"
     ]
    }
   ],
   "source": [
    "## PROTEINOMICS/BIOMARKERS\n",
    "\n",
    "## text\n",
    "text = ['proteinomic', 'immunoglob', 'cytokine', 'biomarker', 'tumor marker', 'tumour marker', 'inflammatory marker',\n",
    "       'peptide', 'interferon', 'laboratory test', 'blood test']\n",
    "\n",
    "feat['bio_text'] = np.where(groups['text'].str.contains('serum marker'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['bio_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['bio_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['bio_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311dfd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33668, '1': 511})\n"
     ]
    }
   ],
   "source": [
    "## NATURAL LANGUAGE PROCESSING\n",
    "\n",
    "## text\n",
    "feat['nlp_text'] = np.where(groups['text'].str.contains(\"natural language\"), \"1\", \"0\")\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['nlp_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30976541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31583, '1': 2596})\n"
     ]
    }
   ],
   "source": [
    "## EHR RECORDS\n",
    "\n",
    "## text\n",
    "text = ['electronic health', 'health record', 'electronic record', 'patient record', 'medical record',\n",
    "        'care record', 'patient registry', 'research registr', 'clinical note', 'patient note', 'patient data',\n",
    "        'care data', 'care note', 'medical data', 'clinical data', 'hospital data', 'hospital note', 'admission note',\n",
    "        'physiological data', 'observational data', 'patient features', 'patient observations', 'patient history',\n",
    "        'medical history', 'care history']\n",
    "\n",
    "feat['ehr_text'] = np.where(groups['text'].str.contains('snomed'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['ehr_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['ehr_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['ehr_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a97eb46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33731, '1': 448})\n"
     ]
    }
   ],
   "source": [
    "## WEARABLE_SENSORS\n",
    "\n",
    "## text\n",
    "text = ['wearable sensor', 'smartwatch', 'internet of thing', 'sensor device', 'smart sensor', 'fitbit', 'fitness band',\n",
    "       'activity tracker', 'fitness tracker']\n",
    "\n",
    "feat['sensor_text'] = np.where(groups['text'].str.contains('smart watch'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['sensor_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['sensor_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['sensor_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebde5f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34139, '1': 40})\n"
     ]
    }
   ],
   "source": [
    "## PROM\n",
    "\n",
    "## text\n",
    "feat['prom_text'] = np.where(groups['text'].str.contains(\"patient reported outcome\"), \"1\", \"0\")\n",
    "feat['prom_text'] = np.where(groups['text'].str.contains(\"patient-reported outcome\"), \"1\", feat['prom_text'])\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['prom_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d94cdf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33877, '1': 302})\n"
     ]
    }
   ],
   "source": [
    "## SMARTPHONE\n",
    "\n",
    "## text\n",
    "feat['phone_text'] = np.where(groups['text'].str.contains(\"smartphone\"), \"1\", \"0\")\n",
    "feat['phone_text'] = np.where(groups['text'].str.contains(\"iphone\"), \"1\", feat['phone_text'])\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['phone_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f54fa182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34000, '1': 179})\n"
     ]
    }
   ],
   "source": [
    "#### DIGITAL STETH / sound\n",
    "\n",
    "## text\n",
    "text = ['heart sound', 'heart murmur', 'breath sound', 'auscultat', 'phonocardio', 'digital steth']\n",
    "\n",
    "feat['sound_text'] = np.where(groups['text'].str.contains('electronic steth'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['sound_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['sound_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"valve\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"murmur\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"resp\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"breath\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['sound_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b868b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "      <th>mamm_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>gene_text</th>\n",
       "      <th>bio_text</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>ehr_text</th>\n",
       "      <th>sensor_text</th>\n",
       "      <th>prom_text</th>\n",
       "      <th>phone_text</th>\n",
       "      <th>sound_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112843</th>\n",
       "      <td>automatic wheezing detection based on signal processing of spectrogram and back-propagation neural network wheezing is a common clinical symptom in patients with obstructive pulmonary diseases such as asthma automatic wheezing detection offers an objective and accurate means for identifying wheezing lung sounds, helping physicians in the diagnosis, long-term auscultation, and analysis of a patient with obstructive pulmonary disease this paper describes the design of a fast and high-performance wheeze recognition system a wheezing detection algorithm based on the order truncate average method and a back-propagation neural network bpnn is proposed some features are extracted from processed spectra to train a bpnn, and subsequently, test samples are analyzed by the trained bpnn to determine whether they are wheezing sounds the respiratory sounds of 58 volunteers 32 asthmatic and 26 healthy adults were recorded for training and testing experimental results of a qualitative analysis of wheeze recognition showed a high sensitivity of 0946 and a high specificity of 10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>rheumatic heart disease screening based on phonocardiogram rheumatic heart disease rhd is one of the most common causes of cardiovascular complications in developing countries it is a heart valve disease that typically affects children impaired heart valves stop functioning properly, resulting in a turbulent blood flow within the heart known as a murmur this murmur can be detected by cardiac auscultation however, the specificity and sensitivity of manual auscultation were reported to be low the other alternative is echocardiography, which is costly and requires a highly qualified physician given the diseases current high prevalence rate the latest reported rate in the study area ethiopia was 565%, there is a pressing need for early detection of the disease through mass screening programs this paper proposes an automated rhd screening approach using machine learning that can be used by non-medically trained persons outside of a clinical setting heart sound data was collected from 124 persons with rhd pwrhd and 46 healthy controls hc in ethiopia with an additional 81 hc records from an open-access dataset thirty-one distinct features were extracted to correctly represent rhd a support vector machine svm classifier was evaluated using two nested cross-validation approaches to quantitatively assess the generalization of the system to previously unseen subjects for regular nested 10-fold cross-validation, an f1-score of 960 ± 09%, recall 958 ± 15%, precision 962 ± 06% and a specificity of 960 ± 06% were achieved in the imbalanced nested cross-validation at a prevalence rate of 5%, it achieved an f1-score of 722 ± 08%, recall 923 ± 04%, precision 592 ± 36%, and a specificity of 948 ± 06% in screening tasks where the prevalence of the disease is small, recall is more important than precision the findings are encouraging, and the proposed screening tool can be inexpensive, easy to deploy, and has an excellent detection rate as a result, it has the potential for mass screening and early detection of rhd in developing countries</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76550</th>\n",
       "      <td>detection of osteoporosis from percussion responses using an electronic stethoscope and machine learning osteoporosis is an asymptomatic bone condition that affects a large proportion of the elderly population around the world, resulting in increased bone fragility and increased risk of fracture previous studies had shown that the vibroacoustic response of bone can indicate the quality of the bone condition therefore, the aim of the authorsproject is to develop a new method to exploit this phenomenon to improve detection of osteoporosis in individuals in this paper a method is described that uses a reflex hammer to exert testing stimuli on a patients tibia and an electronic stethoscope to acquire the impulse responses the signals are processed as mel frequency cepstrum coefficients and passed through an artificial neural network to determine the likelihood of osteoporosis from the tibias impulse responses following some discussions of the mechanism and procedure, this paper details the signal acquisition using the stethoscope and the subsequent signal processing and the statistical machine learning algorithm pilot testing with 12 patients achieved over 80% sensitivity with a false positive rate below 30% and accuracies in the region of 70% an extended dataset of 110 patients achieved an error rate of 30% with some room for improvement in the algorithm by using common clinical apparatus and strategic machine learning, this method might be suitable as a large population screening test for the early diagnosis of osteoporosis, thus avoiding secondary complications</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143756</th>\n",
       "      <td>support vectors machine-based identification of heart valve diseases using heart sounds taking into account that heart auscultation remains the dominant method for heart examination in the small health centers of the rural areas and generally in primary healthcare set-ups, the enhancement of this technique would aid significantly in the diagnosis of heart diseases in this context, the present paper initially surveys the research that has been conducted concerning the exploitation of heart sound signals for automated and semi-automated detection of pathological heart conditions then it proposes an automated diagnosis system for the identification of heart valve diseases based on the support vector machines svm classification of heart sounds this system performs a highly difficult diagnostic task even for experienced physicians, much more difficult than the basic diagnosis of the existence or not of a heart valve disease ie the classification of a heart sound as healthyor having a heart valve disease: it identifies the particular heart valve disease the system was applied in a representative global dataset of 198 heart sound signals, which come both from healthy medical cases and from cases suffering from the four most usual heart valve diseases: aortic stenosis as, aortic regurgitation ar, mitral stenosis ms and mitral regurgitation mr initially the heart sounds were successfully categorized using a svm classifier as normal or disease-related and then the corresponding murmurs in the unhealthy cases were classified as systolic or diastolic for the heart sounds diagnosed as having systolic murmur we used a svm classifier for performing a more detailed classification of them as having aortic stenosis or mitral regurgitation similarly for the heart sounds diagnosed as having diastolic murmur we used a svm classifier for classifying them as having aortic regurgitation or mitral stenosis alternative classifiers have been applied to the same data for comparison ie back-propagation neural networks, k-nearest-neighbour and naïve bayes classifiers, however their performance for the same diagnostic problems was lower than the svm classifiers proposed in this work</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98317</th>\n",
       "      <td>s1 and s2 heart sound recognition using deep neural networks this study focuses on the first s1 and second s2 heart sound recognition based only on acoustic characteristics; the assumptions of the individual durations of s1 and s2 and time intervals of s1-s2 and s2-s1 are not involved in the recognition process the main objective is to investigate whether reliable s1 and s2 recognition performance can still be attained under situations where the duration and interval information might not be accessible</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97085</th>\n",
       "      <td>a mobile platform for automated screening of asthma and chronic obstructive pulmonary disease chronic obstructive pulmonary disease copd and asthma each represent a large proportion of the global disease burden; copd is the third leading cause of death worldwide and asthma is one of the most prevalent chronic diseases, afflicting over 300 million people much of this burden is concentrated in the developing world, where patients lack access to physicians trained in the diagnosis of pulmonary disease as a result, these patients experience high rates of underdiagnosis and misdiagnosis to address this need, we present a mobile platform capable of screening for asthma and copd our solution is based on a mobile smart phone and consists of an electronic stethoscope, a peak flow meter application, and a patient questionnaire this data is combined with a machine learning algorithm to identify patients with asthma and copd to test and validate the design, we collected data from 119 healthy and sick participants using our custom mobile application and ran the analysis on a pc computer for comparison, all subjects were examined by an experienced pulmonologist using a full pulmonary testing laboratory employing a two-stage logistic regression model, our algorithms were first able to identify patients with either asthma or copd from the general population, yielding an roc curve with an auc of 095 then, after identifying these patients, our algorithm was able to distinguish between patients with asthma and patients with copd, yielding an roc curve with auc of 097 this work represents an important milestone towards creating a self-contained mobile phone-based platform that can be used for screening and diagnosis of pulmonary disease in many parts of the world</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18232</th>\n",
       "      <td>deep learning algorithm for automated cardiac murmur detection via a digital stethoscope platform background clinicians vary markedly in their ability to detect murmurs during cardiac auscultation and identify the underlying pathological features deep learning approaches have shown promise in medicine by transforming collected data into clinically significant information the objective of this research is to assess the performance of a deep learning algorithm to detect murmurs and clinically significant valvular heart disease using recordings from a commercial digital stethoscope platform methods and results using &gt;34 hours of previously acquired and annotated heart sound recordings, we trained a deep neural network to detect murmurs to test the algorithm, we enrolled 962 patients in a clinical study and collected recordings at the 4 primary auscultation locations ground truth was established using patient echocardiograms and annotations by 3 expert cardiologists algorithm performance for detecting murmurs has sensitivity and specificity of 763% and 914%, respectively by omitting softer murmurs, those with grade 1 intensity, sensitivity increased to 900% application of the algorithm at the appropriate anatomic auscultation location detected moderate-to-severe or greater aortic stenosis, with sensitivity of 932% and specificity of 860%, and moderate-to-severe or greater mitral regurgitation, with sensitivity of 662% and specificity of 946% conclusions the deep learning algorithms ability to detect murmurs and clinically significant aortic stenosis and mitral regurgitation is comparable to expert cardiologists based on the annotated subset of our database the findings suggest that such algorithms would have utility as front-line clinical support tools to aid clinicians in screening for cardiac murmurs caused by valvular heart disease registration url: https://clinicaltrialsgov; unique identifier: nct03458806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144410</th>\n",
       "      <td>feature extraction for murmur detection based on support vector regression of time-frequency representations this paper presents a nonlinear approach for time-frequency representations tfr data analysis, based on a statistical learning methodology - support vector regression svr, that being a nonlinear framework, matches recent findings on the underlying dynamics of cardiac mechanic activity and phonocardiographic pcg recordings the proposed methodology aims to model the estimated tfrs, and extract relevant features to perform classification between normal and pathologic pcg recordings with murmur modeling of tfr is done by means of svr, and the distance between regressions is calculated through dissimilarity measures based on dot product finally, a k-nn classifier is used for the classification stage, obtaining a validation performance of 9785%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>characterizing effortful swallows from healthy community dwelling adults across the lifespan using high-resolution cervical auscultation signals and mbsimp scores: a preliminary study there is growing enthusiasm to develop inexpensive, non-invasive, and portable methods that accurately assess swallowing and provide biofeedback during dysphagia treatment high-resolution cervical auscultation hrca, which uses acoustic and vibratory signals from non-invasive sensors attached to the anterior laryngeal framework during swallowing, is a novel method for quantifying swallowing physiology via advanced signal processing and machine learning techniques hrca has demonstrated potential as a dysphagia screening method and diagnostic adjunct to vfsss by determining swallowing safety, annotating swallow kinematic events, and classifying swallows between healthy participants and patients with a high degree of accuracy however, its feasibility as a non-invasive biofeedback system has not been explored this study investigated 1 whether hrca can accurately differentiate between non-effortful and effortful swallows; 2 whether differences exist in modified barium swallow impairment profile mbsimp scores #9, #11, #14 between non-effortful and effortful swallows we hypothesized that hrca would accurately classify non-effortful and effortful swallows and that differences in mbsimp scores would exist between the types of swallows we analyzed 247 thin liquid 3 ml command swallows 71 effortful to minimize variation from 36 healthy adults who underwent standardized vfsss with concurrent hrca results revealed differences p &lt; 005 in 9 hrca signal features between non-effortful and effortful swallows using hrca signal features as input, decision trees classified swallows with 76% accuracy, 76% sensitivity, and 77% specificity there were no differences in mbsimp component scores between non-effortful and effortful swallows while preliminary in nature, this study demonstrates the feasibility/promise of hrca as a biofeedback method for dysphagia treatment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49539</th>\n",
       "      <td>classifying dysphagic swallowing sounds with support vector machines swallowing sounds from cervical auscultation include information related to the swallowing function several studies have been conducted on the screening tests of dysphagia the literature shows a significant difference between the characteristics of swallowing sounds obtained from different subjects eg, healthy and dysphagic subjects; young and old adults these studies demonstrate the usefulness of swallowing sounds during dysphagic screening however, the degree of classification for dysphagia based on swallowing sounds has not been thoroughly studied in this study, we investigate the use of machine learning for classifying swallowing sounds into various types, such as normal swallowing or mild, moderate, and severe dysphagia in particular, swallowing sounds were recorded from patients with dysphagia support vector machines svms were trained using some features extracted from the obtained swallowing sounds moreover, the accuracy of the classification of swallowing sounds using the trained svms was evaluated via cross-validation techniques in the two-class scenario, wherein the swallowing sounds were divided into two categories viz normal and dysphagic subjects, the maximum f-measure was 789% in the four-class scenario, where the swallowing sounds were divided into four categories viz normal subject, and mild, moderate, and severe dysphagic subjects, the f-measure values for the classes were 656%, 531%, 511%, and 371%, respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163319</th>\n",
       "      <td>neural classification of lung sounds using wavelet coefficients electronic auscultation is an efficient technique to evaluate the condition of respiratory system using lung sounds as lung sound signals are non-stationary, the conventional method of frequency analysis is not highly successful in diagnostic classification this paper deals with a novel method of analysis of lung sound signals using wavelet transform, and classification using artificial neural network ann lung sound signals were decomposed into the frequency subbands using wavelet transform and a set of statistical features was extracted from the subbands to represent the distribution of wavelet coefficients an ann based system, trained using the resilient back propagation algorithm, was implemented to classify the lung sounds to one of the six categories: normal, wheeze, crackle, squawk, stridor, or rhonchus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41915</th>\n",
       "      <td>upper esophageal sphincter opening segmentation with convolutional recurrent neural networks in high resolution cervical auscultation upper esophageal sphincter is an important anatomical landmark of the swallowing process commonly observed through the kinematic analysis of radiographic examinations that are vulnerable to subjectivity and clinical feasibility issues acting as the doorway of esophagus, upper esophageal sphincter allows the transition of ingested materials from pharyngeal into esophageal stages of swallowing and a reduced duration of opening can lead to penetration/aspiration and/or pharyngeal residue therefore, in this study we consider a non-invasive high resolution cervical auscultation-based screening tool to approximate the human ratings of upper esophageal sphincter opening and closure swallows were collected from 116 patients and a deep neural network was trained to produce a mask that demarcates the duration of upper esophageal sphincter opening the proposed method achieved more than 90% accuracy and similar values of sensitivity and specificity when compared to human ratings even when tested over swallows from an independent clinical experiment moreover, the predicted opening and closure moments surprisingly fell within an inter-human comparable error of their human rated counterparts which demonstrates the clinical significance of high resolution cervical auscultation in replacing ionizing radiation-based evaluation of swallowing kinematics</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94892</th>\n",
       "      <td>recognition of normal-abnormal phonocardiographic signals using deep convolutional neural networks and mel-frequency spectral coefficients intensive care unit patients are heavily monitored, and several clinically-relevant parameters are routinely extracted from high resolution signals</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94281</th>\n",
       "      <td>intelligent phonocardiography for screening ventricular septal defect using time growing neural network this paper presents results of a study on the applicability of the intelligent phonocardiography in discriminating between ventricular spetal defect vsd and regurgitation of the atrioventricular valves an original machine learning method, based on the time growing neural network tgnn, is employed for classifying the phonocardiographic recordings collected from the pediatric referrals to a children hospital 90 individuals, 30 vsd, 30 with the valvular regurgitation, and 30 healthy subjects, participated in the study after obtaining the informed consents the accuracy and sensitivity of the approach is estimated to be 867% and 833%, respectively, showing a good performance to be used as a decision support system</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112575</th>\n",
       "      <td>physiology-based diagnosis algorithm for arteriovenous fistula stenosis detection in this paper, a diagnosis algorithm for arteriovenous fistula avf stenosis is developed based on auscultatory features, signal processing, and machine learning the avf sound signals are recorded by electronic stethoscopes at pre-defined positions before and after percutaneous transluminal angioplasty pta treatment several new signal features of stenosis are identified and quantified, and the physiological explanations for these features are provided utilizing support vector machine method, an average of 90% two-fold cross-validation hit-rate can be obtained, with angiography as the gold standard this offers a non-invasive easy-to-use diagnostic method for medical staff or even patients themselves for early detection of avf stenosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134904</th>\n",
       "      <td>comparative classification of thrombotic formations on bileaflet mechanical heart valves by phonographic analysis haemodynamic performance of bileaflet mechanical heart valves can be severely affected by the formation of thrombotic deposits hence, early detection of thrombi is fundamental for a prompt diagnosis and adequate therapy this article aims at designing a novel diagnostic and prognostic tool able to detect valvular thrombosis at early stages of formation, ie, before the appearance of critical symptoms in patients who can be effectively treated by pharmacological therapy, preventing re-operation this approach relies on the acquisition of the acoustic signals produced by mechanical heart valves in the closing phase; the corresponding power spectra are then analysed by means of artificial neural networks trained to identify the presence of thrombi and classify their occurrence five commercial bileaflet mechanical heart valves were investigated in vitro in a sheffield pulse duplicator; for each valve six functional conditions were considered, each corresponding to a risk class for patients one normofunctioning and five thrombosed: they have been simulated by placing artificial deposits of increasing weight and different shape on the valve leaflet and on the annular housing; the case of one completely blocked leaflet was also investigated these six functional conditions represent risk classes: they were examined under various hydrodynamic regimes the acoustic signals produced by the valves were acquired by means of a phonocardiographic apparatus, then analysed and classified the ability to detect and classify thrombotic formations on mechanical valve leaflet would allow ranking patients by assigning them to one of the six risk classes, helping clinicians in establish adequate therapeutic approaches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96017</th>\n",
       "      <td>a decision support system for cardiac disease diagnosis based on machine learning methods this paper proposes a decision support system for screening pediatric cardiac disease in primary healthcare centres relying on the heart sound time series analysis the proposed system employs our processing method which is based on the hidden markov model for extracting appropriate information from the time series the binary output resulting from the method is discriminative for the two classes of time series existing in our databank, corresponding to the children with heart disease and the healthy ones a total 90 children referrals to a university hospital, constituting of 55 healthy and 35 children with congenital heart disease, were enrolled into the study after obtaining the informed consent accuracy and sensitivity of the method was estimated to be 864% and 856%, respectively, showing a superior performance than what a paediatric cardiologist could achieve performing auscultation the method can be easily implemented using mobile and web technology to develop an easy-to-use tool for paediatric cardiac disease diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62837</th>\n",
       "      <td>heart sound classification using the snmfnet classifier heart sound classification still suffers from the challenges involved in achieving high accuracy in the case of small samples dimension reduction attempts to extract low-dimensional features with more discriminability from high-dimensional spaces or raw data, and is popular in learning predictive models that target small sample problems however, it can also be harmful to classification, because any reduction has the potential to lose information containing category attributes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47277</th>\n",
       "      <td>non-invasive identification of swallows via deep learning in high resolution cervical auscultation recordings high resolution cervical auscultation is a very promising noninvasive method for dysphagia screening and aspiration detection, as it does not involve the use of harmful ionizing radiation approaches automatic extraction of swallowing events in cervical auscultation is a key step for swallowing analysis to be clinically effective using time-varying spectral estimation of swallowing signals and deep feed forward neural networks, we propose an automatic segmentation algorithm for swallowing accelerometry and sounds that works directly on the raw swallowing signals in an online fashion the algorithm was validated qualitatively and quantitatively using the swallowing data collected from 248 patients, yielding over 3000 swallows manually labeled by experienced speech language pathologists with a detection accuracy that exceeded 95%, the algorithm has shown superior performance in comparison to the existing algorithms and demonstrated its generalizability when tested over 76 completely unseen swallows from a different population the proposed method is not only of great importance to any subsequent swallowing signal analysis steps, but also provides an evidence that such signals can capture the physiological signature of the swallowing process</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30305</th>\n",
       "      <td>automatic recognition of murmurs of ventricular septal defect using convolutional recurrent neural networks with temporal attentive pooling recognizing specific heart sound patterns is important for the diagnosis of structural heart diseases however, the correct recognition of heart murmur depends largely on clinical experience accurately identifying abnormal heart sound patterns is challenging for young and inexperienced clinicians this study is aimed at the development of a novel algorithm that can automatically recognize systolic murmurs in patients with ventricular septal defects vsds heart sounds from 51 subjects with vsds and 25 subjects without a significant heart malformation were obtained in this study subsequently, the soundtracks were divided into different training and testing sets to establish the recognition system and evaluate the performance the automatic murmur recognition system was based on a novel temporal attentive pooling-convolutional recurrent neural network tap-crnn model on analyzing the performance using the test data that comprised 178 vsd heart sounds and 60 normal heart sounds, a sensitivity rate of 960% was obtained along with a specificity of 967% when analyzing the heart sounds recorded in the second aortic and tricuspid areas, both the sensitivity and specificity were 100% we demonstrated that the proposed tap-crnn system can accurately recognize the systolic murmurs of vsd patients, showing promising potential for the development of software for classifying the heart murmurs of several other structural heart diseases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "112843                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          automatic wheezing detection based on signal processing of spectrogram and back-propagation neural network wheezing is a common clinical symptom in patients with obstructive pulmonary diseases such as asthma automatic wheezing detection offers an objective and accurate means for identifying wheezing lung sounds, helping physicians in the diagnosis, long-term auscultation, and analysis of a patient with obstructive pulmonary disease this paper describes the design of a fast and high-performance wheeze recognition system a wheezing detection algorithm based on the order truncate average method and a back-propagation neural network bpnn is proposed some features are extracted from processed spectra to train a bpnn, and subsequently, test samples are analyzed by the trained bpnn to determine whether they are wheezing sounds the respiratory sounds of 58 volunteers 32 asthmatic and 26 healthy adults were recorded for training and testing experimental results of a qualitative analysis of wheeze recognition showed a high sensitivity of 0946 and a high specificity of 10    \n",
       "890                                                                                                                                              rheumatic heart disease screening based on phonocardiogram rheumatic heart disease rhd is one of the most common causes of cardiovascular complications in developing countries it is a heart valve disease that typically affects children impaired heart valves stop functioning properly, resulting in a turbulent blood flow within the heart known as a murmur this murmur can be detected by cardiac auscultation however, the specificity and sensitivity of manual auscultation were reported to be low the other alternative is echocardiography, which is costly and requires a highly qualified physician given the diseases current high prevalence rate the latest reported rate in the study area ethiopia was 565%, there is a pressing need for early detection of the disease through mass screening programs this paper proposes an automated rhd screening approach using machine learning that can be used by non-medically trained persons outside of a clinical setting heart sound data was collected from 124 persons with rhd pwrhd and 46 healthy controls hc in ethiopia with an additional 81 hc records from an open-access dataset thirty-one distinct features were extracted to correctly represent rhd a support vector machine svm classifier was evaluated using two nested cross-validation approaches to quantitatively assess the generalization of the system to previously unseen subjects for regular nested 10-fold cross-validation, an f1-score of 960 ± 09%, recall 958 ± 15%, precision 962 ± 06% and a specificity of 960 ± 06% were achieved in the imbalanced nested cross-validation at a prevalence rate of 5%, it achieved an f1-score of 722 ± 08%, recall 923 ± 04%, precision 592 ± 36%, and a specificity of 948 ± 06% in screening tasks where the prevalence of the disease is small, recall is more important than precision the findings are encouraging, and the proposed screening tool can be inexpensive, easy to deploy, and has an excellent detection rate as a result, it has the potential for mass screening and early detection of rhd in developing countries   \n",
       "76550                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               detection of osteoporosis from percussion responses using an electronic stethoscope and machine learning osteoporosis is an asymptomatic bone condition that affects a large proportion of the elderly population around the world, resulting in increased bone fragility and increased risk of fracture previous studies had shown that the vibroacoustic response of bone can indicate the quality of the bone condition therefore, the aim of the authorsproject is to develop a new method to exploit this phenomenon to improve detection of osteoporosis in individuals in this paper a method is described that uses a reflex hammer to exert testing stimuli on a patients tibia and an electronic stethoscope to acquire the impulse responses the signals are processed as mel frequency cepstrum coefficients and passed through an artificial neural network to determine the likelihood of osteoporosis from the tibias impulse responses following some discussions of the mechanism and procedure, this paper details the signal acquisition using the stethoscope and the subsequent signal processing and the statistical machine learning algorithm pilot testing with 12 patients achieved over 80% sensitivity with a false positive rate below 30% and accuracies in the region of 70% an extended dataset of 110 patients achieved an error rate of 30% with some room for improvement in the algorithm by using common clinical apparatus and strategic machine learning, this method might be suitable as a large population screening test for the early diagnosis of osteoporosis, thus avoiding secondary complications   \n",
       "143756  support vectors machine-based identification of heart valve diseases using heart sounds taking into account that heart auscultation remains the dominant method for heart examination in the small health centers of the rural areas and generally in primary healthcare set-ups, the enhancement of this technique would aid significantly in the diagnosis of heart diseases in this context, the present paper initially surveys the research that has been conducted concerning the exploitation of heart sound signals for automated and semi-automated detection of pathological heart conditions then it proposes an automated diagnosis system for the identification of heart valve diseases based on the support vector machines svm classification of heart sounds this system performs a highly difficult diagnostic task even for experienced physicians, much more difficult than the basic diagnosis of the existence or not of a heart valve disease ie the classification of a heart sound as healthyor having a heart valve disease: it identifies the particular heart valve disease the system was applied in a representative global dataset of 198 heart sound signals, which come both from healthy medical cases and from cases suffering from the four most usual heart valve diseases: aortic stenosis as, aortic regurgitation ar, mitral stenosis ms and mitral regurgitation mr initially the heart sounds were successfully categorized using a svm classifier as normal or disease-related and then the corresponding murmurs in the unhealthy cases were classified as systolic or diastolic for the heart sounds diagnosed as having systolic murmur we used a svm classifier for performing a more detailed classification of them as having aortic stenosis or mitral regurgitation similarly for the heart sounds diagnosed as having diastolic murmur we used a svm classifier for classifying them as having aortic regurgitation or mitral stenosis alternative classifiers have been applied to the same data for comparison ie back-propagation neural networks, k-nearest-neighbour and naïve bayes classifiers, however their performance for the same diagnostic problems was lower than the svm classifiers proposed in this work   \n",
       "98317                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      s1 and s2 heart sound recognition using deep neural networks this study focuses on the first s1 and second s2 heart sound recognition based only on acoustic characteristics; the assumptions of the individual durations of s1 and s2 and time intervals of s1-s2 and s2-s1 are not involved in the recognition process the main objective is to investigate whether reliable s1 and s2 recognition performance can still be attained under situations where the duration and interval information might not be accessible   \n",
       "97085                                                                                                                                                                                                                                                                                                                                                                                                                                    a mobile platform for automated screening of asthma and chronic obstructive pulmonary disease chronic obstructive pulmonary disease copd and asthma each represent a large proportion of the global disease burden; copd is the third leading cause of death worldwide and asthma is one of the most prevalent chronic diseases, afflicting over 300 million people much of this burden is concentrated in the developing world, where patients lack access to physicians trained in the diagnosis of pulmonary disease as a result, these patients experience high rates of underdiagnosis and misdiagnosis to address this need, we present a mobile platform capable of screening for asthma and copd our solution is based on a mobile smart phone and consists of an electronic stethoscope, a peak flow meter application, and a patient questionnaire this data is combined with a machine learning algorithm to identify patients with asthma and copd to test and validate the design, we collected data from 119 healthy and sick participants using our custom mobile application and ran the analysis on a pc computer for comparison, all subjects were examined by an experienced pulmonologist using a full pulmonary testing laboratory employing a two-stage logistic regression model, our algorithms were first able to identify patients with either asthma or copd from the general population, yielding an roc curve with an auc of 095 then, after identifying these patients, our algorithm was able to distinguish between patients with asthma and patients with copd, yielding an roc curve with auc of 097 this work represents an important milestone towards creating a self-contained mobile phone-based platform that can be used for screening and diagnosis of pulmonary disease in many parts of the world   \n",
       "18232                                                                                                                                                                                                                                                               deep learning algorithm for automated cardiac murmur detection via a digital stethoscope platform background clinicians vary markedly in their ability to detect murmurs during cardiac auscultation and identify the underlying pathological features deep learning approaches have shown promise in medicine by transforming collected data into clinically significant information the objective of this research is to assess the performance of a deep learning algorithm to detect murmurs and clinically significant valvular heart disease using recordings from a commercial digital stethoscope platform methods and results using >34 hours of previously acquired and annotated heart sound recordings, we trained a deep neural network to detect murmurs to test the algorithm, we enrolled 962 patients in a clinical study and collected recordings at the 4 primary auscultation locations ground truth was established using patient echocardiograms and annotations by 3 expert cardiologists algorithm performance for detecting murmurs has sensitivity and specificity of 763% and 914%, respectively by omitting softer murmurs, those with grade 1 intensity, sensitivity increased to 900% application of the algorithm at the appropriate anatomic auscultation location detected moderate-to-severe or greater aortic stenosis, with sensitivity of 932% and specificity of 860%, and moderate-to-severe or greater mitral regurgitation, with sensitivity of 662% and specificity of 946% conclusions the deep learning algorithms ability to detect murmurs and clinically significant aortic stenosis and mitral regurgitation is comparable to expert cardiologists based on the annotated subset of our database the findings suggest that such algorithms would have utility as front-line clinical support tools to aid clinicians in screening for cardiac murmurs caused by valvular heart disease registration url: https://clinicaltrialsgov; unique identifier: nct03458806   \n",
       "144410                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       feature extraction for murmur detection based on support vector regression of time-frequency representations this paper presents a nonlinear approach for time-frequency representations tfr data analysis, based on a statistical learning methodology - support vector regression svr, that being a nonlinear framework, matches recent findings on the underlying dynamics of cardiac mechanic activity and phonocardiographic pcg recordings the proposed methodology aims to model the estimated tfrs, and extract relevant features to perform classification between normal and pathologic pcg recordings with murmur modeling of tfr is done by means of svr, and the distance between regressions is calculated through dissimilarity measures based on dot product finally, a k-nn classifier is used for the classification stage, obtaining a validation performance of 9785%   \n",
       "3375                                                                                                                                         characterizing effortful swallows from healthy community dwelling adults across the lifespan using high-resolution cervical auscultation signals and mbsimp scores: a preliminary study there is growing enthusiasm to develop inexpensive, non-invasive, and portable methods that accurately assess swallowing and provide biofeedback during dysphagia treatment high-resolution cervical auscultation hrca, which uses acoustic and vibratory signals from non-invasive sensors attached to the anterior laryngeal framework during swallowing, is a novel method for quantifying swallowing physiology via advanced signal processing and machine learning techniques hrca has demonstrated potential as a dysphagia screening method and diagnostic adjunct to vfsss by determining swallowing safety, annotating swallow kinematic events, and classifying swallows between healthy participants and patients with a high degree of accuracy however, its feasibility as a non-invasive biofeedback system has not been explored this study investigated 1 whether hrca can accurately differentiate between non-effortful and effortful swallows; 2 whether differences exist in modified barium swallow impairment profile mbsimp scores #9, #11, #14 between non-effortful and effortful swallows we hypothesized that hrca would accurately classify non-effortful and effortful swallows and that differences in mbsimp scores would exist between the types of swallows we analyzed 247 thin liquid 3 ml command swallows 71 effortful to minimize variation from 36 healthy adults who underwent standardized vfsss with concurrent hrca results revealed differences p < 005 in 9 hrca signal features between non-effortful and effortful swallows using hrca signal features as input, decision trees classified swallows with 76% accuracy, 76% sensitivity, and 77% specificity there were no differences in mbsimp component scores between non-effortful and effortful swallows while preliminary in nature, this study demonstrates the feasibility/promise of hrca as a biofeedback method for dysphagia treatment   \n",
       "49539                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               classifying dysphagic swallowing sounds with support vector machines swallowing sounds from cervical auscultation include information related to the swallowing function several studies have been conducted on the screening tests of dysphagia the literature shows a significant difference between the characteristics of swallowing sounds obtained from different subjects eg, healthy and dysphagic subjects; young and old adults these studies demonstrate the usefulness of swallowing sounds during dysphagic screening however, the degree of classification for dysphagia based on swallowing sounds has not been thoroughly studied in this study, we investigate the use of machine learning for classifying swallowing sounds into various types, such as normal swallowing or mild, moderate, and severe dysphagia in particular, swallowing sounds were recorded from patients with dysphagia support vector machines svms were trained using some features extracted from the obtained swallowing sounds moreover, the accuracy of the classification of swallowing sounds using the trained svms was evaluated via cross-validation techniques in the two-class scenario, wherein the swallowing sounds were divided into two categories viz normal and dysphagic subjects, the maximum f-measure was 789% in the four-class scenario, where the swallowing sounds were divided into four categories viz normal subject, and mild, moderate, and severe dysphagic subjects, the f-measure values for the classes were 656%, 531%, 511%, and 371%, respectively   \n",
       "163319                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            neural classification of lung sounds using wavelet coefficients electronic auscultation is an efficient technique to evaluate the condition of respiratory system using lung sounds as lung sound signals are non-stationary, the conventional method of frequency analysis is not highly successful in diagnostic classification this paper deals with a novel method of analysis of lung sound signals using wavelet transform, and classification using artificial neural network ann lung sound signals were decomposed into the frequency subbands using wavelet transform and a set of statistical features was extracted from the subbands to represent the distribution of wavelet coefficients an ann based system, trained using the resilient back propagation algorithm, was implemented to classify the lung sounds to one of the six categories: normal, wheeze, crackle, squawk, stridor, or rhonchus   \n",
       "41915                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                upper esophageal sphincter opening segmentation with convolutional recurrent neural networks in high resolution cervical auscultation upper esophageal sphincter is an important anatomical landmark of the swallowing process commonly observed through the kinematic analysis of radiographic examinations that are vulnerable to subjectivity and clinical feasibility issues acting as the doorway of esophagus, upper esophageal sphincter allows the transition of ingested materials from pharyngeal into esophageal stages of swallowing and a reduced duration of opening can lead to penetration/aspiration and/or pharyngeal residue therefore, in this study we consider a non-invasive high resolution cervical auscultation-based screening tool to approximate the human ratings of upper esophageal sphincter opening and closure swallows were collected from 116 patients and a deep neural network was trained to produce a mask that demarcates the duration of upper esophageal sphincter opening the proposed method achieved more than 90% accuracy and similar values of sensitivity and specificity when compared to human ratings even when tested over swallows from an independent clinical experiment moreover, the predicted opening and closure moments surprisingly fell within an inter-human comparable error of their human rated counterparts which demonstrates the clinical significance of high resolution cervical auscultation in replacing ionizing radiation-based evaluation of swallowing kinematics   \n",
       "94892                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   recognition of normal-abnormal phonocardiographic signals using deep convolutional neural networks and mel-frequency spectral coefficients intensive care unit patients are heavily monitored, and several clinically-relevant parameters are routinely extracted from high resolution signals   \n",
       "94281                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           intelligent phonocardiography for screening ventricular septal defect using time growing neural network this paper presents results of a study on the applicability of the intelligent phonocardiography in discriminating between ventricular spetal defect vsd and regurgitation of the atrioventricular valves an original machine learning method, based on the time growing neural network tgnn, is employed for classifying the phonocardiographic recordings collected from the pediatric referrals to a children hospital 90 individuals, 30 vsd, 30 with the valvular regurgitation, and 30 healthy subjects, participated in the study after obtaining the informed consents the accuracy and sensitivity of the approach is estimated to be 867% and 833%, respectively, showing a good performance to be used as a decision support system   \n",
       "112575                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       physiology-based diagnosis algorithm for arteriovenous fistula stenosis detection in this paper, a diagnosis algorithm for arteriovenous fistula avf stenosis is developed based on auscultatory features, signal processing, and machine learning the avf sound signals are recorded by electronic stethoscopes at pre-defined positions before and after percutaneous transluminal angioplasty pta treatment several new signal features of stenosis are identified and quantified, and the physiological explanations for these features are provided utilizing support vector machine method, an average of 90% two-fold cross-validation hit-rate can be obtained, with angiography as the gold standard this offers a non-invasive easy-to-use diagnostic method for medical staff or even patients themselves for early detection of avf stenosis    \n",
       "134904                                                                                                                                                                                                                                                                                                                                                                       comparative classification of thrombotic formations on bileaflet mechanical heart valves by phonographic analysis haemodynamic performance of bileaflet mechanical heart valves can be severely affected by the formation of thrombotic deposits hence, early detection of thrombi is fundamental for a prompt diagnosis and adequate therapy this article aims at designing a novel diagnostic and prognostic tool able to detect valvular thrombosis at early stages of formation, ie, before the appearance of critical symptoms in patients who can be effectively treated by pharmacological therapy, preventing re-operation this approach relies on the acquisition of the acoustic signals produced by mechanical heart valves in the closing phase; the corresponding power spectra are then analysed by means of artificial neural networks trained to identify the presence of thrombi and classify their occurrence five commercial bileaflet mechanical heart valves were investigated in vitro in a sheffield pulse duplicator; for each valve six functional conditions were considered, each corresponding to a risk class for patients one normofunctioning and five thrombosed: they have been simulated by placing artificial deposits of increasing weight and different shape on the valve leaflet and on the annular housing; the case of one completely blocked leaflet was also investigated these six functional conditions represent risk classes: they were examined under various hydrodynamic regimes the acoustic signals produced by the valves were acquired by means of a phonocardiographic apparatus, then analysed and classified the ability to detect and classify thrombotic formations on mechanical valve leaflet would allow ranking patients by assigning them to one of the six risk classes, helping clinicians in establish adequate therapeutic approaches   \n",
       "96017                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a decision support system for cardiac disease diagnosis based on machine learning methods this paper proposes a decision support system for screening pediatric cardiac disease in primary healthcare centres relying on the heart sound time series analysis the proposed system employs our processing method which is based on the hidden markov model for extracting appropriate information from the time series the binary output resulting from the method is discriminative for the two classes of time series existing in our databank, corresponding to the children with heart disease and the healthy ones a total 90 children referrals to a university hospital, constituting of 55 healthy and 35 children with congenital heart disease, were enrolled into the study after obtaining the informed consent accuracy and sensitivity of the method was estimated to be 864% and 856%, respectively, showing a superior performance than what a paediatric cardiologist could achieve performing auscultation the method can be easily implemented using mobile and web technology to develop an easy-to-use tool for paediatric cardiac disease diagnosis   \n",
       "62837                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         heart sound classification using the snmfnet classifier heart sound classification still suffers from the challenges involved in achieving high accuracy in the case of small samples dimension reduction attempts to extract low-dimensional features with more discriminability from high-dimensional spaces or raw data, and is popular in learning predictive models that target small sample problems however, it can also be harmful to classification, because any reduction has the potential to lose information containing category attributes   \n",
       "47277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            non-invasive identification of swallows via deep learning in high resolution cervical auscultation recordings high resolution cervical auscultation is a very promising noninvasive method for dysphagia screening and aspiration detection, as it does not involve the use of harmful ionizing radiation approaches automatic extraction of swallowing events in cervical auscultation is a key step for swallowing analysis to be clinically effective using time-varying spectral estimation of swallowing signals and deep feed forward neural networks, we propose an automatic segmentation algorithm for swallowing accelerometry and sounds that works directly on the raw swallowing signals in an online fashion the algorithm was validated qualitatively and quantitatively using the swallowing data collected from 248 patients, yielding over 3000 swallows manually labeled by experienced speech language pathologists with a detection accuracy that exceeded 95%, the algorithm has shown superior performance in comparison to the existing algorithms and demonstrated its generalizability when tested over 76 completely unseen swallows from a different population the proposed method is not only of great importance to any subsequent swallowing signal analysis steps, but also provides an evidence that such signals can capture the physiological signature of the swallowing process   \n",
       "30305                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        automatic recognition of murmurs of ventricular septal defect using convolutional recurrent neural networks with temporal attentive pooling recognizing specific heart sound patterns is important for the diagnosis of structural heart diseases however, the correct recognition of heart murmur depends largely on clinical experience accurately identifying abnormal heart sound patterns is challenging for young and inexperienced clinicians this study is aimed at the development of a novel algorithm that can automatically recognize systolic murmurs in patients with ventricular septal defects vsds heart sounds from 51 subjects with vsds and 25 subjects without a significant heart malformation were obtained in this study subsequently, the soundtracks were divided into different training and testing sets to establish the recognition system and evaluate the performance the automatic murmur recognition system was based on a novel temporal attentive pooling-convolutional recurrent neural network tap-crnn model on analyzing the performance using the test data that comprised 178 vsd heart sounds and 60 normal heart sounds, a sensitivity rate of 960% was obtained along with a specificity of 967% when analyzing the heart sounds recorded in the second aortic and tricuspid areas, both the sensitivity and specificity were 100% we demonstrated that the proposed tap-crnn system can accurately recognize the systolic murmurs of vsd patients, showing promising potential for the development of software for classifying the heart murmurs of several other structural heart diseases   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "112843       0       0        0         0       0        0        0        0   \n",
       "890          0       0        0         1       0        0        0        0   \n",
       "76550        0       0        0         0       0        0        0        0   \n",
       "143756       0       0        0         0       0        0        0        0   \n",
       "98317        0       0        0         0       0        0        0        0   \n",
       "97085        0       0        0         0       0        0        0        0   \n",
       "18232        0       0        0         1       0        0        0        0   \n",
       "144410       0       0        0         0       0        0        0        0   \n",
       "3375         0       0        0         0       0        0        0        0   \n",
       "49539        0       0        0         0       0        0        0        0   \n",
       "163319       0       0        0         0       0        0        0        0   \n",
       "41915        1       0        0         0       0        0        0        0   \n",
       "94892        0       0        0         0       0        0        0        0   \n",
       "94281        0       0        0         0       0        0        0        0   \n",
       "112575       0       0        0         0       0        0        0        0   \n",
       "134904       0       0        0         0       0        0        0        0   \n",
       "96017        0       0        0         0       0        0        0        0   \n",
       "62837        0       0        0         0       0        0        0        0   \n",
       "47277        0       0        0         0       0        0        0        0   \n",
       "30305        0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text mamm_text endo_text derm_text gene_text bio_text  \\\n",
       "112843          0        0         0         0         0         0        0   \n",
       "890             0        0         0         0         0         0        0   \n",
       "76550           0        0         0         0         0         0        0   \n",
       "143756          0        0         0         0         0         0        0   \n",
       "98317           0        0         0         0         0         0        0   \n",
       "97085           0        0         0         0         0         0        0   \n",
       "18232           0        0         0         0         0         0        0   \n",
       "144410          0        0         0         0         0         0        0   \n",
       "3375            0        0         0         0         0         0        0   \n",
       "49539           0        0         0         0         0         0        0   \n",
       "163319          0        0         0         0         0         0        0   \n",
       "41915           0        0         0         0         0         0        0   \n",
       "94892           0        0         0         0         0         0        0   \n",
       "94281           0        0         0         0         0         0        0   \n",
       "112575          0        0         0         0         0         0        0   \n",
       "134904          0        0         0         0         0         0        0   \n",
       "96017           0        0         0         0         0         0        0   \n",
       "62837           0        0         0         0         0         0        0   \n",
       "47277           0        0         0         0         0         0        0   \n",
       "30305           0        0         0         0         0         0        0   \n",
       "\n",
       "       nlp_text ehr_text sensor_text prom_text phone_text sound_text  \n",
       "112843        0        0           0         0          0          1  \n",
       "890           0        0           0         0          0          1  \n",
       "76550         0        0           0         0          0          1  \n",
       "143756        0        0           0         0          0          1  \n",
       "98317         0        0           0         0          0          1  \n",
       "97085         0        0           0         0          0          1  \n",
       "18232         0        0           0         0          0          1  \n",
       "144410        0        0           0         0          0          1  \n",
       "3375          0        0           0         0          0          1  \n",
       "49539         0        0           0         0          0          1  \n",
       "163319        0        0           0         0          0          1  \n",
       "41915         0        0           0         0          0          1  \n",
       "94892         0        0           0         0          0          1  \n",
       "94281         0        0           0         0          0          1  \n",
       "112575        0        0           0         0          0          1  \n",
       "134904        0        0           0         0          0          1  \n",
       "96017         0        0           0         0          0          1  \n",
       "62837         0        0           0         0          0          1  \n",
       "47277         0        0           0         0          0          1  \n",
       "30305         0        0           0         0          0          1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['sound_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f83c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINE\n",
    "labelled['feat_xr'] = np.where(feat['xr_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_ct'] = np.where(feat['ct_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_mri'] = np.where(feat['mri_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_eeg'] = np.where(feat['eeg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_ecg'] = np.where(feat['ecg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_emg'] = np.where(feat['emg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_us'] = np.where(feat['us_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_echo'] = np.where(feat['echo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_histo'] = np.where(feat['histo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_oct'] = np.where(feat['oct_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_mamm'] = np.where(feat['mamm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_endoscop'] = np.where(feat['endo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_derm'] = np.where(feat['derm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_gene'] = np.where(feat['gene_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_bio'] = np.where(feat['bio_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_nlp'] = np.where(feat['nlp_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_ehr'] = np.where(feat['ehr_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_sensor'] = np.where(feat['sensor_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_phone'] = np.where(feat['phone_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_prom'] = np.where(feat['prom_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_sound'] = np.where(feat['sound_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "#feat.to_csv('output/feat_tagged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d94c23",
   "metadata": {},
   "source": [
    "## Tag Specialties / Use-Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90780931",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CLASS TAGS - by mesh for disease type\n",
    "######################\n",
    "\n",
    "\n",
    "######################\n",
    "## CLASS TAGS - by specialty, not mutually exclusive\n",
    "######################\n",
    "## INTENSIVE CARE MEDICINE / icu\n",
    "\n",
    "## EMERGENCY MEDICINE / ed\n",
    "\n",
    "## INFECTIONS [C01] / id\n",
    "    #### SEPSIS / sepsis\n",
    "    #### COVID-19 / cov19\n",
    "    #### MALARIA / malaria\n",
    "    #### HIV / hiv\n",
    "    #### TB / tb\n",
    "    #### TROPICAL DISEASE / tropic\n",
    "    \n",
    "## DERMATOLOGY [C17] / derm\n",
    "    ####SKIN CANCERS / dermca\n",
    "\n",
    "## NEOPLASMS [C04] / onc\n",
    "    #### RADIOTHERAPY / rx\n",
    "    #### LUNG / lungca\n",
    "    #### NEURO / neuroca\n",
    "    #### GI / gica\n",
    "    #### HPB / hepca\n",
    "    #### GYNAE / gynonc\n",
    "    #### PROSTATE / prosca\n",
    "    #### RENAL / renalca\n",
    "    #### HAEM / haemonc\n",
    "    \n",
    "## BREAST / breast (<- almost entirely onc)\n",
    "    #### BREAST CA / breastca\n",
    "    \n",
    "## PSYCHIATRY / psych\n",
    "    #### SUICIDE / suicide\n",
    "    \n",
    "## MUSCULOSKELETAL [C05] / msk\n",
    "    #### FRACTURE / frac\n",
    "\n",
    "## CONNECTIVE TISSUE [C17] / rheum\n",
    "\n",
    "## GASTROINTESTINAL [C06] / gi\n",
    "\n",
    "## HEPATOLOGY & BILIARY [C06] / hep\n",
    "\n",
    "## RESPIRATORY [C08] / resp\n",
    "    #### PNEUMONIA / pneum\n",
    "    #### OBSTRUCTIVE SLEEP / osa\n",
    "    #### PULMONARY EMBOLISM / pe\n",
    "    \n",
    "## NERVOUS SYSTEM [C10] / neuro\n",
    "    #### STROKE / cva\n",
    "    #### SEIZURE / epilep\n",
    "    #### DEMENTIA / alzh\n",
    "\n",
    "## CARDIOVASCULAR [C14] / cvs\n",
    "    #### ISCHAEMIC HEART DISEASE / ihd\n",
    "    #### CARDIAC FAILURE / hf\n",
    "    #### ARRHYTHMIA / arrhyt\n",
    "    \n",
    "## ENDOCRINE [C19] (no dm) / endo\n",
    "\n",
    "## DIABETES / dm\n",
    "    #### INSULIN / insulin\n",
    "    #### RETINOPATHY / retina\n",
    "        \n",
    "## OPHTHALMOLOGY [C11] / eye\n",
    "\n",
    "## HAEMATOLOGIC [C15] / haem\n",
    "\n",
    "## GYNAE/OBSTETRIC [C13] / obs\n",
    "\n",
    "## NEPHROLOGY [C12] / renal\n",
    "    #### ACUTE & CHRONIC KIDNEY / ackd\n",
    "    \n",
    "## PAEDIATRICS / paeds\n",
    "\n",
    "## STOMATOGNATHIC [C07] / dental\n",
    "\n",
    "## AUDIOLOGY [C09] / ent\n",
    "\n",
    "## PUBLIC HEALTH / pubh\n",
    "\n",
    "########exclude?############# \n",
    "\n",
    "## ALCOHOL & SUBSTANCES [C25] / etoh\n",
    "## WOUNDS AND INJURIES [C26] -> TRAUMA\n",
    "## ENVIRONMENTAL [C21] / env\n",
    "\n",
    "\n",
    "######################\n",
    "## SPECIAL\n",
    "######################\n",
    "## BCI\n",
    "## CONTROL\n",
    "#### PROSTHESIS CONTROL\n",
    "#### WHEELCHAIR CONTROL\n",
    "\n",
    "\n",
    "\n",
    "## vitals monitoring / deterioration\n",
    "## trauma?\n",
    "## sleep\n",
    "## pulmonary embolism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "590779eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = groups[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d069aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33431, '1': 748})\n"
     ]
    }
   ],
   "source": [
    "## INTENSIVE CARE MEDICINE / icu\n",
    "\n",
    "## text\n",
    "text = ['intensive care', 'critical care', 'mechanical ventilation', 'invasive ventilation', 'ventilator', 'pressure ventilation', \n",
    "       'acute respiratory distress syndrome', 'organ failure', 'tracheal intubation', 'vasopressor', 'inotrope',\n",
    "       'hemofiltration', 'membrane oxygenation', 'ecmo', ' ett ', 'layngoscope', 'endotracheal tube']\n",
    "\n",
    "spec['icu_text'] = np.where(groups['text'].str.contains('intensive therapy unit'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['icu_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['icu_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output    \n",
    "print('text counts:')\n",
    "print(Counter(spec['icu_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68d4c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33768, '1': 411})\n"
     ]
    }
   ],
   "source": [
    "## EMERGENCY MEDICINE / ed\n",
    "\n",
    "## text\n",
    "text = ['emergency department', 'emergency room', 'emergency physician', 'emergency doctor', 'emergency medicine',\n",
    "       'emergency care', 'accident and emergency', 'a&e', 'accident & emergency', 'prehospital', 'pre-hospital',\n",
    "       'casualty room', 'emergency ward']\n",
    "\n",
    "spec['ed_text'] = np.where(groups['text'].str.contains('casualty department'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['ed_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['ed_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['ed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f704cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31602, '1': 2577})\n"
     ]
    }
   ],
   "source": [
    "## INFECTIONS / id + bacteriology/virology/parasitology\n",
    "\n",
    "## text\n",
    "text = ['bacter', 'microbiol', 'sepsis', 'septic', 'toxic shock', 'microbe', 'tuberculosis',\n",
    "       'cholera', 'shigella', 'bubonic', 'plague', 'anthrax', 'gonorrhea', 'syphilis', 'diphtheria', 'legionell',\n",
    "       'leptospirosis', 'listeriosis', 'tetanus', 'pertussis', 'staph', 'strep', 'escherichia', 'leprosy', \n",
    "        'mycobacter', 'blood culture',\n",
    "       \n",
    "       'fungus', 'fungal', 'fungaemia', 'fungemia', 'candida ', 'aspergill',\n",
    "       \n",
    "       'virolog', 'virus', 'viral', 'virulen', 'influenza', 'hepatitis', 'herpes', 'varicella',\n",
    "       'measles', 'covid', 'sars-cov', 'coronavirus', 'severe acute respiratory syndrome', 'yellow fever', 'dengue',\n",
    "       'rabies', 'zika', 'ebola', 'polio', 'hemorrhagic fever', 'haemorrhagic fever', 'rabies',\n",
    "       \n",
    "       'transmitted disease', 'sexually transmit', 'sexual transmis',\n",
    "       \n",
    "       ' lyme', 'malaria', 'falciparum', 'anopheles', 'parasit', 'helminth', 'protozoa', \n",
    "        'leishmaniasis', 'trypanosom', 'chagas', 'schistosomiasis', 'filariasis', 'toxoplasm' 'tropical disease',\n",
    "       \n",
    "       ' hiv ', 'human immunodeficiency virus', 'acquired immune deficiency syndrome']\n",
    "\n",
    "spec['id_text'] = np.where(groups['text'].str.contains('infectio'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['id_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['id_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['id_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05c7a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33928, '1': 251})\n"
     ]
    }
   ],
   "source": [
    "#### SEPSIS / sepsis\n",
    "\n",
    "## text\n",
    "text = ['sepsis', 'septic', 'bacteraem', 'bacterem', 'toxic shock syndrome', 'pyaemia']\n",
    "\n",
    "spec['sepsis_text'] = np.where(groups['text'].str.contains('pyemia'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['sepsis_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['sepsis_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['sepsis_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "534bcd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32994, '1': 1185})\n"
     ]
    }
   ],
   "source": [
    "#### COVID-19 / cov19\n",
    "\n",
    "## text\n",
    "text = ['sars-cov', 'coronavirus disease 2019', 'novel coronavirus', 'coronavirus disease 19', 'sars cov']\n",
    "\n",
    "spec['cov19_text'] = np.where(groups['text'].str.contains('covid'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['cov19_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['cov19_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['cov19_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78c203fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33996, '1': 183})\n"
     ]
    }
   ],
   "source": [
    "#### HIV / hiv\n",
    "\n",
    "## text\n",
    "text = ['human immunodeficiency virus', 'acquired immune deficiency syndrome', ' aids ']\n",
    "\n",
    "spec['hiv_text'] = np.where(groups['text'].str.contains(' hiv '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hiv_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hiv_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hiv_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f260f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34011, '1': 168})\n"
     ]
    }
   ],
   "source": [
    "#### TUBERCULOSIS / tb\n",
    "\n",
    "## text\n",
    "text = ['tuberculosis', 'mycobacterium tuberc']\n",
    "\n",
    "spec['tb_text'] = np.where(groups['text'].str.contains('tubercu'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['tb_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['tb_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['tb_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eee84916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34053, '1': 126})\n"
     ]
    }
   ],
   "source": [
    "#### TROPICAL DISEASE / tropic\n",
    "\n",
    "## text\n",
    "text = ['malaria', 'falciparum', 'anopheles', 'parasit', 'helminth', 'protozoa', \n",
    "        'leishmaniasis', 'trypanosom', 'chagas', 'schistosomiasis', 'filariasis', 'toxoplasm',\n",
    "       'yellow fever', 'dengue', 'rabies', 'cholera', 'zika', 'ebola', 'hemorrhagic fever', 'haemorrhagic fever',\n",
    "        'tropical disease', 'tropical medicine', 'filariasis']\n",
    "\n",
    "spec['tropic_text'] = np.where(groups['text'].str.contains('falciparum'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['tropic_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['tropic_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['tropic_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0be31c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34120, '1': 59})\n"
     ]
    }
   ],
   "source": [
    "#### MALARIA / malaria\n",
    "\n",
    "## text\n",
    "text = ['malaria', 'anopheles']\n",
    "\n",
    "spec['malaria_text'] = np.where(groups['text'].str.contains('falciparum'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['malaria_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['malaria_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['malaria_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7edd69f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33397, '1': 782})\n"
     ]
    }
   ],
   "source": [
    "## DERMATOLOGY / derm\n",
    "\n",
    "## text\n",
    "text = ['dermato', 'dermatitis', 'erythema', 'cutaneous', 'eczema', 'psoriasis', 'rosacea', 'vitiligo', 'urticaria',\n",
    "       'pruritus', 'impetigo', 'pemphigoid', 'pityriasis', 'melanoma', 'basal cell ca', 'merkel cell',\n",
    "       'skin cancer', 'skin lesion', 'skin rash', 'nevus', 'naevus', 'dermal cancer', 'dermal lesion']\n",
    "\n",
    "spec['derm_text'] = np.where(groups['text'].str.contains('emollient'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['derm_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['derm_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['derm_text'])\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['derm_text'])\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['derm_text'])\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['derm_text'])\n",
    "                             \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['derm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea294850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33787, '1': 392})\n"
     ]
    }
   ],
   "source": [
    "#### SKIN CANCERS / dermca\n",
    "\n",
    "## text\n",
    "text = ['melanoma', 'melanocytic', 'casal cell ca', 'skin cancer', 'dysplastic nevus', 'dysplastic naevus',\n",
    "       'merkel cell', 'atypical nevus', 'atypical naevus']\n",
    "\n",
    "spec['dermca_text'] = np.where(groups['text'].str.contains('skin cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['dermca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['dermca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['dermca_text'])\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['dermca_text'])\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['dermca_text'])\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['dermca_text'])\n",
    "                          \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['dermca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c6f5cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 25235, '1': 8944})\n"
     ]
    }
   ],
   "source": [
    "## ONCOLOGY / onc\n",
    "\n",
    "## text\n",
    "text = [ 'cancer', 'carcinoma', 'oncolog', 'neoplasm', 'neoplastic',\n",
    "        'radiotherapy', 'radiation therapy', 'mammog', 'breast ca', 'breast tum', 'invasive lobular carcinoma', \n",
    "        ' dcis ', 'ductal carcinoma in situ', 'lung cancer', 'lung malignancy', 'lung carcinoma', 'lung nodule',\n",
    "        'pulmonary nodule', 'mesothelioma', 'nsclc',\n",
    "       'neuroonc', 'neuro onc', 'neuro-onc', 'brain cancer', 'brain tumor', 'brain tumour', 'brain malignancy',\n",
    "       'glioma', 'glioblastoma', 'astrocytoma', 'pituitary adenoma', 'acoustic neuroma', 'meningioma',\n",
    "       'cns lymphoma', 'oligodendroglioma', 'meningeal cancer', 'meningeal carcinomatosis',\n",
    "       'melanoma', 'melanocytic', 'casal cell ca', 'skin cancer', 'dysplastic nevus', 'dysplastic naevus',\n",
    "       'merkel cell', 'atypical nevus', 'atypical naevus',\n",
    "       'gi cancer', 'gastrointestinal cancer', 'colon cancer', 'colon carcinoma', 'colon polyp', 'colon adeno', 'colon tumo',\n",
    "       'colonic cancer', 'colonic carcinoma', 'colonic adeno', 'colonic polyp', 'colonic tumo', 'colonic neoplasm',\n",
    "        'rectal cancer', 'rectal carcinoma', 'rectal polyp', 'rectal tumo', 'rectal neoplasm', 'bowel cancer', 'bowel neoplasm',\n",
    "       'bowel tumo', 'stomach cancer', 'gastric cancer', 'gastric carcinoma', 'gastric neoplasm', 'gastric tumo',\n",
    "       'esophageal cancer', 'esophageal tumo', 'esophageal neoplasm',\n",
    "       'hepatocellular cancer', 'hepatocellular carcinoma', 'hepatic cancer', 'hepatic carcinoma', 'hepatic tumo',\n",
    "       'hepatic neoplasm', 'liver cancer', 'liver carcinoma', 'liver tumo', 'cholangioca', 'pancreatic cancer',\n",
    "       'pancreatic neoplasm', 'pancreatic tumo', 'biliary cancer', 'bile duct cancer',\n",
    "       'prostate cancer', 'prostate specific antigen', 'prostate carcinoma', 'prostate neoplasm', 'prostate tumo',\n",
    "       'prostate adeno', 'prostatic cancer', 'prostatic neoplasm', 'prostatic tumo', 'prostatic adeno', 'prostatectomy',\n",
    "       ' psa ', 'kidney cancer', 'kidney tumo', 'renal cell carcinoma', 'renal call cancer', 'renal tumo', 'renal cancer',\n",
    "       'wilms tumo', 'bladder cancer', 'bladder carcinoma', 'transitional cell ca', 'urothelial cancer', 'urothelial carcinoma',\n",
    "        'gynecologic cancer', 'gynecological cancer', 'gynaecologic cancer', 'gynaecological cancer', 'ovarian cancer',\n",
    "       'ovarian carcinoma', 'uterine cancer', 'uterine carcinoma', 'cervical cancer', 'cervical carcinoma', 'colposcop',\n",
    "       'haematological cancer', 'hematological cancer', 'haematological malig', 'hematological malig', 'myelodysplas',\n",
    "       'myeloprolif', 'lymphoprolif', 'leukaemoa', 'leukemia', 'myelofibro', 'thrombocythemia', 'polycythemia vera',\n",
    "       'polycythemia rubra vera', 'thrombocythaemia', 'polycythaemia vera', 'polycythaemia rubra vera', 'lymphoma',\n",
    "       'myeloma', ' gvhd', 'stem cell transpl', 'bone marrow aspirate']\n",
    "\n",
    "spec['onc_text'] = np.where(groups['text'].str.contains('metasta'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['onc_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['onc_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['onc_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb55cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33920, '1': 259})\n"
     ]
    }
   ],
   "source": [
    "#### RADIOTHERAPY / rx\n",
    "\n",
    "## text\n",
    "spec['rx_text'] = np.where(groups['text'].str.contains(\"radiotherapy\"), \"1\", \"0\")\n",
    "spec['rx_text'] = np.where(groups['text'].str.contains(\"radiation therapy\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(spec['rx_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4085c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32173, '1': 2006})\n"
     ]
    }
   ],
   "source": [
    "#### BREAST / breast\n",
    "\n",
    "## text\n",
    "text = ['mammog', 'breast ca', 'breast tum', 'invasive lobular carcinoma', ' dcis ', 'ductal carcinoma in situ']\n",
    "\n",
    "spec['breast_text'] = np.where(groups['text'].str.contains(' breast '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['breast_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['breast_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['breast_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5213e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32422, '1': 1757})\n"
     ]
    }
   ],
   "source": [
    "#### BREAST CANCER / breastca\n",
    "\n",
    "## text\n",
    "text = ['mammog', 'breast ca', 'breast tum', 'invasive lobular carcinoma', ' dcis ', 'ductal carcinoma in situ']\n",
    "\n",
    "spec['breastca_text'] = np.where(groups['text'].str.contains('breast cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['breastca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['breastca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['breastca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d8ff1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33001, '1': 1178})\n"
     ]
    }
   ],
   "source": [
    "#### LUNG CA / lungca\n",
    "\n",
    "## text\n",
    "text = ['lung cancer', 'lung malignancy', 'lung carcinoma', 'lung nodule', 'pulmonary nodule', 'mesothelioma', 'nsclc']\n",
    "\n",
    "spec['lungca_text'] = np.where(groups['text'].str.contains('lung cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['lungca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['lungca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"adenoca\")) , \"1\", spec['lungca_text'])\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"small cell\")) , \"1\", spec['lungca_text'])\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"squamous\")) , \"1\", spec['lungca_text'])\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"small-cell\")) , \"1\", spec['lungca_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['lungca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e524470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33275, '1': 904})\n"
     ]
    }
   ],
   "source": [
    "#### NEURO ONC / neuroca\n",
    "\n",
    "## text\n",
    "text = ['neuroonc', 'neuro onc', 'neuro-onc', 'brain cancer', 'brain tumor', 'brain tumour', 'brain malignancy',\n",
    "       'glioma', 'glioblastoma', 'astrocytoma', 'pituitary adenoma', 'acoustic neuroma', 'meningioma',\n",
    "       'cns lymphoma', 'oligodendroglioma', 'meningeal cancer', 'meningeal carcinomatosis']\n",
    "\n",
    "spec['brainca_text'] = np.where(groups['text'].str.contains('brain cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['brainca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['brainca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['brainca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69fa7878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33331, '1': 848})\n"
     ]
    }
   ],
   "source": [
    "#### GI ONC / gica\n",
    "\n",
    "## text\n",
    "text = ['gi cancer', 'gastrointestinal cancer', 'colon cancer', 'colon carcinoma', 'colon polyp', 'colon adeno', 'colon tumo',\n",
    "       'colonic cancer', 'colonic carcinoma', 'colonic adeno', 'colonic polyp', 'colonic tumo', 'colonic neoplasm',\n",
    "        'rectal cancer', 'rectal carcinoma', 'rectal polyp', 'rectal tumo', 'rectal neoplasm', 'bowel cancer', 'bowel neoplasm',\n",
    "       'bowel tumo', 'stomach cancer', 'gastric cancer', 'gastric carcinoma', 'gastric neoplasm', 'gastric tumo',\n",
    "       'esophageal cancer', 'esophageal tumo', 'esophageal neoplasm']\n",
    "\n",
    "spec['gica_text'] = np.where(groups['text'].str.contains('luminal cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['gica_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['gica_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['gica_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1f9928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33682, '1': 497})\n"
     ]
    }
   ],
   "source": [
    "#### HPB ONC / hepca\n",
    "\n",
    "## text\n",
    "text = ['hepatocellular cancer', 'hepatocellular carcinoma', 'hepatic cancer', 'hepatic carcinoma', 'hepatic tumo',\n",
    "       'hepatic neoplasm', 'liver cancer', 'liver carcinoma', 'liver tumo', 'cholangioca', 'pancreatic cancer',\n",
    "       'pancreatic neoplasm', 'pancreatic tumo', 'biliary cancer', 'bile duct cancer']\n",
    "\n",
    "spec['hepca_text'] = np.where(groups['text'].str.contains('cancer of the pancreas'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hepca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hepca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hepca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db3d1c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32548, '1': 1631})\n"
     ]
    }
   ],
   "source": [
    "#### UROLOGY / urology\n",
    "\n",
    "## text\n",
    "text = ['prostate', 'prostatic', 'prostatectomy', ' psa ', 'urolog', 'urethra', 'bladder']\n",
    "\n",
    "spec['urology_text'] = np.where(groups['text'].str.contains('prostatectomy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['urology_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['urology_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['urology_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "633c1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33506, '1': 673})\n"
     ]
    }
   ],
   "source": [
    "#### PROSTATE ONC / prosca\n",
    "\n",
    "## text\n",
    "text = ['prostate cancer', 'prostate specific antigen', 'prostate carcinoma', 'prostate neoplasm', 'prostate tumo',\n",
    "       'prostate adeno', 'prostatic cancer', 'prostatic neoplasm', 'prostatic tumo', 'prostatic adeno', 'prostatectomy',\n",
    "       ' psa ']\n",
    "\n",
    "spec['prosca_text'] = np.where(groups['text'].str.contains('prostatectomy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['prosca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['prosca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['prosca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ac6d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33920, '1': 259})\n"
     ]
    }
   ],
   "source": [
    "#### RENAL & BLADDER / renalca\n",
    "\n",
    "## text\n",
    "text = ['kidney cancer', 'kidney tumo', 'renal cell carcinoma', 'renal call cancer', 'renal tumo', 'renal cancer',\n",
    "       'wilms tumo', 'bladder cancer', 'bladder carcinoma', 'transitional cell ca', 'urothelial cancer', 'urothelial carcinoma']\n",
    "\n",
    "spec['renalca_text'] = np.where(groups['text'].str.contains('renal carcinoma'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['renalca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['renalca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['renalca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6aaa489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33770, '1': 409})\n"
     ]
    }
   ],
   "source": [
    "#### GYNAE / gynonc\n",
    "\n",
    "## text\n",
    "text = ['gynecologic cancer', 'gynecological cancer', 'gynaecologic cancer', 'gynaecological cancer', 'ovarian cancer',\n",
    "       'ovarian carcinoma', 'uterine cancer', 'uterine carcinoma', 'cervical cancer', 'cervical carcinoma', 'colposcop',\n",
    "       'endometrial cancer']\n",
    "\n",
    "spec['gynonc_text'] = np.where(groups['text'].str.contains('pap smear'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['gynonc_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['gynonc_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['gynonc_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8418f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33775, '1': 404})\n"
     ]
    }
   ],
   "source": [
    "#### HAEM / haemonc\n",
    "\n",
    "## text\n",
    "text = ['haematological cancer', 'hematological cancer', 'haematological malig', 'hematological malig', 'myelodysplas',\n",
    "       'myeloprolif', 'lymphoprolif', 'leukaemia', 'leukemia', 'myelofibro', 'thrombocythemia', 'polycythemia vera',\n",
    "       'polycythemia rubra vera', 'thrombocythaemia', 'polycythaemia vera', 'polycythaemia rubra vera', 'lymphoma',\n",
    "       'myeloma', ' gvhd', 'stem cell transpl', 'bone marrow aspirate']\n",
    "\n",
    "spec['haemonc_text'] = np.where(groups['text'].str.contains('bone marrow biopsy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['haemonc_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['haemonc_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['haemonc_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b77cf66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32061, '1': 2118})\n"
     ]
    }
   ],
   "source": [
    "## PSYCHIATRY / psych\n",
    "\n",
    "## text\n",
    "text = ['psych', 'schizo', 'depressive disorder', 'anxiety disorder', 'stress disorder', 'suicide', 'suicidal', 'mood disorder',\n",
    "        'self harm', 'self-harm', 'self injury', 'self-injury',\n",
    "        'mental disorder', 'hyperactivity disorder', 'hyperactive disorder', 'psychological distress', 'bipolar', \n",
    "       'addiction disorder', 'autism', 'autistic']\n",
    "\n",
    "spec['psych_text'] = np.where(groups['text'].str.contains('mental health'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['psych_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['psych_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['psych_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67194724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33660, '1': 519})\n"
     ]
    }
   ],
   "source": [
    "## SUICIDE / suicide\n",
    "\n",
    "## text\n",
    "text = ['suicide', 'suicidal', 'self harm', 'self-harm', 'self injury', 'self-injury', 'depressive disorder']\n",
    "\n",
    "spec['suicide_text'] = np.where(groups['text'].str.contains('low mood'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['suicide_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['suicide_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['suicide_text'] = np.where((groups['text'].str.contains(\"psych\")) &\n",
    "                             (groups['text'].str.contains(\"depression\")) , \"1\", spec['suicide_text'])\n",
    "spec['suicide_text'] = np.where((groups['text'].str.contains(\"mental\")) &\n",
    "                             (groups['text'].str.contains(\"depression\")) , \"1\", spec['suicide_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['suicide_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b185895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33237, '1': 942})\n"
     ]
    }
   ],
   "source": [
    "## MUSCULOSKELETAL / msk\n",
    "\n",
    "## text\n",
    "text = ['musculoskeletal', 'bone disease', 'bone cyst', 'chondritis', 'fasciitis', 'ankylos', 'osteoarth', 'orthoped',\n",
    "       'orthopaed', 'bursitis', 'synovitis', 'congenital hip', 'joint instability', 'joint stability', 'myositis',\n",
    "       'polymyalgia', 'fibromyalgia', ' gout', 'tendinopath', 'arthro', 'ligament', 'fracture', 'hip surgery',\n",
    "       'hip replacement', 'acetabul', 'cruciate', 'joint space', 'dysplatic hip', 'hip dysplas', 'vertebral', 'discectomy',\n",
    "       'lumbar spine', 'thoracic spine', 'cervical spine', 'whole spine', 'osteoporosis', 'bone mineral density']\n",
    "\n",
    "spec['msk_text'] = np.where(groups['text'].str.contains('broken bone'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['msk_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['msk_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['msk_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5247fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33942, '1': 237})\n"
     ]
    }
   ],
   "source": [
    "#### FRACTURE / frac\n",
    "\n",
    "##text\n",
    "spec['frac_text'] = np.where(groups['text'].str.contains(\"fracture\"), \"1\", \"0\")\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['frac_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50ac65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34015, '1': 164})\n"
     ]
    }
   ],
   "source": [
    "## CONNECTIVE TISSUE [C17] / rheum\n",
    "\n",
    "## text\n",
    "text = ['rheumatoid', 'scleroderma', 'wegener', 'polyangiitis', 'churg-strauss', 'lupus', 'connective tissue disease',\n",
    "        'mixed connective tissue', 'polymyositis', 'dermatomyositis', 'sjogren', 'vasculitis', 'vasculitide', 'marfan',\n",
    "       'ehlers-danlos', 'osteogenesis imperfecta']\n",
    "\n",
    "spec['rheum_text'] = np.where(groups['text'].str.contains('rheumatolog'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['rheum_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['rheum_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['rheum_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58806800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32628, '1': 1551})\n"
     ]
    }
   ],
   "source": [
    "## LUMINAL GI / gi\n",
    "\n",
    "## text\n",
    "text = ['gastro', 'gastri', 'intestin', 'duoden', 'colonic', 'colonoscop', 'colitis', 'rectal', 'ileus', 'ileitis',\n",
    "       'crohn', 'esophag', 'proctitis', 'proctolog', 'bowel disease', 'bowel cancer', 'bowel neoplasm' ,'bowel tumo',\n",
    "       'celiac', 'coeliac', 'diverticulitis', 'diverticulosis', 'stomach', 'small bowel', 'large bowel']\n",
    "\n",
    "spec['gi_text'] = np.where(groups['text'].str.contains('gi tract'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['gi_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['gi_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['gi_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35b09f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32964, '1': 1215})\n"
     ]
    }
   ],
   "source": [
    "## HEPATOLOGY (and pancreatobiliary) / hep\n",
    "\n",
    "## text\n",
    "text = ['hepato', 'hepati', 'cholang', 'gallbladder', 'gall bladder', 'biliary' , 'pancreas', 'pancreat', 'wilson disease',\n",
    "       'wilsons disease', 'liver fibrosis' ,'liver cirrhosis', 'nafld', 'hemochromatosis', 'haemochromatosis']\n",
    "\n",
    "spec['hep_text'] = np.where(groups['text'].str.contains(' liver '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hep_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hep_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hep_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "295fda5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 30722, '1': 3457})\n"
     ]
    }
   ],
   "source": [
    "## RESPIRATORY / resp\n",
    "\n",
    "## text\n",
    "text = ['respiratory', 'pneumonia', 'lung cancer', 'lung disease', 'lung nodule', 'pulmonary', 'asthma', 'obstructive sleep ap',\n",
    "       'copd', 'pleura', 'mesothelioma', 'lung fibrosis', 'lung adeno', 'nsclc', 'interstitial lung', 'occupational lung', 'tuberculosis',\n",
    "       'bronch']\n",
    "\n",
    "spec['resp_text'] = np.where(groups['text'].str.contains(' lung '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['resp_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['resp_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['resp_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "931578e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33598, '1': 581})\n"
     ]
    }
   ],
   "source": [
    "#### PNEUMONIA / pneum\n",
    "\n",
    "## text\n",
    "text = ['respiratory infection', 'pulmonary infection', 'pneumonia', 'alveolar consolidation', 'lung consolidation', 'lung infection',\n",
    "       'pulmonary consolidation']\n",
    "\n",
    "spec['pneum_text'] = np.where(groups['text'].str.contains('lower respiratory tract infection'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['pneum_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['pneum_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['pneum_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cda3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33909, '1': 270})\n"
     ]
    }
   ],
   "source": [
    "#### OBSTRUCTIVE SLEEP / osa\n",
    "\n",
    "## text\n",
    "text = ['obstructive sleep ap', 'sleep apnoea']\n",
    "\n",
    "spec['osa_text'] = np.where(groups['text'].str.contains('sleep apnea'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['osa_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['osa_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['osa_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "161d6d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34126, '1': 53})\n"
     ]
    }
   ],
   "source": [
    "#### PULMONARY EMBOLISM / pe\n",
    "\n",
    "## text\n",
    "text = ['saddle embol', 'pulmonary angiogr']\n",
    "\n",
    "spec['pe_text'] = np.where(groups['text'].str.contains('pulmonary embol'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['pe_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['pe_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['pe_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5465503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33865, '1': 314})\n"
     ]
    }
   ],
   "source": [
    "#### PUBLIC HEALTH / pubh\n",
    "\n",
    "## text\n",
    "spec['pubh_text'] = np.where(groups['text'].str.contains(\"public health\"), \"1\", \"0\")\n",
    "spec['pubh_text'] = np.where(groups['text'].str.contains(\"population health\"), \"1\", spec['pubh_text'])\n",
    "spec['pubh_text'] = np.where(groups['text'].str.contains(\"health protection\"), \"1\", spec['pubh_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['pubh_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09055c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 26382, '1': 7797})\n"
     ]
    }
   ],
   "source": [
    "## NERVOUS SYSTEM / neuro\n",
    "\n",
    "## text\n",
    "text = ['neuro', 'brain', 'nervous system', 'multiple sclerosis', 'amyotrophic', 'motor neuron disease',\n",
    "       'dementia', 'cognitive impairment', 'alzheimer', 'epilepsy', 'parkinson', 'dyskinesia', 'cerebellar', 'cerebral',\n",
    "       'guillain', 'myelin', 'migraine', 'headache', 'meningeal', 'meningitis', 'encephalitis', 'ischemic stroke', 'ischaemic stroke',\n",
    "       'hemorrhagic stroke', 'haemorrhagic stroke', 'embolic stroke', 'thrombotic stroke', 'myasthenia', 'movement disorder',\n",
    "       'subdural', 'extradural', 'arachnoid', 'glioma', 'astrocytoma', 'glioblast', ' mci ', 'cerebrovascular']\n",
    "\n",
    "spec['neuro_text'] = np.where(groups['text'].str.contains('white matter'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['neuro_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['neuro_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['neuro_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"aneurysm\")) , \"1\", spec['neuro_text'])\n",
    "spec['neuro_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"aneurysm\")) , \"1\", spec['neuro_text'])   \n",
    "spec['neuro_text'] = np.where((groups['text'].str.contains(\"cranial\")) &\n",
    "                             (groups['text'].str.contains(\"aneurysm\")) , \"1\", spec['neuro_text'])    \n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['neuro_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a435932c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33473, '1': 706})\n"
     ]
    }
   ],
   "source": [
    "#### STROKE/bleed / cva\n",
    "\n",
    "## text\n",
    "text = ['cerebrovascular', 'ischemic stroke', 'ischaemic stroke', 'hemorrhagic stroke', 'haemorrhagic stroke', \n",
    "        'embolic stroke', 'thrombotic stroke', 'subarachnoid hemorrhage', 'subarachnoid haemorrhage', 'cerebral artery stroke',\n",
    "       'cerebral artery infarct', 'malignant middle cerebral', 'malignant mca']\n",
    "\n",
    "spec['cva_text'] = np.where(groups['text'].str.contains(' ich '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['cva_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['cva_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"infarct\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"infarct\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"stroke\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"stroke\")) , \"1\", spec['cva_text'])    \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"bleed\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"bleed\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"haemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"haemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cranial\")) &\n",
    "                             (groups['text'].str.contains(\"haemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"hemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"hemorrhage\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cranial\")) &\n",
    "                             (groups['text'].str.contains(\"hemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"aneurysm\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"aneurysm\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cranial\")) &\n",
    "                             (groups['text'].str.contains(\"aneurysm\")) , \"1\", spec['cva_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['cva_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e018284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33357, '1': 822})\n"
     ]
    }
   ],
   "source": [
    "#### EPILEPSY / epilep\n",
    "\n",
    "## text\n",
    "spec['epilep_text'] = np.where(groups['text'].str.contains(\"epilep\"), \"1\", \"0\")\n",
    "spec['epilep_text'] = np.where(groups['text'].str.contains(\"seizure\"), \"1\", spec['epilep_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['epilep_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "27454ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32660, '1': 1519})\n"
     ]
    }
   ],
   "source": [
    "#### DEMENTIA / alzh\n",
    "\n",
    "## text\n",
    "text = ['dementia', 'cognitive impairment', 'alzheimer', 'cognitive dysfunction', 'cognitive decline', 'lewy body',\n",
    "       'huntington', 'progressive supranuclear', 'corticobasal degen']\n",
    "\n",
    "spec['alzh_text'] = np.where(groups['text'].str.contains(' mci '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['alzh_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['alzh_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['alzh_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "448b1bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 30436, '1': 3743})\n"
     ]
    }
   ],
   "source": [
    "## CARDIOVASCULAR / cvs\n",
    "\n",
    "## text\n",
    "text = ['cardiac', 'cardiovascular', 'cardial', 'cardiol', 'carditis', 'cardium', 'atherosclerosis', 'coronary', 'heart disease',\n",
    "       'cardiomegaly', 'cardiomyopathy', 'valve disease', 'mitral', 'tricuspid', 'pulmonary valve', 'aortic', 'atrial', 'heart failure',\n",
    "       'ventricular failure', 'right heart', 'left heart', 'cor pulm', 'hypertension', 'vascular disease', 'arrhythmia', \n",
    "       'vena cava', 'venous insuff', 'echocard', 'electrocard', 'sinus node', 'sinoatrial node', ' ecg', ' ekg', 'ventricular tachy', 'ventricular fibrillation',\n",
    "       'ischemic heart', 'ischaemic heart', 'peripheral vascular']\n",
    "\n",
    "spec['cvs_text'] = np.where(groups['text'].str.contains('cardiac'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['cvs_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['cvs_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['cvs_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3635034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33229, '1': 950})\n"
     ]
    }
   ],
   "source": [
    "#### ISCHAEMIC HEART DISEASE / ihd\n",
    "\n",
    "## text\n",
    "text = ['coronary', 'cardiac risk', 'cardiovascular risk', 'cardiac stent',\n",
    "       'ischemic heart', 'ischaemic heart', 'cardial infarction']\n",
    "\n",
    "spec['ihd_text'] = np.where(groups['text'].str.contains('heart attack'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['ihd_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['ihd_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"ischemi\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"ischaemi\")) , \"1\", spec['ihd_text'])    \n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"infarction\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"infarction\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"angio\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"angio\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"atherosclero\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"atherosclero\")) , \"1\", spec['ihd_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['ihd_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28f5d458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36978</th>\n",
       "      <td>development of novel artificial intelligence to detect the presence of clinically meaningful coronary atherosclerotic stenosis in major branch from coronary angiography video the clinically meaningful coronary stenosis is diagnosed by trained interventional cardiologists whether artificial intelligence ai could detect coronary stenosis from cag video is unclear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85013</th>\n",
       "      <td>heart rate monitoring and therapeutic devices: a wavelet transform based approach for the modeling and classification of congestive heart failure heart rate monitoring and therapeutic devices include real-time sensing capabilities reflecting the state of the heart current circuitry can be interpreted as a cardiac electrical signal compression algorithm representing the time signal information into a single event description of the cardiac activity it is observed that some detection techniques developed for ecg signal detection like artificial neural network, genetic algorithm, hilbert transform, hidden markov model are some sophisticated algorithms which provide suitable results but their implementation on a silicon chip is very complicated due to less complexity and high performance, wavelet transform based approaches are widely used in this paper, after a thorough analysis of various wavelet transforms, it is found that biorthogonal wavelet transform is best suited to detect ecg signals qrs complex the main steps involved in ecg detection process consist of de-noising and locating different ecg peaks using adaptive slope prediction thresholding furthermore, the significant challenges involved in the wireless transmission of ecg data are data conversion and power consumption as medical regulatory boards demand a lossless compression technique, lossless compression technique with a high bit compression ratio is highly required furthermore, in this work, lzma based ecg data compression technique is proposed the proposed methodology achieves the highest signal to noise ratio, and lowest root mean square error also, the proposed ecg detection technique is capable of distinguishing accurately between healthy, myocardial infarction, congestive heart failure and coronary artery disease patients with a detection accuracy, sensitivity, specificity, and error of 9992%, 9994%, 9992% and 00013, respectively the use of lzma data compression of ecg data achieves a high compression ratio of 1884 the advantages and effectiveness of the proposed algorithm are verified by comparing with the existing methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104830</th>\n",
       "      <td>intelligence system for diagnosis level of coronary heart disease with k-star algorithm coronary heart disease is the leading cause of death worldwide, and it is important to diagnose the level of the disease intelligence systems for diagnosis proved can be used to support diagnosis of the disease unfortunately, most of the data available between the level/type of coronary heart disease is unbalanced as a result system performance is low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124787</th>\n",
       "      <td>use of genetic programming, logistic regression, and artificial neural nets to predict readmission after coronary artery bypass surgery as many as 14 % of patients undergoing coronary artery bypass surgery are readmitted within 30 days readmission is usually the result of morbidity and may lead to death the purpose of this study is to develop and compare statistical and genetic programming models to predict readmission patients were divided into separate construction and validation populations using 88 variables, logistic regression, genetic programs, and artificial neural nets were used to develop predictive models models were first constructed and tested on the construction populations, then validated on the validation population areas under the receiver operator characteristic curves au roc were used to compare the models two hundred and two patients 76 % in the 2,644 patient construction group and 216 80 % of the 2,711 patient validation group were re-admitted within 30 days of cabg surgery logistic regression predicted readmission with au roc = 675 ± 021 in the construction group genetic programs significantly improved the accuracy, au roc = 767 ± 001, p &lt; 001 artificial neural nets were less accurate with au roc = 0597 ± 001 in the construction group predictive accuracy of all three techniques fell in the validation group however, the accuracy of genetic programming au roc = 654 ± 001 was still trivially but statistically non-significantly better than that of the logistic regression au roc = 644 ± 020, p = 61 genetic programming and logistic regression provide alternative methods to predict readmission that are similarly accurate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34005</th>\n",
       "      <td>deep learning-based detection of early renal function impairment using retinal fundus images: model development and validation retinal imaging has been applied for detecting eye diseases and cardiovascular risks using deep learning-based methods furthermore, retinal microvascular and structural changes were found in renal function impairments however, a deep learning-based method using retinal images for detecting early renal function impairment has not yet been well studied</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61640</th>\n",
       "      <td>automated plaque classification using computed tomography angiography and gabor transformations cardiovascular diseases are the primary cause of death globally these are often associated with atherosclerosis this inflammation process triggers important variations in the coronary arteries ca and can lead to coronary artery disease cad the presence of ca calcification cac has recently been shown to be a strong predictor of cad in this clinical setting, computed tomography angiography cta has begun to play a crucial role as a non-intrusive imaging method to characterize and study ca plaques herein, we describe an automated algorithm to classify plaque as either normal, calcified, or non-calcified using 2646 cta images acquired from 73 patients the automated technique is based on various features that are extracted from the gabor transform of the acquired cta images specifically, seven features are extracted from the gabor coefficients : energy, and kapur, max, rényi, shannon, vajda, and yager entropies the features were then ordered based on the f-value and input to numerous classification methods to achieve the best classification accuracy with the least number of features moreover, two well-known feature reduction techniques were employed, and the features acquired were also ranked according to f-value and input to several classifiers the best classification results were obtained using all computed features without the employment of feature reduction, using a probabilistic neural network an accuracy, positive predictive value, sensitivity, and specificity of 8909%, 9170%, 9183% and 8370% was obtained, respectively based on these results, it is evident that the technique can be helpful in the automated classification of plaques present in cta images, and may become an important tool to reduce procedural costs and patient radiation dose this could also aid clinicians in plaque diagnostics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169598</th>\n",
       "      <td>models to predict cardiovascular risk: comparison of cart, multilayer perceptron and logistic regression the estimate of a multivariate risk is now required in guidelines for cardiovascular prevention limitations of existing statistical risk models lead to explore machine-learning methods this study evaluates the implementation and performance of a decision tree cart and a multilayer perceptron mlp to predict cardiovascular risk from real data the study population was randomly splitted in a learning set n = 10,296 and a test set n = 5,148 cart and the mlp were implemented at their best performance on the learning set and applied on the test set and compared to a logistic model implementation, explicative and discriminative performance criteria are considered, based on roc analysis areas under roc curves and their 95% confidence interval are 078 075-081, 078 075-080 and 076 073-079 respectively for logistic regression, mlp and cart given their implementation and explicative characteristics, these methods can complement existing statistical models and contribute to the interpretation of risk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43066</th>\n",
       "      <td>using intravascular ultrasound image-based fluid-structure interaction models and machine learning methods to predict human coronary plaque vulnerability change plaque vulnerability prediction is of great importance in cardiovascular research in vivo follow-up intravascular ultrasound ivus coronary plaque data were acquired from nine patients to construct fluid-structure interaction models to obtain plaque biomechanical conditions morphological plaque vulnerability index mpvi was defined to measure plaque vulnerability the generalized linear mixed regression model glmm, support vector machine svm and random forest rf were introduced to predict mpvi change δmpvi = mpvi&lt;sub&gt;follow-up&lt;/sub&gt;‒mpvi&lt;sub&gt;baseline&lt;/sub&gt; using ten risk factors at baseline the combination of mean wall thickness, lumen area, plaque area, critical plaque wall stress, and mpvi was the best predictor using rf with the highest prediction accuracy 9147%, compared to 9078% from svm, and 8556% from glmm machine learning method rf improved the prediction accuracy by 591% over that from glmm mpvi was the best single risk factor using both glmm 8209% and rf 7853% while plaque area was the best using svm 8129%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20746</th>\n",
       "      <td>machine learning for patient risk stratification: standing on, or looking over, the shoulders of clinicians machine learning can help clinicians to make individualized patient predictions only if researchers demonstrate models that contribute novel insights, rather than learning the most likely next step in a set of actions a clinician will take we trained deep learning models using only clinician-initiated, administrative data for 429 million admissions using three subsets of data: demographic data only, demographic data and information available at admission, and the previous data plus charges recorded during the first day of admission models trained on charges during the first day of admission achieve performance close to published full emr-based benchmarks for inpatient outcomes: inhospital mortality 089 auc, prolonged length of stay 082 auc, and 30-day readmission rate 071 auc similar performance between models trained with only clinician-initiated data and those trained with full emr data purporting to include information about patient state and physiology should raise concern in the deployment of these models furthermore, these models exhibited significant declines in performance when evaluated over only myocardial infarction mi patients relative to models trained over mi patients alone, highlighting the importance of physician diagnosis in the prognostic performance of these models these results provide a benchmark for predictive accuracy trained only on prior clinical actions and indicate that models with similar performance may derive their signal by looking over clinicians shoulders-using clinical behavior as the expression of preexisting intuition and suspicion to generate a prediction for models to guide clinicians in individual decisions, performance exceeding these benchmarks is necessary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91858</th>\n",
       "      <td>risk prediction model for in-hospital mortality in women with st-elevation myocardial infarction: a machine learning approach studies had shown that mortality due to st-elevation myocardial infarction stemi is higher in women compared with men the purpose of this study is to develop and validate prediction models for all-cause in-hospital mortality in women admitted with stemi using logistic regression and random forest, and to compare the performance and validity of the different models</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39460</th>\n",
       "      <td>prediction of revascularization by coronary ct angiography using a machine learning ischemia risk score the machine learning ischemia risk score ml-irs is a machine learning-based algorithm designed to identify hemodynamically significant coronary disease using quantitative coronary computed tomography angiography ccta the purpose of this study was to examine whether the ml-irs can predict revascularization in patients referred for invasive coronary angiography ica after ccta</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32110</th>\n",
       "      <td>machine learning methods for prediction of hospital mortality in patients with coronary heart disease after coronary artery bypass grafting aim      to compare the accuracy of predicting an in-hospital fatal outcome for models based on current machine-learning technologies in patients with ischemic heart disease ihd after coronary bypass cb surgerymaterial and methods  a retrospective analysis of 866 electronic medical records was performed for patients 685 men and 181 women who have had a cb surgery for ihd in 2008-2018 results of clinical, laboratory, and instrumental evaluations obtained prior to the cb surgery were analyzed patients were divided into two groups: group 1 included 35 4 % patients who died within the first 20 days of cb, and group 2 consisted of 831 96 % patients with a beneficial outcome of the surgery predictors of the in-hospital fatal outcome were identified by a multistep selection procedure with analysis of statistical hypotheses and calculation of weight coefficients for construction of models and verification of predictors, machine-learning methods were used, including the multifactorial logistic regression lr, random forest rf, and artificial neural networks ann model accuracy was evaluated by three metrics: area under the roc curve auc, sensitivity, and specificity cross validation of the models was performed on test samples, and the control validation was performed on a cohort of patients with ihd after cb, whose data were not used in development of the modelsresults the following 7 risk factors for in-hospital fatal outcome with the greatest predictive potential were isolated from the euroscore ii scale: ejection fraction ef &amp;lt;30 %, ef 30-50 %, age of patients with recent mi, damage of peripheral arterial circulation, urgency of cb, functional class iii-iv chronic heart failure, and 5 additional predictors, including heart rate, systolic blood pressure, presence of aortic stenosis, posterior left ventricular lv wall relative thickness index rti, and lv relative mass index lvrmi the models developed by the authors using lr, rf and ann methods had higher auc values and sensitivity compared to the classical euroscore ii scale the ann models including the rti and lvrmi predictors demonstrated a maximum level of prognostic accuracy, which was illustrated by values of the quality metrics, auc 93 %, sensitivity 90 %, and specificity 96 % the predictive robustness of the models was confirmed by results of the control validationconclusion      the use of current machine-learning technologies allowed developing a novel algorithm for selection of predictors and highly accurate models for predicting an in-hospital fatal outcome after cb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59528</th>\n",
       "      <td>machine learning reveals serum sphingolipids as cholesterol-independent biomarkers of coronary artery disease backgroundceramides are sphingolipids that play causative roles in diabetes and heart disease, with their serum levels measured clinically as biomarkers of cardiovascular disease cvdmethodswe performed targeted lipidomics on serum samples from individuals with familial coronary artery disease cad n = 462 and population-based controls n = 212 to explore the relationship between serum sphingolipids and cad, using unbiased machine learning to identify sphingolipid species positively associated with cadresultsnearly every sphingolipid measured n = 30 of 32 was significantly elevated in subjects with cad compared with measurements in population controls we generated a novel sphingolipid-inclusive cad risk score, termed sic, that demarcates patients with cad independently and more effectively than conventional clinical cvd biomarkers including serum ldl cholesterol and triglycerides this new metric comprises several minor lipids that likely serve as measures of flux through the ceramide biosynthesis pathway rather than the abundant deleterious ceramide species that are included in other ceramide-based scoresconclusionthis study validates serum ceramides as candidate biomarkers of cvd and suggests that comprehensive sphingolipid panels should be considered as measures of cvdfundingthe nih dk112826, dk108833, dk115824, dk116888, and dk116450; the juvenile diabetes research foundation jdrf 3-sra-2019-768-a-b; the american diabetes association; the american heart association; the margolis foundation; the national cancer institute, nih 5r00ca218694-03; and the huntsman cancer institute cancer center support grant p30ca040214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30197</th>\n",
       "      <td>coronary angiography image segmentation based on pspnet coronary artery disease cad is known to have high prevalence, high disability and mortality the incidence and mortality of cardiovascular disease are also gradually increasing worldwide therefore, our paper proposes to use a more efficient image processing method to extract accurate vascular structures from vascular images by combining computer vision and deep learning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59813</th>\n",
       "      <td>deep learning analysis of coronary arteries in cardiac ct angiography for detection of patients requiring invasive coronary angiography in patients with obstructive coronary artery disease, the functional significance of a coronary artery stenosis needs to be determined to guide treatment this is typically established through fractional flow reserve ffr measurement, performed during invasive coronary angiography ica we present a method for automatic and non-invasive detection of patients requiring ica, employing deep unsupervised analysis of complete coronary arteries in cardiac ct angiography ccta images we retrospectively collected ccta scans of 187 patients, 137 of them underwent invasive ffr measurement in 192 different coronary arteries these ffr measurements served as a reference standard for the functional significance of the coronary stenosis the centerlines of the coronary arteries were extracted and used to reconstruct straightened multi-planar reformatted mpr volumes to automatically identify arteries with functionally significant stenosis that require ica, each mpr volume was encoded into a fixed number of encodings using two disjoint 3d and 1d convolutional autoencoders performing spatial and sequential encodings, respectively thereafter, these encodings were employed to classify arteries using a support vector machine classifier the detection of coronary arteries requiring invasive evaluation, evaluated using repeated cross-validation experiments, resulted in an area under the receiver operating characteristic curve of 081 ± 002 on the artery-level, and 087 ± 002 on the patient-level the results demonstrate the feasibility of automatic non-invasive detection of patients that require ica and possibly subsequent coronary artery intervention this could potentially reduce the number of patients that unnecessarily undergo ica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163100</th>\n",
       "      <td>effects of neural network feedback to physicians on admit/discharge decision for emergency department patients with chest pain neural networks can risk-stratify emergency department ed patients with potential acute coronary syndromes with a high specificity, potentially facilitating ed discharge of patients to home we hypothesized that the use of real-time neural networks would decrease the admission rate for ed chest pain patients</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23322</th>\n",
       "      <td>application of machine learning and laser optical-acoustic spectroscopy to study the profile of exhaled air volatile markers of acute myocardial infarction conventional acute myocardial infarction ami diagnosis is quite accurate and has proved its effectiveness however, despite this, discovering more operative methods of this disease detection is underway from this point of view, the application of exhaled air analysis for a similar diagnosis is valuable the aim of the paper is to research effective machine learning algorithms for the predictive model for ami diagnosis constructing, using exhaled air spectral data the target group included 30 patients with primary myocardial infarction the control group included 42 healthy volunteers the laserbreezelaser gas analyzer special technologies ltd, russia, based on the dual-channel resonant photoacoustic detector cell and optical parametric oscillator as the laser source, had been used the pattern recognition approach was applied in the same manner for the set of extracted concentrations of ami volatile markers and the set of absorption coefficients in a most informative spectral range 2900 ± 0125&lt;i&gt;µ&lt;/i&gt;m the created predictive model based on the set of absorption coefficients provided 086 of the mean values of both the sensitivity and specificity when linear support vector machine svm combined with principal component analysis was used the created predictive model based on using six volatile ami markers c&lt;sub&gt;5&lt;/sub&gt;h&lt;sub&gt;12&lt;/sub&gt;, n&lt;sub&gt;2&lt;/sub&gt;o, no&lt;sub&gt;2&lt;/sub&gt;, c&lt;sub&gt;2&lt;/sub&gt;h&lt;sub&gt;4&lt;/sub&gt;, co, co&lt;sub&gt;2&lt;/sub&gt; provided 082 and 093 of the mean values of the sensitivity and specificity, respectively, when linear svm was used</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116697</th>\n",
       "      <td>usefulness of the heart-rate variability complex for predicting cardiac mortality after acute myocardial infarction previous studies indicate that decreased heart-rate variability hrv is related to the risk of death in patients after acute myocardial infarction ami however, the conventional indices of hrv have poor predictive value for mortality our aim was to develop novel predictive models based on support vector machine svm to study the integrated features of hrv for improving risk stratification after ami</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83713</th>\n",
       "      <td>machine learning techniques in cardiac risk assessment the objective of this study was to predict the mortality risk of patients during or shortly after cardiac surgery by using machine learning techniques and their learning abilities from collected data</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36341</th>\n",
       "      <td>baseline and dynamic risk predictors of appropriate implantable cardioverter defibrillator therapy background current approaches fail to separate patients at high versus low risk for ventricular arrhythmias owing to overreliance on a snapshot left ventricular ejection fraction measure we used statistical machine learning to identify important cardiac imaging and time-varying risk predictors methods and results three hundred eighty-two cardiomyopathy patients left ventricular ejection fraction ≤35% underwent cardiac magnetic resonance before primary prevention implantable cardioverter defibrillator insertion the primary end point was appropriate implantable cardioverter defibrillator discharge or sudden death patient characteristics; serum biomarkers of inflammation, neurohormonal status, and injury; and cardiac magnetic resonance-measured left ventricle and left atrial indices and myocardial scar burden were assessed at baseline time-varying covariates comprised interval heart failure hospitalizations and left ventricular ejection fractions a random forest statistical method for survival, longitudinal, and multivariable outcomes incorporating baseline and time-varying variables was compared with 1 seattle heart failure model scores and 2 random forest survival and cox regression models incorporating baseline characteristics with and without imaging variables age averaged 57±13 years with 28% women, 66% white, 51% ischemic, and follow-up time of 59±23 years the primary end point n=75 occurred at 33±24 years random forest statistical method for survival, longitudinal, and multivariable outcomes with baseline and time-varying predictors had the highest area under the receiver operating curve, median 088 95% ci, 075-096 top predictors comprised heart failure hospitalization, left ventricle scar, left ventricle and left atrial volumes, left atrial function, and interleukin-6 level; heart failure accounted for 67% of the variation explained by the prediction, imaging 27%, and interleukin-6 2% serial left ventricular ejection fraction was not a significant predictor conclusions hospitalization for heart failure and baseline cardiac metrics substantially improve ventricular arrhythmic risk prediction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "36978                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        development of novel artificial intelligence to detect the presence of clinically meaningful coronary atherosclerotic stenosis in major branch from coronary angiography video the clinically meaningful coronary stenosis is diagnosed by trained interventional cardiologists whether artificial intelligence ai could detect coronary stenosis from cag video is unclear   \n",
       "85013                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    heart rate monitoring and therapeutic devices: a wavelet transform based approach for the modeling and classification of congestive heart failure heart rate monitoring and therapeutic devices include real-time sensing capabilities reflecting the state of the heart current circuitry can be interpreted as a cardiac electrical signal compression algorithm representing the time signal information into a single event description of the cardiac activity it is observed that some detection techniques developed for ecg signal detection like artificial neural network, genetic algorithm, hilbert transform, hidden markov model are some sophisticated algorithms which provide suitable results but their implementation on a silicon chip is very complicated due to less complexity and high performance, wavelet transform based approaches are widely used in this paper, after a thorough analysis of various wavelet transforms, it is found that biorthogonal wavelet transform is best suited to detect ecg signals qrs complex the main steps involved in ecg detection process consist of de-noising and locating different ecg peaks using adaptive slope prediction thresholding furthermore, the significant challenges involved in the wireless transmission of ecg data are data conversion and power consumption as medical regulatory boards demand a lossless compression technique, lossless compression technique with a high bit compression ratio is highly required furthermore, in this work, lzma based ecg data compression technique is proposed the proposed methodology achieves the highest signal to noise ratio, and lowest root mean square error also, the proposed ecg detection technique is capable of distinguishing accurately between healthy, myocardial infarction, congestive heart failure and coronary artery disease patients with a detection accuracy, sensitivity, specificity, and error of 9992%, 9994%, 9992% and 00013, respectively the use of lzma data compression of ecg data achieves a high compression ratio of 1884 the advantages and effectiveness of the proposed algorithm are verified by comparing with the existing methods   \n",
       "104830                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         intelligence system for diagnosis level of coronary heart disease with k-star algorithm coronary heart disease is the leading cause of death worldwide, and it is important to diagnose the level of the disease intelligence systems for diagnosis proved can be used to support diagnosis of the disease unfortunately, most of the data available between the level/type of coronary heart disease is unbalanced as a result system performance is low   \n",
       "124787                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  use of genetic programming, logistic regression, and artificial neural nets to predict readmission after coronary artery bypass surgery as many as 14 % of patients undergoing coronary artery bypass surgery are readmitted within 30 days readmission is usually the result of morbidity and may lead to death the purpose of this study is to develop and compare statistical and genetic programming models to predict readmission patients were divided into separate construction and validation populations using 88 variables, logistic regression, genetic programs, and artificial neural nets were used to develop predictive models models were first constructed and tested on the construction populations, then validated on the validation population areas under the receiver operator characteristic curves au roc were used to compare the models two hundred and two patients 76 % in the 2,644 patient construction group and 216 80 % of the 2,711 patient validation group were re-admitted within 30 days of cabg surgery logistic regression predicted readmission with au roc = 675 ± 021 in the construction group genetic programs significantly improved the accuracy, au roc = 767 ± 001, p < 001 artificial neural nets were less accurate with au roc = 0597 ± 001 in the construction group predictive accuracy of all three techniques fell in the validation group however, the accuracy of genetic programming au roc = 654 ± 001 was still trivially but statistically non-significantly better than that of the logistic regression au roc = 644 ± 020, p = 61 genetic programming and logistic regression provide alternative methods to predict readmission that are similarly accurate    \n",
       "34005                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    deep learning-based detection of early renal function impairment using retinal fundus images: model development and validation retinal imaging has been applied for detecting eye diseases and cardiovascular risks using deep learning-based methods furthermore, retinal microvascular and structural changes were found in renal function impairments however, a deep learning-based method using retinal images for detecting early renal function impairment has not yet been well studied   \n",
       "61640                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     automated plaque classification using computed tomography angiography and gabor transformations cardiovascular diseases are the primary cause of death globally these are often associated with atherosclerosis this inflammation process triggers important variations in the coronary arteries ca and can lead to coronary artery disease cad the presence of ca calcification cac has recently been shown to be a strong predictor of cad in this clinical setting, computed tomography angiography cta has begun to play a crucial role as a non-intrusive imaging method to characterize and study ca plaques herein, we describe an automated algorithm to classify plaque as either normal, calcified, or non-calcified using 2646 cta images acquired from 73 patients the automated technique is based on various features that are extracted from the gabor transform of the acquired cta images specifically, seven features are extracted from the gabor coefficients : energy, and kapur, max, rényi, shannon, vajda, and yager entropies the features were then ordered based on the f-value and input to numerous classification methods to achieve the best classification accuracy with the least number of features moreover, two well-known feature reduction techniques were employed, and the features acquired were also ranked according to f-value and input to several classifiers the best classification results were obtained using all computed features without the employment of feature reduction, using a probabilistic neural network an accuracy, positive predictive value, sensitivity, and specificity of 8909%, 9170%, 9183% and 8370% was obtained, respectively based on these results, it is evident that the technique can be helpful in the automated classification of plaques present in cta images, and may become an important tool to reduce procedural costs and patient radiation dose this could also aid clinicians in plaque diagnostics   \n",
       "169598                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                models to predict cardiovascular risk: comparison of cart, multilayer perceptron and logistic regression the estimate of a multivariate risk is now required in guidelines for cardiovascular prevention limitations of existing statistical risk models lead to explore machine-learning methods this study evaluates the implementation and performance of a decision tree cart and a multilayer perceptron mlp to predict cardiovascular risk from real data the study population was randomly splitted in a learning set n = 10,296 and a test set n = 5,148 cart and the mlp were implemented at their best performance on the learning set and applied on the test set and compared to a logistic model implementation, explicative and discriminative performance criteria are considered, based on roc analysis areas under roc curves and their 95% confidence interval are 078 075-081, 078 075-080 and 076 073-079 respectively for logistic regression, mlp and cart given their implementation and explicative characteristics, these methods can complement existing statistical models and contribute to the interpretation of risk   \n",
       "43066                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              using intravascular ultrasound image-based fluid-structure interaction models and machine learning methods to predict human coronary plaque vulnerability change plaque vulnerability prediction is of great importance in cardiovascular research in vivo follow-up intravascular ultrasound ivus coronary plaque data were acquired from nine patients to construct fluid-structure interaction models to obtain plaque biomechanical conditions morphological plaque vulnerability index mpvi was defined to measure plaque vulnerability the generalized linear mixed regression model glmm, support vector machine svm and random forest rf were introduced to predict mpvi change δmpvi = mpvi<sub>follow-up</sub>‒mpvi<sub>baseline</sub> using ten risk factors at baseline the combination of mean wall thickness, lumen area, plaque area, critical plaque wall stress, and mpvi was the best predictor using rf with the highest prediction accuracy 9147%, compared to 9078% from svm, and 8556% from glmm machine learning method rf improved the prediction accuracy by 591% over that from glmm mpvi was the best single risk factor using both glmm 8209% and rf 7853% while plaque area was the best using svm 8129%   \n",
       "20746                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         machine learning for patient risk stratification: standing on, or looking over, the shoulders of clinicians machine learning can help clinicians to make individualized patient predictions only if researchers demonstrate models that contribute novel insights, rather than learning the most likely next step in a set of actions a clinician will take we trained deep learning models using only clinician-initiated, administrative data for 429 million admissions using three subsets of data: demographic data only, demographic data and information available at admission, and the previous data plus charges recorded during the first day of admission models trained on charges during the first day of admission achieve performance close to published full emr-based benchmarks for inpatient outcomes: inhospital mortality 089 auc, prolonged length of stay 082 auc, and 30-day readmission rate 071 auc similar performance between models trained with only clinician-initiated data and those trained with full emr data purporting to include information about patient state and physiology should raise concern in the deployment of these models furthermore, these models exhibited significant declines in performance when evaluated over only myocardial infarction mi patients relative to models trained over mi patients alone, highlighting the importance of physician diagnosis in the prognostic performance of these models these results provide a benchmark for predictive accuracy trained only on prior clinical actions and indicate that models with similar performance may derive their signal by looking over clinicians shoulders-using clinical behavior as the expression of preexisting intuition and suspicion to generate a prediction for models to guide clinicians in individual decisions, performance exceeding these benchmarks is necessary   \n",
       "91858                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       risk prediction model for in-hospital mortality in women with st-elevation myocardial infarction: a machine learning approach studies had shown that mortality due to st-elevation myocardial infarction stemi is higher in women compared with men the purpose of this study is to develop and validate prediction models for all-cause in-hospital mortality in women admitted with stemi using logistic regression and random forest, and to compare the performance and validity of the different models   \n",
       "39460                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   prediction of revascularization by coronary ct angiography using a machine learning ischemia risk score the machine learning ischemia risk score ml-irs is a machine learning-based algorithm designed to identify hemodynamically significant coronary disease using quantitative coronary computed tomography angiography ccta the purpose of this study was to examine whether the ml-irs can predict revascularization in patients referred for invasive coronary angiography ica after ccta   \n",
       "32110   machine learning methods for prediction of hospital mortality in patients with coronary heart disease after coronary artery bypass grafting aim      to compare the accuracy of predicting an in-hospital fatal outcome for models based on current machine-learning technologies in patients with ischemic heart disease ihd after coronary bypass cb surgerymaterial and methods  a retrospective analysis of 866 electronic medical records was performed for patients 685 men and 181 women who have had a cb surgery for ihd in 2008-2018 results of clinical, laboratory, and instrumental evaluations obtained prior to the cb surgery were analyzed patients were divided into two groups: group 1 included 35 4 % patients who died within the first 20 days of cb, and group 2 consisted of 831 96 % patients with a beneficial outcome of the surgery predictors of the in-hospital fatal outcome were identified by a multistep selection procedure with analysis of statistical hypotheses and calculation of weight coefficients for construction of models and verification of predictors, machine-learning methods were used, including the multifactorial logistic regression lr, random forest rf, and artificial neural networks ann model accuracy was evaluated by three metrics: area under the roc curve auc, sensitivity, and specificity cross validation of the models was performed on test samples, and the control validation was performed on a cohort of patients with ihd after cb, whose data were not used in development of the modelsresults the following 7 risk factors for in-hospital fatal outcome with the greatest predictive potential were isolated from the euroscore ii scale: ejection fraction ef &lt;30 %, ef 30-50 %, age of patients with recent mi, damage of peripheral arterial circulation, urgency of cb, functional class iii-iv chronic heart failure, and 5 additional predictors, including heart rate, systolic blood pressure, presence of aortic stenosis, posterior left ventricular lv wall relative thickness index rti, and lv relative mass index lvrmi the models developed by the authors using lr, rf and ann methods had higher auc values and sensitivity compared to the classical euroscore ii scale the ann models including the rti and lvrmi predictors demonstrated a maximum level of prognostic accuracy, which was illustrated by values of the quality metrics, auc 93 %, sensitivity 90 %, and specificity 96 % the predictive robustness of the models was confirmed by results of the control validationconclusion      the use of current machine-learning technologies allowed developing a novel algorithm for selection of predictors and highly accurate models for predicting an in-hospital fatal outcome after cb   \n",
       "59528                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            machine learning reveals serum sphingolipids as cholesterol-independent biomarkers of coronary artery disease backgroundceramides are sphingolipids that play causative roles in diabetes and heart disease, with their serum levels measured clinically as biomarkers of cardiovascular disease cvdmethodswe performed targeted lipidomics on serum samples from individuals with familial coronary artery disease cad n = 462 and population-based controls n = 212 to explore the relationship between serum sphingolipids and cad, using unbiased machine learning to identify sphingolipid species positively associated with cadresultsnearly every sphingolipid measured n = 30 of 32 was significantly elevated in subjects with cad compared with measurements in population controls we generated a novel sphingolipid-inclusive cad risk score, termed sic, that demarcates patients with cad independently and more effectively than conventional clinical cvd biomarkers including serum ldl cholesterol and triglycerides this new metric comprises several minor lipids that likely serve as measures of flux through the ceramide biosynthesis pathway rather than the abundant deleterious ceramide species that are included in other ceramide-based scoresconclusionthis study validates serum ceramides as candidate biomarkers of cvd and suggests that comprehensive sphingolipid panels should be considered as measures of cvdfundingthe nih dk112826, dk108833, dk115824, dk116888, and dk116450; the juvenile diabetes research foundation jdrf 3-sra-2019-768-a-b; the american diabetes association; the american heart association; the margolis foundation; the national cancer institute, nih 5r00ca218694-03; and the huntsman cancer institute cancer center support grant p30ca040214   \n",
       "30197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        coronary angiography image segmentation based on pspnet coronary artery disease cad is known to have high prevalence, high disability and mortality the incidence and mortality of cardiovascular disease are also gradually increasing worldwide therefore, our paper proposes to use a more efficient image processing method to extract accurate vascular structures from vascular images by combining computer vision and deep learning   \n",
       "59813                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         deep learning analysis of coronary arteries in cardiac ct angiography for detection of patients requiring invasive coronary angiography in patients with obstructive coronary artery disease, the functional significance of a coronary artery stenosis needs to be determined to guide treatment this is typically established through fractional flow reserve ffr measurement, performed during invasive coronary angiography ica we present a method for automatic and non-invasive detection of patients requiring ica, employing deep unsupervised analysis of complete coronary arteries in cardiac ct angiography ccta images we retrospectively collected ccta scans of 187 patients, 137 of them underwent invasive ffr measurement in 192 different coronary arteries these ffr measurements served as a reference standard for the functional significance of the coronary stenosis the centerlines of the coronary arteries were extracted and used to reconstruct straightened multi-planar reformatted mpr volumes to automatically identify arteries with functionally significant stenosis that require ica, each mpr volume was encoded into a fixed number of encodings using two disjoint 3d and 1d convolutional autoencoders performing spatial and sequential encodings, respectively thereafter, these encodings were employed to classify arteries using a support vector machine classifier the detection of coronary arteries requiring invasive evaluation, evaluated using repeated cross-validation experiments, resulted in an area under the receiver operating characteristic curve of 081 ± 002 on the artery-level, and 087 ± 002 on the patient-level the results demonstrate the feasibility of automatic non-invasive detection of patients that require ica and possibly subsequent coronary artery intervention this could potentially reduce the number of patients that unnecessarily undergo ica   \n",
       "163100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               effects of neural network feedback to physicians on admit/discharge decision for emergency department patients with chest pain neural networks can risk-stratify emergency department ed patients with potential acute coronary syndromes with a high specificity, potentially facilitating ed discharge of patients to home we hypothesized that the use of real-time neural networks would decrease the admission rate for ed chest pain patients   \n",
       "23322                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   application of machine learning and laser optical-acoustic spectroscopy to study the profile of exhaled air volatile markers of acute myocardial infarction conventional acute myocardial infarction ami diagnosis is quite accurate and has proved its effectiveness however, despite this, discovering more operative methods of this disease detection is underway from this point of view, the application of exhaled air analysis for a similar diagnosis is valuable the aim of the paper is to research effective machine learning algorithms for the predictive model for ami diagnosis constructing, using exhaled air spectral data the target group included 30 patients with primary myocardial infarction the control group included 42 healthy volunteers the laserbreezelaser gas analyzer special technologies ltd, russia, based on the dual-channel resonant photoacoustic detector cell and optical parametric oscillator as the laser source, had been used the pattern recognition approach was applied in the same manner for the set of extracted concentrations of ami volatile markers and the set of absorption coefficients in a most informative spectral range 2900 ± 0125<i>µ</i>m the created predictive model based on the set of absorption coefficients provided 086 of the mean values of both the sensitivity and specificity when linear support vector machine svm combined with principal component analysis was used the created predictive model based on using six volatile ami markers c<sub>5</sub>h<sub>12</sub>, n<sub>2</sub>o, no<sub>2</sub>, c<sub>2</sub>h<sub>4</sub>, co, co<sub>2</sub> provided 082 and 093 of the mean values of the sensitivity and specificity, respectively, when linear svm was used   \n",
       "116697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                usefulness of the heart-rate variability complex for predicting cardiac mortality after acute myocardial infarction previous studies indicate that decreased heart-rate variability hrv is related to the risk of death in patients after acute myocardial infarction ami however, the conventional indices of hrv have poor predictive value for mortality our aim was to develop novel predictive models based on support vector machine svm to study the integrated features of hrv for improving risk stratification after ami   \n",
       "83713                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     machine learning techniques in cardiac risk assessment the objective of this study was to predict the mortality risk of patients during or shortly after cardiac surgery by using machine learning techniques and their learning abilities from collected data   \n",
       "36341                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            baseline and dynamic risk predictors of appropriate implantable cardioverter defibrillator therapy background current approaches fail to separate patients at high versus low risk for ventricular arrhythmias owing to overreliance on a snapshot left ventricular ejection fraction measure we used statistical machine learning to identify important cardiac imaging and time-varying risk predictors methods and results three hundred eighty-two cardiomyopathy patients left ventricular ejection fraction ≤35% underwent cardiac magnetic resonance before primary prevention implantable cardioverter defibrillator insertion the primary end point was appropriate implantable cardioverter defibrillator discharge or sudden death patient characteristics; serum biomarkers of inflammation, neurohormonal status, and injury; and cardiac magnetic resonance-measured left ventricle and left atrial indices and myocardial scar burden were assessed at baseline time-varying covariates comprised interval heart failure hospitalizations and left ventricular ejection fractions a random forest statistical method for survival, longitudinal, and multivariable outcomes incorporating baseline and time-varying variables was compared with 1 seattle heart failure model scores and 2 random forest survival and cox regression models incorporating baseline characteristics with and without imaging variables age averaged 57±13 years with 28% women, 66% white, 51% ischemic, and follow-up time of 59±23 years the primary end point n=75 occurred at 33±24 years random forest statistical method for survival, longitudinal, and multivariable outcomes with baseline and time-varying predictors had the highest area under the receiver operating curve, median 088 95% ci, 075-096 top predictors comprised heart failure hospitalization, left ventricle scar, left ventricle and left atrial volumes, left atrial function, and interleukin-6 level; heart failure accounted for 67% of the variation explained by the prediction, imaging 27%, and interleukin-6 2% serial left ventricular ejection fraction was not a significant predictor conclusions hospitalization for heart failure and baseline cardiac metrics substantially improve ventricular arrhythmic risk prediction   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "36978         0       0       0           0          0        0       0   \n",
       "85013         0       0       0           0          0        0       0   \n",
       "104830        0       0       0           0          0        0       0   \n",
       "124787        0       0       0           0          0        0       0   \n",
       "34005         0       0       0           0          0        0       0   \n",
       "61640         0       0       0           0          0        0       0   \n",
       "169598        0       0       0           0          0        0       0   \n",
       "43066         0       0       0           0          0        0       0   \n",
       "20746         0       0       0           0          0        0       0   \n",
       "91858         0       0       0           0          0        0       0   \n",
       "39460         0       0       0           0          0        0       0   \n",
       "32110         0       0       0           0          0        0       0   \n",
       "59528         0       0       0           0          0        0       0   \n",
       "30197         0       0       0           0          0        0       0   \n",
       "59813         0       0       0           0          0        0       0   \n",
       "163100        0       1       0           0          0        0       0   \n",
       "23322         0       0       0           0          0        0       0   \n",
       "116697        0       0       0           0          0        0       0   \n",
       "83713         0       0       0           0          0        0       0   \n",
       "36341         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "36978            0            0         0           0        0       0   \n",
       "85013            0            0         0           0        0       0   \n",
       "104830           0            0         0           0        0       0   \n",
       "124787           0            0         0           0        0       0   \n",
       "34005            0            0         0           0        0       0   \n",
       "61640            0            0         0           0        0       0   \n",
       "169598           0            0         0           0        0       0   \n",
       "43066            0            0         0           0        0       0   \n",
       "20746            0            0         0           0        0       0   \n",
       "91858            0            0         0           0        0       0   \n",
       "39460            0            0         0           0        0       0   \n",
       "32110            0            0         0           0        0       0   \n",
       "59528            0            0         0           0        1       0   \n",
       "30197            0            0         0           0        0       0   \n",
       "59813            0            0         0           0        0       0   \n",
       "163100           0            0         0           0        0       0   \n",
       "23322            0            0         0           0        0       0   \n",
       "116697           0            0         0           0        0       0   \n",
       "83713            0            0         0           0        0       0   \n",
       "36341            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "36978            0             0           0            0         0   \n",
       "85013            0             0           0            0         0   \n",
       "104830           0             0           0            0         0   \n",
       "124787           0             0           0            0         0   \n",
       "34005            0             0           0            0         0   \n",
       "61640            0             0           0            0         0   \n",
       "169598           0             0           0            0         0   \n",
       "43066            0             0           0            0         0   \n",
       "20746            0             0           0            0         0   \n",
       "91858            0             0           0            0         0   \n",
       "39460            0             0           0            0         0   \n",
       "32110            0             0           0            0         0   \n",
       "59528            0             0           0            0         0   \n",
       "30197            0             0           0            0         0   \n",
       "59813            0             0           0            0         0   \n",
       "163100           0             0           0            0         0   \n",
       "23322            0             0           0            0         0   \n",
       "116697           0             0           0            0         0   \n",
       "83713            0             0           0            0         0   \n",
       "36341            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "36978           0            0           0            0           0   \n",
       "85013           0            0           0            0           0   \n",
       "104830          0            0           0            0           0   \n",
       "124787          0            0           0            0           0   \n",
       "34005           0            0           0            0           0   \n",
       "61640           0            0           0            0           0   \n",
       "169598          0            0           0            0           0   \n",
       "43066           0            0           0            0           0   \n",
       "20746           0            0           0            0           0   \n",
       "91858           0            0           0            0           0   \n",
       "39460           0            0           0            0           0   \n",
       "32110           0            0           0            0           0   \n",
       "59528           0            0           0            0           0   \n",
       "30197           0            0           0            0           0   \n",
       "59813           0            0           0            0           0   \n",
       "163100          0            0           0            0           0   \n",
       "23322           0            0           0            0           0   \n",
       "116697          0            0           0            0           0   \n",
       "83713           0            0           0            0           0   \n",
       "36341           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "36978             0          0            0        0         0          0   \n",
       "85013             0          0            0        0         0          0   \n",
       "104830            0          0            0        0         0          0   \n",
       "124787            0          0            0        0         0          0   \n",
       "34005             0          0            0        0         0          0   \n",
       "61640             0          0            0        0         0          0   \n",
       "169598            0          0            0        0         0          0   \n",
       "43066             0          0            0        0         0          0   \n",
       "20746             0          0            0        0         0          0   \n",
       "91858             0          0            0        0         0          0   \n",
       "39460             0          0            0        0         0          0   \n",
       "32110             0          0            0        0         0          0   \n",
       "59528             0          0            0        0         0          0   \n",
       "30197             0          0            0        0         0          0   \n",
       "59813             0          0            0        0         0          0   \n",
       "163100            0          0            0        0         0          0   \n",
       "23322             0          0            0        0         0          0   \n",
       "116697            0          0            0        0         0          0   \n",
       "83713             0          0            0        0         0          0   \n",
       "36341             0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "36978        0        0         0          0        0       0         0   \n",
       "85013        0        0         0          0        0       0         0   \n",
       "104830       0        0         0          0        0       0         0   \n",
       "124787       0        0         0          0        0       0         0   \n",
       "34005        0        0         0          0        0       0         0   \n",
       "61640        0        0         0          0        0       0         0   \n",
       "169598       0        0         0          0        0       0         0   \n",
       "43066        0        0         0          0        0       0         0   \n",
       "20746        0        0         0          0        0       0         0   \n",
       "91858        0        0         0          0        0       0         0   \n",
       "39460        0        0         0          0        0       0         0   \n",
       "32110        0        0         0          0        0       0         0   \n",
       "59528        0        0         0          0        0       0         0   \n",
       "30197        0        0         0          0        0       0         0   \n",
       "59813        0        0         0          0        0       0         0   \n",
       "163100       0        0         0          0        0       0         0   \n",
       "23322        0        0         0          0        0       0         0   \n",
       "116697       0        0         0          0        0       0         0   \n",
       "83713        0        0         0          0        0       0         0   \n",
       "36341        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text  \n",
       "36978           0        0           0         0        1        1  \n",
       "85013           0        0           0         0        1        1  \n",
       "104830          0        0           0         0        1        1  \n",
       "124787          0        0           0         0        1        1  \n",
       "34005           0        0           0         0        1        1  \n",
       "61640           0        0           0         0        1        1  \n",
       "169598          0        0           0         0        1        1  \n",
       "43066           0        0           0         0        1        1  \n",
       "20746           0        0           0         0        1        1  \n",
       "91858           0        0           0         0        1        1  \n",
       "39460           0        0           0         0        1        1  \n",
       "32110           0        0           0         0        1        1  \n",
       "59528           0        0           0         0        1        1  \n",
       "30197           0        0           0         0        1        1  \n",
       "59813           0        0           0         0        1        1  \n",
       "163100          0        0           0         0        1        1  \n",
       "23322           0        0           0         0        1        1  \n",
       "116697          0        0           0         0        1        1  \n",
       "83713           0        0           0         0        1        1  \n",
       "36341           1        0           0         0        1        1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['ihd_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1186dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33731, '1': 448})\n"
     ]
    }
   ],
   "source": [
    "#### HEART FAILURE or VENTRICULAR FUNCTION / hf\n",
    "\n",
    "## text\n",
    "text = ['heart failure', 'cardiac failure', 'ejection fraction', 'ventricular dysfunction', 'cardiac dysfunction']\n",
    "\n",
    "spec['hf_text'] = np.where(groups['text'].str.contains(' lvf '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hf_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hf_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"left ventric\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"right ventric\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"left ventric\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"right ventric\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"diastolic\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"diastolic\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"systolic\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"systolic\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hf_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "289aa278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138429</th>\n",
       "      <td>a neuro-fuzzy decision support system for the diagnosis of heart failure a neuro-fuzzy decision support system is proposed for the diagnosis of heart failure the system comprises; knowledge base database, neural networks and fuzzy logic of both the quantitative and qualitative knowledge of the diagnosis of heart failure, neuro-fuzzy inference engine and decision support engine the neural networks employ a multi-layers perception back propagation learning process while the fuzzy logic uses the root sum square inference procedure the neuro-fuzzy inference engine uses a weighted average of the premise and consequent parameters with the fuzzy rules serving as the nodes and the fuzzy sets representing the weights of the nodes the decision support engine carries out the cognitive and emotional filtering of the objective and subjective feelings of the medical practitioner an experimental study of the decision support system was carried out using cases of some patients from three hospitals in nigeria with the assistance of their medical personnel who collected patientsdata over a period of six months the results of the study show that the neuro-fuzzy system provides a highly reliable diagnosis, while the emotional and cognitive filters further refine the diagnosis results by taking care of the contextual elements of medical diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36341</th>\n",
       "      <td>baseline and dynamic risk predictors of appropriate implantable cardioverter defibrillator therapy background current approaches fail to separate patients at high versus low risk for ventricular arrhythmias owing to overreliance on a snapshot left ventricular ejection fraction measure we used statistical machine learning to identify important cardiac imaging and time-varying risk predictors methods and results three hundred eighty-two cardiomyopathy patients left ventricular ejection fraction ≤35% underwent cardiac magnetic resonance before primary prevention implantable cardioverter defibrillator insertion the primary end point was appropriate implantable cardioverter defibrillator discharge or sudden death patient characteristics; serum biomarkers of inflammation, neurohormonal status, and injury; and cardiac magnetic resonance-measured left ventricle and left atrial indices and myocardial scar burden were assessed at baseline time-varying covariates comprised interval heart failure hospitalizations and left ventricular ejection fractions a random forest statistical method for survival, longitudinal, and multivariable outcomes incorporating baseline and time-varying variables was compared with 1 seattle heart failure model scores and 2 random forest survival and cox regression models incorporating baseline characteristics with and without imaging variables age averaged 57±13 years with 28% women, 66% white, 51% ischemic, and follow-up time of 59±23 years the primary end point n=75 occurred at 33±24 years random forest statistical method for survival, longitudinal, and multivariable outcomes with baseline and time-varying predictors had the highest area under the receiver operating curve, median 088 95% ci, 075-096 top predictors comprised heart failure hospitalization, left ventricle scar, left ventricle and left atrial volumes, left atrial function, and interleukin-6 level; heart failure accounted for 67% of the variation explained by the prediction, imaging 27%, and interleukin-6 2% serial left ventricular ejection fraction was not a significant predictor conclusions hospitalization for heart failure and baseline cardiac metrics substantially improve ventricular arrhythmic risk prediction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>coronary ct fractional flow reserve before transcatheter aortic valve replacement: clinical outcomes background the role of ct angiography-derived fractional flow reserve ct-ffr in pre-transcatheter aortic valve replacement tavr assessment is uncertain purpose to evaluate the predictive value of on-site machine learning-based ct-ffr for adverse clinical outcomes in candidates for tavr materials and methods this observational retrospective study included patients with severe aortic stenosis referred to tavr after coronary ct angiography ccta between september 2014 and december 2019 clinical end points comprised major adverse cardiac events mace nonfatal myocardial infarction, unstable angina, cardiac death, or heart failure admission and all-cause mortality ct-ffr was obtained semiautomatically using an on-site machine learning algorithm the ability of ct-ffr abnormal if ≤075 to predict outcomes and improve the predictive value of the current noninvasive work-up was assessed survival analysis was performed, and the c-index was used to assess the performance of each predictive model to compare nested models, the likelihood ratio χ&lt;sup&gt;2&lt;/sup&gt; test was performed results a total of 196 patients mean age ± standard deviation, 75 years ± 11; 110 women 56% were included; the median time of follow-up was 18 months mace occurred in 16% 31 of 196 patients and all-cause mortality in 19% 38 of 196 patients univariable analysis revealed ct-ffr was predictive of mace hazard ratio hr, 41; 95% ci: 16, 108; &lt;i&gt;p&lt;/i&gt; = 01 but not all-cause mortality hr, 12; 95% ci: 06, 22; &lt;i&gt;p&lt;/i&gt; = 63 ct-ffr was independently associated with mace hr, 40; 95% ci: 15, 105; &lt;i&gt;p&lt;/i&gt; = 01 when adjusting for potential confounders adding ct-ffr as a predictor to models that include ccta and clinical data improved their predictive value for mace &lt;i&gt;p&lt;/i&gt; = 002 but not all-cause mortality &lt;i&gt;p&lt;/i&gt; = 67, and it showed good discriminative ability for mace c-index, 071 conclusion ct angiography-derived fractional flow reserve was associated with major adverse cardiac events in candidates for transcatheter aortic valve replacement and improved the predictive value of coronary ct angiography assessment © rsna, 2021 &lt;i&gt;online supplemental material is available for this article&lt;/i&gt; see also the editorial by choe in this issue</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65797</th>\n",
       "      <td>readmission prediction using deep learning on electronic health records unscheduled 30-day readmissions are a hallmark of congestive heart failure chf patients that pose significant health risks and escalate care cost in order to reduce readmissions and curb the cost of care, it is important to initiate targeted intervention programs for patients at risk of readmission this requires identifying high-risk patients at the time of discharge from hospital here, using real data from over 7500 chf patients hospitalized between 2012 and 2016 in sweden, we built and tested a deep learning framework to predict 30-day unscheduled readmission we present a cost-sensitive formulation of long short-term memory lstm neural network using expert features and contextual embedding of clinical concepts this study targets key elements of an electronic health record ehr driven prediction model in a single framework: using both expert and machine derived features, incorporating sequential patterns and addressing the class imbalance problem we evaluate the contribution of each element towards prediction performance roc-auc, f1-measure and cost-savings we show that the model with all key elements achieves higher discrimination ability auc: 077; f1: 051; cost: 22% of maximum possible savings outperforming the reduced models in at least two evaluation metrics additionally, we present a simple financial analysis to estimate annual savings if targeted interventions are offered to high risk patients</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15921</th>\n",
       "      <td>a model-agnostic approach for understanding heart failure risk factors understanding the risk factors for developing heart failure among patients with type 2 diabetes can contribute to preventing deterioration of quality of life for those persons electronic health records ehr provide an opportunity to use sophisticated machine learning models to understand and compare the effect of different risk factors for developing hf as the complexity of the model increases, however, the transparency of the model often decreases to interpret the results, we aimed to develop a model-agnostic approach to shed light on complex models and interpret the effect of features on developing heart failure using the healthfacts ehr database of the cerner ehr, we extracted the records of 723 patients with at least 6 yeas of follow up of type 2 diabetes, of whom 134 developed heart failure using age and comorbidities as features and heart failure as the outcome, we trained logistic regression, random forest, xgboost, neural network, and then applied our proposed approach to rank the effect of each factor on developing heart failure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20254</th>\n",
       "      <td>blood pressure monitoring system using a two-channel ballistocardiogram and convolutional neural networks hypertension is a chronic disease that kills 76 million people worldwide annually a continuous blood pressure monitoring system is required to accurately diagnose hypertension here, a chair-shaped ballistocardiogram bcg-based blood pressure estimation system was developed with no sensors attached to users two experimental sessions were conducted with 30 subjects in the first session, two-channel bcg and blood pressure data were recorded for each subject in the second session, the two-channel bcg and blood pressure data were recorded after running on a treadmill and then resting on the newly developed system the empirical mode decomposition algorithm was used to remove noise in the two-channel bcg, and the instantaneous phase was calculated by applying a hilbert transform to the first intrinsic mode functions after training a convolutional neural network regression model that predicts the systolic and diastolic blood pressures sbp and dbp from the two-channel bcg phase, the results of the first session rest and second session recovery were compared the results confirmed that the proposed model accurately estimates the rapidly rising blood pressure in the recovery state results from the rest sessions satisfied the association for the advancement of medical instrumentation aami international standards the standard deviation of the sbp results in the recovery session exceeded 07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25997</th>\n",
       "      <td>machine learning based congestive heart failure detection using feature importance ranking of multimodal features in this study, we ranked the multimodal features extracted from congestive heart failure chf and normal sinus rhythm nsr subjects we categorized the ranked features into 1 to 5 categories based on empirical receiver operating characteristics eroc values instead of using all multimodal features, we use high ranking features for detection of chf and normal subjects we employed powerful machine learning techniques such as decision tree dt, naïve bayes nb, svm gaussian, svm rbf and svm polynomial the performance was measured in terms of sensitivity, specificity, positive predictive value ppv, negative predictive value npv, accuracy, false positive rate fpr, and area under the receiver operating characteristic curve auc the highest detection performance in terms of accuracy and auc was obtained with all multimodal features using svm gaussian with sensitivity 9306%, specificity 8182%, accuracy 8879% and auc 095 using the top five ranked features, the highest performance was obtained with svm gaussian yields accuracy 8448%, auc 086; top nine ranked features using decision tree and naïve bayes got accuracy 8448%, auc 088; last thirteen ranked features using svm polynomial obtained accuracy 8017%, auc 084 the findings indicate that proposed approach with feature ranking can be very useful for automatic detection of congestive heart failure patients and can be very helpful for further decision making by the clinicians and physicians in order to decrease the mortality rate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80151</th>\n",
       "      <td>an interactive assistant for patients with cardiac implantable electronic devices: a study protocol of the lucy trial patients with chronic heart failure chf and reduced left ventricle ejection fraction benefit from cardiac resynchronization therapy crt and implantable cardioverter defibrillator icd however, increasing numbers of patient with crt and icd devices produce overload of cardiology centers where patients are admitted to ambulatory visits this study aims to find multivariate model predicting the requirement for ambulatory follow-up of cardiac implantable electronic devices ciedsthe lucy study is an observational, cohort, prospective, 2-stage trial as equal number of patients 300 will be included in the first and the second part of the study, finally, 600 patients will be included in the study the inclusion criteria will be: age between 18 and 90 years, chf new york heart association classes i-iii and implanted icd or crt at least 30 days before study inclusion the exclusion criteria will be dementia and other conditions impeding cooperation during the study all patients included in the study will undergo standard ambulatory visit primary endpoint will be defined as any ambulatory visit qualified as necessary due to patients condition or device malfunction diagnose by the cardiologist: any change in pharmacotherapy related to patients clinical status assessed during the visit, any change in tachyarrythmia counter or discriminator status, any change in tachyarrythmia threshold, presence of ventricular undersensing or oversensing, presence of atrial or ventricular ineffective pacing, or devices pocket infection secondary endpoint will be defined as any ambulatory visit qualified as necessary due to the alarm identified via medtronic carelink express mcle: sustained or treated ventricular tachyarrythmia, any not previously diagnosed supraventricular tachyarrythmia, or elective replacement indicatorour study is the first attempt of implementation of the machine learning and elements artificial intelligence in health care optimization of patients with cied the lucy will be an open product, available for additional testing and improvement with supplementary functionalities: quality of life assessment, teleconsultation, video-streaming, automated imagine recognizing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64369</th>\n",
       "      <td>combining structured and unstructured data for predicting risk of readmission for heart failure patients researchers have studied many models for predicting the risk of readmission for heart failure over the last decade most models have used a parametric statistical approach while a few have ventured into using machine learning methods such as statistical natural language processing we created three predictive models by combining these two techniques for the cohort of 1,629 patients from six hosptials using structured data along with their 136,963 clinical notes till their index admission, stored in the emr system over five years the aucs for structured and combined models were very close 06494 and 06447 and that for the unstructured model was 05219 the clinical impact of the models using decision curve analysis showed that, at a threshold predicted probability of 020, the combined model offered 15%, 30%, and 70% net benefit over its individual counterparts, treat-all, and treat-none strategy respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44614</th>\n",
       "      <td>biventricular imaging markers to predict outcomes in non-compaction cardiomyopathy: a machine learning study left ventricular non-compaction cardiomyopathy lvnc is a genetic heart disease, with heart failure, arrhythmias, and embolic events as main clinical manifestations the goal of this study was to analyse a large set of echocardiographic echo and cardiac magnetic resonance imaging cmri parameters using machine learning ml techniques to find imaging predictors of clinical outcomes in a long-term follow-up of lvnc patients</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163963</th>\n",
       "      <td>differential diagnosis of pleural effusions by fuzzy-logic-based analysis of cytokines pleural effusions can be caused by highly different underlying diseases and are characterized by complex interactions of various local and circulating cells as well as numerous soluble parameters like interleukins il knowledge about this complex network could help to indicate underlying disease therefore, we have investigated immunoreactive concentrations of il-4, il-6, il-11, il-15, il-17, il-18, and tumor necrosis factor-alpha tnf-alpha in pleural effusions and peripheral blood from patients with tuberculosis, bronchial carcinoma and other carcinomas as well as congestive heart failure chf and pneumonias to determine the value of cytokine measurement for differential diagnosis, statistical and fuzzy-logic methods were applied quantitative analysis showed high concentrations of il-6 and il-11 only in pleural effusions il-15, il-17, il-18 and tnf-alpha could be detected also in blood plasma lowest amounts were detected in chf indicating the non-inflammatory origin of effusions statistical analysis did not provide evidence for diagnostic relevance of singular cytokines fuzzy-logic analysis was able to assign patients to the correct diseases with 80% accuracy using il-6 and il-15 measurement our results confirm the pathogenetic role of these cytokines in pleural effusions fuzzy-logic-based procedures may help to characterize and distinguish effusions of unknown origin even in small patient groups</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84041</th>\n",
       "      <td>a machine learning model to predict the risk of 30-day readmissions in patients with heart failure: a retrospective analysis of electronic medical records data heart failure is one of the leading causes of hospitalization in the united states advances in big data solutions allow for storage, management, and mining of large volumes of structured and semi-structured data, such as complex healthcare data applying these advances to complex healthcare data has led to the development of risk prediction models to help identify patients who would benefit most from disease management programs in an effort to reduce readmissions and healthcare cost, but the results of these efforts have been varied the primary aim of this study was to develop a 30-day readmission risk prediction model for heart failure patients discharged from a hospital admission</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91984</th>\n",
       "      <td>machine learning improves risk stratification after acute coronary syndrome the accurate assessment of a patients risk of adverse events remains a mainstay of clinical care commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history, presenting signs and symptoms, and lab values more sophisticated methods, such as artificial neural networks ann, form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data, yielding classifiers with improved performance using two cohorts consisting of patients admitted with a non-st-segment elevation acute coronary syndrome, we constructed an ann that identifies patients at high risk of cardiovascular death cvd the ann was trained and tested using patient subsets derived from a cohort containing 4395 patients area under the curve auc 0743 and validated on an independent holdout set containing 861 patients auc 0767 the ann 1-year hazard ratio for cvd was 372 95% confidence interval 104-143 after adjusting for the timi risk score, left ventricular ejection fraction, and b-type natriuretic peptide a unique feature of our approach is that it captures small changes in the st segment over time that cannot be detected by visual inspection these findings highlight the important role that anns can play in risk stratification</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60525</th>\n",
       "      <td>multi-view ensemble learning with empirical kernel for heart failure mortality prediction heart failure hf refers to the hearts inability to pump sufficient blood to maintain the bodys needs, which has a very serious impact on human health in recent years, the prevalence of hf has remained high this paper proposes a multi-view ensemble learning algorithm based on empirical kernel mapping called mve-ek, which predicts the mortality of patient through hospital records multi-view ensemble learning can take advantage of the consistency and complementarity of different views the mve-ek first divides the patients features into multiple views and then divides the samples of each view to multiple subsets through under sampling, which can reduce the imbalance rate of the original dataset and obtain some relatively balanced subsets each subset is mapped into kernel space by empirical kernel mapping, which can map samples from linearly inseparable spaces to linearly separable spaces finally, the multi-view ensemble learning is performed by the designed loss of acquaintance between views the effectiveness of the algorithm is verified on the three datasets of hf patient in the real world the performance of the algorithm is better than other comparison algorithms the datasets are collected from shanghai shuguang hospital and involve 10 203 hospitalization records for 4682 hf patients between march 2009 and april 2016 the prediction information provided by the algorithm can assist the clinician in providing a more personalized treatment plan for patients with hf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22193</th>\n",
       "      <td>sex-specific patterns of mortality predictors among patients undergoing cardiac resynchronization therapy: a machine learning approach &lt;b&gt;background:&lt;/b&gt; the relative importance of variables explaining sex-related differences in outcomes is scarcely explored in patients undergoing cardiac resynchronization therapy crt we sought to implement and evaluate machine learning ml algorithms for the prediction of 1- and 3-year all-cause mortality in crt patients we also aimed to assess the sex-specific differences in predictors of mortality utilizing ml &lt;b&gt;methods:&lt;/b&gt; using a retrospective registry of 2,191 crt patients, ml models were implemented in 6 partially overlapping patient subsets all patients, females, or males with 1- or 3-year follow-up each cohort was randomly split into training 80% and test sets 20% after hyperparameter tuning in the training sets, the best performing algorithm was evaluated in the test sets model discrimination was quantified using the area under the receiver-operating characteristic curves auc the most important predictors were identified using the permutation feature importances method &lt;b&gt;results:&lt;/b&gt; conditional inference random forest exhibited the best performance with aucs of 0728 0645-0802 and 0732 0681-0784 for the prediction of 1- and 3-year mortality, respectively etiology of heart failure, nyha class, left ventricular ejection fraction, and qrs morphology had higher predictive power, whereas hemoglobin was less important in females compared to males the importance of atrial fibrillation and age increased, while the importance of serum creatinine decreased from 1- to 3-year follow-up in both sexes &lt;b&gt;conclusions:&lt;/b&gt; using ml techniques in combination with easily obtainable clinical features, our models effectively predicted 1- and 3-year all-cause mortality in crt patients sex-specific patterns of predictors were identified, showing a dynamic variation over time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31480</th>\n",
       "      <td>an enhanced random forests approach to predict heart failure from small imbalanced gene expression data myocardial infarctions and heart failure are the cause of more than 17 million deaths annually worldwide st-segment elevation myocardial infarctions stemi require timely treatment, because delays of minutes have serious clinical impacts machine learning can provide alternative ways to predict heart failure and identify genes invovled in heart failure for these scopes, we applied a random forests classifier enhanced with feature elimination to microarray gene expression of 111 patients diagnosed with stemi, and measured the classification performance through standard metrics such as the matthews correlation coefficient mcc and area under the receiver operating characteristic curve roc auc afterwards, we used the same approach to rank all genes by importance, and to detect the genes more strongly associated with heart failure we validated this ranking by literature review and gene set enrichment analysis our classifier achieved mcc = +087 and roc auc = 0918, and our analysis identified klhl22, wdr11, or4q3, gpatch3, and fah as top five protein-coding genes related to heart failure our results confirm the effectiveness of machine learning feature elimination in predicting heart failure from gene expression, and the top genes found by our approach will be able to help biologists and cardiologists further our understanding of heart failure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94489</th>\n",
       "      <td>differences in repolarization heterogeneity among heart failure with preserved ejection fraction phenotypic subgroups heart failure with preserved ejection fraction hfpef is a highly heterogeneous syndrome associated with multiple medical comorbidities and pathophysiologic pathways or phenotypes we recently developed a phenomapping method combining deep phenotyping with machine learning analysis to classify hfpef patients into 3 clinically distinct phenotypic subgroups phenogroups with different clinical outcomes phenogroup #1 was younger with lower b-type natriuretic peptide levels, phenogroup #2 had the highest prevalence of obesity and diabetes mellitus, and phenogroup #3 was the oldest with the most factors for chronic kidney disease, the most dysfunctional myocardial mechanics, and the highest adverse outcomes the pathophysiological differences between these phenogroups, however, remain incompletely described we sought to evaluate whether these 3 groups differ on the basis of repolarization heterogeneity, which has previously been linked to adverse outcomes in hfpef the t-peak to t-end tpte interval, a well-validated index of repolarization heterogeneity, was measured by 2 readers blinded to each other and all other clinical data on the electrocardiograms of 201 hfpef patients enrolled in a systematic observational study tpte duration was associated with higher b-type natriuretic peptide level p = 0006, increased qrs-t angle p = 0008, and lower septal evelocity p = 0007 tpte duration was greatest in phenogroup #3 1004 ± 245 ms compared with phenogroups #1 912 ± 173 ms and #2 902 ± 170 ms p = 00098 on multivariable analyses, increased tpte was independently associated with the high-risk phenogroup #3 classification in conclusion, repolarization heterogeneity is a marker of a specific subset of hfpef patients identified using unsupervised machine learning analysis and therefore may be a key pathophysiologic marker in this subset of hfpef patients</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44365</th>\n",
       "      <td>dynamically constructed network with error correction for accurate ventricle volume estimation automated ventricle volume estimation avve on cardiac magnetic resonance cmr images is very important for clinical cardiac disease diagnosis however, current avve methods ignore the error correction for the estimated volume this results in clinically intolerable ventricle volume estimation error and further leads to wrong ejection fraction ef assessment, which significantly limits the application potential of avve methods the objective of this paper is to address this problem with avve and further make it more clinically applicable we proposed a dynamically constructed network to achieve accurate avve first, we introduced a novel dynamically constructed deep learning framework, that evolves a single model into a bi-model volume estimation network in this way, the ef correlation can be built directly based on the bi-model network second, we proposed an error correction strategy using dynamically created residual nodes, which is based on stochastic configurations with an ef correlation constraint finally, we formulated the proposed method into an end-to-end joint optimization framework for accurate ventricle volume estimation with effective error correction experiments and comparisons on large-scale cardiac magnetic resonance datasets were carried out results show that the proposed method outperforms state-of-the-art methods, and has good potential for clinical application besides, the proposed method is the first work to achieve error correction for avve and also has the potential to be extended to other medical index estimation tasks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>using deep learning to identify high-risk patients with heart failure with reduced ejection fraction &lt;b&gt;background:&lt;/b&gt; deep learning dl has not been well-established as a method to identify high-risk patients among patients with heart failure hf &lt;b&gt;objectives:&lt;/b&gt; this study aimed to use dl models to predict hospitalizations, worsening hf events, and 30-day and 90-day readmissions in patients with heart failure with reduced ejection fraction hfref &lt;b&gt;methods:&lt;/b&gt; we analyzed the data of adult hfref patients from the ibm® marketscan® commercial and medicare supplement databases between january 1, 2015 and december 31, 2017 a sequential model architecture based on bi-directional long short-term memory bi-lstm layers was utilized for dl models to predict hf hospitalizations and worsening hf events, we utilized two study designs: with and without a buffer window for comparison, we also tested multiple traditional machine learning models including logistic regression, random forest, and extreme gradient boosting xgboost model performance was assessed by area under the curve auc values, precision, and recall on an independent testing dataset &lt;b&gt;results:&lt;/b&gt; a total of 47 498 hfref patients were included; 9427 with at least one hf hospitalization the best aucs of dl models without a buffer window in predicting hf hospitalizations and worsening hf events in the total patient cohort were 0977 and 0972; with a 7-day buffer window the best aucs were 0573 and 0608, respectively the best aucs in predicting 30- and 90-day readmissions in all adult patients were 0597 and 0614, respectively an auc of 0861 was attained for prediction of 90-day readmission in patients aged 18-64 for all outcomes assessed, the dl approach outperformed traditional machine learning models &lt;b&gt;discussion:&lt;/b&gt; the dl approach can automate feature engineering during the model learning, which can increase the clinical applicability and lead to comparable or better model performance however, the lack of granular clinical data, and sample size and imbalance issues may have limited the models performance &lt;b&gt;conclusions:&lt;/b&gt; a dl approach using bi-lstm was shown to be a feasible and useful tool to predict hf-related outcomes this study can help inform the future development and deployment of predictive tools to identify high-risk hfref patients and ultimately facilitate targeted interventions in clinical practice</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104818</th>\n",
       "      <td>data mining framework for identification of myocardial infarction stages in ultrasound: a hybrid feature extraction paradigm part 2 early expansion of infarcted zone after acute myocardial infarction ami has serious short and long-term consequences and contributes to increased mortality thus, identification of moderate and severe phases of ami before leading to other catastrophic post-mi medical condition is most important for aggressive treatment and management advanced image processing techniques together with robust classifier using two-dimensional 2d echocardiograms may aid for automated classification of the extent of infarcted myocardium therefore, this paper proposes novel algorithms namely curvelet transform ct and local configuration pattern lcp for an automated detection of normal, moderately infarcted and severely infarcted myocardium using 2d echocardiograms the methodology extracts the lcp features from ct coefficients of echocardiograms the obtained features are subjected to marginal fisher analysis mfa dimensionality reduction technique followed by fuzzy entropy based ranking method different classifiers are used to differentiate ranked features into three classes normal, moderate and severely infarcted based on the extent of damage to myocardium the developed algorithm has achieved an accuracy of 9899%, sensitivity of 9848% and specificity of 100% for support vector machine svm classifier using only six features furthermore, we have developed an integrated index called myocardial infarction risk index miri to detect the normal, moderately and severely infarcted myocardium using a single number the proposed system may aid the clinicians in faster identification and quantification of the extent of infarcted myocardium using 2d echocardiogram this system may also aid in identifying the person at risk of developing heart failure based on the extent of infarcted myocardium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "138429                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a neuro-fuzzy decision support system for the diagnosis of heart failure a neuro-fuzzy decision support system is proposed for the diagnosis of heart failure the system comprises; knowledge base database, neural networks and fuzzy logic of both the quantitative and qualitative knowledge of the diagnosis of heart failure, neuro-fuzzy inference engine and decision support engine the neural networks employ a multi-layers perception back propagation learning process while the fuzzy logic uses the root sum square inference procedure the neuro-fuzzy inference engine uses a weighted average of the premise and consequent parameters with the fuzzy rules serving as the nodes and the fuzzy sets representing the weights of the nodes the decision support engine carries out the cognitive and emotional filtering of the objective and subjective feelings of the medical practitioner an experimental study of the decision support system was carried out using cases of some patients from three hospitals in nigeria with the assistance of their medical personnel who collected patientsdata over a period of six months the results of the study show that the neuro-fuzzy system provides a highly reliable diagnosis, while the emotional and cognitive filters further refine the diagnosis results by taking care of the contextual elements of medical diagnosis   \n",
       "36341                                                                                                                                                                                      baseline and dynamic risk predictors of appropriate implantable cardioverter defibrillator therapy background current approaches fail to separate patients at high versus low risk for ventricular arrhythmias owing to overreliance on a snapshot left ventricular ejection fraction measure we used statistical machine learning to identify important cardiac imaging and time-varying risk predictors methods and results three hundred eighty-two cardiomyopathy patients left ventricular ejection fraction ≤35% underwent cardiac magnetic resonance before primary prevention implantable cardioverter defibrillator insertion the primary end point was appropriate implantable cardioverter defibrillator discharge or sudden death patient characteristics; serum biomarkers of inflammation, neurohormonal status, and injury; and cardiac magnetic resonance-measured left ventricle and left atrial indices and myocardial scar burden were assessed at baseline time-varying covariates comprised interval heart failure hospitalizations and left ventricular ejection fractions a random forest statistical method for survival, longitudinal, and multivariable outcomes incorporating baseline and time-varying variables was compared with 1 seattle heart failure model scores and 2 random forest survival and cox regression models incorporating baseline characteristics with and without imaging variables age averaged 57±13 years with 28% women, 66% white, 51% ischemic, and follow-up time of 59±23 years the primary end point n=75 occurred at 33±24 years random forest statistical method for survival, longitudinal, and multivariable outcomes with baseline and time-varying predictors had the highest area under the receiver operating curve, median 088 95% ci, 075-096 top predictors comprised heart failure hospitalization, left ventricle scar, left ventricle and left atrial volumes, left atrial function, and interleukin-6 level; heart failure accounted for 67% of the variation explained by the prediction, imaging 27%, and interleukin-6 2% serial left ventricular ejection fraction was not a significant predictor conclusions hospitalization for heart failure and baseline cardiac metrics substantially improve ventricular arrhythmic risk prediction   \n",
       "1670                                                                                               coronary ct fractional flow reserve before transcatheter aortic valve replacement: clinical outcomes background the role of ct angiography-derived fractional flow reserve ct-ffr in pre-transcatheter aortic valve replacement tavr assessment is uncertain purpose to evaluate the predictive value of on-site machine learning-based ct-ffr for adverse clinical outcomes in candidates for tavr materials and methods this observational retrospective study included patients with severe aortic stenosis referred to tavr after coronary ct angiography ccta between september 2014 and december 2019 clinical end points comprised major adverse cardiac events mace nonfatal myocardial infarction, unstable angina, cardiac death, or heart failure admission and all-cause mortality ct-ffr was obtained semiautomatically using an on-site machine learning algorithm the ability of ct-ffr abnormal if ≤075 to predict outcomes and improve the predictive value of the current noninvasive work-up was assessed survival analysis was performed, and the c-index was used to assess the performance of each predictive model to compare nested models, the likelihood ratio χ<sup>2</sup> test was performed results a total of 196 patients mean age ± standard deviation, 75 years ± 11; 110 women 56% were included; the median time of follow-up was 18 months mace occurred in 16% 31 of 196 patients and all-cause mortality in 19% 38 of 196 patients univariable analysis revealed ct-ffr was predictive of mace hazard ratio hr, 41; 95% ci: 16, 108; <i>p</i> = 01 but not all-cause mortality hr, 12; 95% ci: 06, 22; <i>p</i> = 63 ct-ffr was independently associated with mace hr, 40; 95% ci: 15, 105; <i>p</i> = 01 when adjusting for potential confounders adding ct-ffr as a predictor to models that include ccta and clinical data improved their predictive value for mace <i>p</i> = 002 but not all-cause mortality <i>p</i> = 67, and it showed good discriminative ability for mace c-index, 071 conclusion ct angiography-derived fractional flow reserve was associated with major adverse cardiac events in candidates for transcatheter aortic valve replacement and improved the predictive value of coronary ct angiography assessment © rsna, 2021 <i>online supplemental material is available for this article</i> see also the editorial by choe in this issue   \n",
       "65797                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       readmission prediction using deep learning on electronic health records unscheduled 30-day readmissions are a hallmark of congestive heart failure chf patients that pose significant health risks and escalate care cost in order to reduce readmissions and curb the cost of care, it is important to initiate targeted intervention programs for patients at risk of readmission this requires identifying high-risk patients at the time of discharge from hospital here, using real data from over 7500 chf patients hospitalized between 2012 and 2016 in sweden, we built and tested a deep learning framework to predict 30-day unscheduled readmission we present a cost-sensitive formulation of long short-term memory lstm neural network using expert features and contextual embedding of clinical concepts this study targets key elements of an electronic health record ehr driven prediction model in a single framework: using both expert and machine derived features, incorporating sequential patterns and addressing the class imbalance problem we evaluate the contribution of each element towards prediction performance roc-auc, f1-measure and cost-savings we show that the model with all key elements achieves higher discrimination ability auc: 077; f1: 051; cost: 22% of maximum possible savings outperforming the reduced models in at least two evaluation metrics additionally, we present a simple financial analysis to estimate annual savings if targeted interventions are offered to high risk patients   \n",
       "15921                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          a model-agnostic approach for understanding heart failure risk factors understanding the risk factors for developing heart failure among patients with type 2 diabetes can contribute to preventing deterioration of quality of life for those persons electronic health records ehr provide an opportunity to use sophisticated machine learning models to understand and compare the effect of different risk factors for developing hf as the complexity of the model increases, however, the transparency of the model often decreases to interpret the results, we aimed to develop a model-agnostic approach to shed light on complex models and interpret the effect of features on developing heart failure using the healthfacts ehr database of the cerner ehr, we extracted the records of 723 patients with at least 6 yeas of follow up of type 2 diabetes, of whom 134 developed heart failure using age and comorbidities as features and heart failure as the outcome, we trained logistic regression, random forest, xgboost, neural network, and then applied our proposed approach to rank the effect of each factor on developing heart failure   \n",
       "20254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              blood pressure monitoring system using a two-channel ballistocardiogram and convolutional neural networks hypertension is a chronic disease that kills 76 million people worldwide annually a continuous blood pressure monitoring system is required to accurately diagnose hypertension here, a chair-shaped ballistocardiogram bcg-based blood pressure estimation system was developed with no sensors attached to users two experimental sessions were conducted with 30 subjects in the first session, two-channel bcg and blood pressure data were recorded for each subject in the second session, the two-channel bcg and blood pressure data were recorded after running on a treadmill and then resting on the newly developed system the empirical mode decomposition algorithm was used to remove noise in the two-channel bcg, and the instantaneous phase was calculated by applying a hilbert transform to the first intrinsic mode functions after training a convolutional neural network regression model that predicts the systolic and diastolic blood pressures sbp and dbp from the two-channel bcg phase, the results of the first session rest and second session recovery were compared the results confirmed that the proposed model accurately estimates the rapidly rising blood pressure in the recovery state results from the rest sessions satisfied the association for the advancement of medical instrumentation aami international standards the standard deviation of the sbp results in the recovery session exceeded 07   \n",
       "25997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             machine learning based congestive heart failure detection using feature importance ranking of multimodal features in this study, we ranked the multimodal features extracted from congestive heart failure chf and normal sinus rhythm nsr subjects we categorized the ranked features into 1 to 5 categories based on empirical receiver operating characteristics eroc values instead of using all multimodal features, we use high ranking features for detection of chf and normal subjects we employed powerful machine learning techniques such as decision tree dt, naïve bayes nb, svm gaussian, svm rbf and svm polynomial the performance was measured in terms of sensitivity, specificity, positive predictive value ppv, negative predictive value npv, accuracy, false positive rate fpr, and area under the receiver operating characteristic curve auc the highest detection performance in terms of accuracy and auc was obtained with all multimodal features using svm gaussian with sensitivity 9306%, specificity 8182%, accuracy 8879% and auc 095 using the top five ranked features, the highest performance was obtained with svm gaussian yields accuracy 8448%, auc 086; top nine ranked features using decision tree and naïve bayes got accuracy 8448%, auc 088; last thirteen ranked features using svm polynomial obtained accuracy 8017%, auc 084 the findings indicate that proposed approach with feature ranking can be very useful for automatic detection of congestive heart failure patients and can be very helpful for further decision making by the clinicians and physicians in order to decrease the mortality rate   \n",
       "80151                                                                                                         an interactive assistant for patients with cardiac implantable electronic devices: a study protocol of the lucy trial patients with chronic heart failure chf and reduced left ventricle ejection fraction benefit from cardiac resynchronization therapy crt and implantable cardioverter defibrillator icd however, increasing numbers of patient with crt and icd devices produce overload of cardiology centers where patients are admitted to ambulatory visits this study aims to find multivariate model predicting the requirement for ambulatory follow-up of cardiac implantable electronic devices ciedsthe lucy study is an observational, cohort, prospective, 2-stage trial as equal number of patients 300 will be included in the first and the second part of the study, finally, 600 patients will be included in the study the inclusion criteria will be: age between 18 and 90 years, chf new york heart association classes i-iii and implanted icd or crt at least 30 days before study inclusion the exclusion criteria will be dementia and other conditions impeding cooperation during the study all patients included in the study will undergo standard ambulatory visit primary endpoint will be defined as any ambulatory visit qualified as necessary due to patients condition or device malfunction diagnose by the cardiologist: any change in pharmacotherapy related to patients clinical status assessed during the visit, any change in tachyarrythmia counter or discriminator status, any change in tachyarrythmia threshold, presence of ventricular undersensing or oversensing, presence of atrial or ventricular ineffective pacing, or devices pocket infection secondary endpoint will be defined as any ambulatory visit qualified as necessary due to the alarm identified via medtronic carelink express mcle: sustained or treated ventricular tachyarrythmia, any not previously diagnosed supraventricular tachyarrythmia, or elective replacement indicatorour study is the first attempt of implementation of the machine learning and elements artificial intelligence in health care optimization of patients with cied the lucy will be an open product, available for additional testing and improvement with supplementary functionalities: quality of life assessment, teleconsultation, video-streaming, automated imagine recognizing   \n",
       "64369                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 combining structured and unstructured data for predicting risk of readmission for heart failure patients researchers have studied many models for predicting the risk of readmission for heart failure over the last decade most models have used a parametric statistical approach while a few have ventured into using machine learning methods such as statistical natural language processing we created three predictive models by combining these two techniques for the cohort of 1,629 patients from six hosptials using structured data along with their 136,963 clinical notes till their index admission, stored in the emr system over five years the aucs for structured and combined models were very close 06494 and 06447 and that for the unstructured model was 05219 the clinical impact of the models using decision curve analysis showed that, at a threshold predicted probability of 020, the combined model offered 15%, 30%, and 70% net benefit over its individual counterparts, treat-all, and treat-none strategy respectively   \n",
       "44614                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           biventricular imaging markers to predict outcomes in non-compaction cardiomyopathy: a machine learning study left ventricular non-compaction cardiomyopathy lvnc is a genetic heart disease, with heart failure, arrhythmias, and embolic events as main clinical manifestations the goal of this study was to analyse a large set of echocardiographic echo and cardiac magnetic resonance imaging cmri parameters using machine learning ml techniques to find imaging predictors of clinical outcomes in a long-term follow-up of lvnc patients   \n",
       "163963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            differential diagnosis of pleural effusions by fuzzy-logic-based analysis of cytokines pleural effusions can be caused by highly different underlying diseases and are characterized by complex interactions of various local and circulating cells as well as numerous soluble parameters like interleukins il knowledge about this complex network could help to indicate underlying disease therefore, we have investigated immunoreactive concentrations of il-4, il-6, il-11, il-15, il-17, il-18, and tumor necrosis factor-alpha tnf-alpha in pleural effusions and peripheral blood from patients with tuberculosis, bronchial carcinoma and other carcinomas as well as congestive heart failure chf and pneumonias to determine the value of cytokine measurement for differential diagnosis, statistical and fuzzy-logic methods were applied quantitative analysis showed high concentrations of il-6 and il-11 only in pleural effusions il-15, il-17, il-18 and tnf-alpha could be detected also in blood plasma lowest amounts were detected in chf indicating the non-inflammatory origin of effusions statistical analysis did not provide evidence for diagnostic relevance of singular cytokines fuzzy-logic analysis was able to assign patients to the correct diseases with 80% accuracy using il-6 and il-15 measurement our results confirm the pathogenetic role of these cytokines in pleural effusions fuzzy-logic-based procedures may help to characterize and distinguish effusions of unknown origin even in small patient groups   \n",
       "84041                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            a machine learning model to predict the risk of 30-day readmissions in patients with heart failure: a retrospective analysis of electronic medical records data heart failure is one of the leading causes of hospitalization in the united states advances in big data solutions allow for storage, management, and mining of large volumes of structured and semi-structured data, such as complex healthcare data applying these advances to complex healthcare data has led to the development of risk prediction models to help identify patients who would benefit most from disease management programs in an effort to reduce readmissions and healthcare cost, but the results of these efforts have been varied the primary aim of this study was to develop a 30-day readmission risk prediction model for heart failure patients discharged from a hospital admission   \n",
       "91984                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          machine learning improves risk stratification after acute coronary syndrome the accurate assessment of a patients risk of adverse events remains a mainstay of clinical care commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history, presenting signs and symptoms, and lab values more sophisticated methods, such as artificial neural networks ann, form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data, yielding classifiers with improved performance using two cohorts consisting of patients admitted with a non-st-segment elevation acute coronary syndrome, we constructed an ann that identifies patients at high risk of cardiovascular death cvd the ann was trained and tested using patient subsets derived from a cohort containing 4395 patients area under the curve auc 0743 and validated on an independent holdout set containing 861 patients auc 0767 the ann 1-year hazard ratio for cvd was 372 95% confidence interval 104-143 after adjusting for the timi risk score, left ventricular ejection fraction, and b-type natriuretic peptide a unique feature of our approach is that it captures small changes in the st segment over time that cannot be detected by visual inspection these findings highlight the important role that anns can play in risk stratification   \n",
       "60525                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        multi-view ensemble learning with empirical kernel for heart failure mortality prediction heart failure hf refers to the hearts inability to pump sufficient blood to maintain the bodys needs, which has a very serious impact on human health in recent years, the prevalence of hf has remained high this paper proposes a multi-view ensemble learning algorithm based on empirical kernel mapping called mve-ek, which predicts the mortality of patient through hospital records multi-view ensemble learning can take advantage of the consistency and complementarity of different views the mve-ek first divides the patients features into multiple views and then divides the samples of each view to multiple subsets through under sampling, which can reduce the imbalance rate of the original dataset and obtain some relatively balanced subsets each subset is mapped into kernel space by empirical kernel mapping, which can map samples from linearly inseparable spaces to linearly separable spaces finally, the multi-view ensemble learning is performed by the designed loss of acquaintance between views the effectiveness of the algorithm is verified on the three datasets of hf patient in the real world the performance of the algorithm is better than other comparison algorithms the datasets are collected from shanghai shuguang hospital and involve 10 203 hospitalization records for 4682 hf patients between march 2009 and april 2016 the prediction information provided by the algorithm can assist the clinician in providing a more personalized treatment plan for patients with hf   \n",
       "22193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  sex-specific patterns of mortality predictors among patients undergoing cardiac resynchronization therapy: a machine learning approach <b>background:</b> the relative importance of variables explaining sex-related differences in outcomes is scarcely explored in patients undergoing cardiac resynchronization therapy crt we sought to implement and evaluate machine learning ml algorithms for the prediction of 1- and 3-year all-cause mortality in crt patients we also aimed to assess the sex-specific differences in predictors of mortality utilizing ml <b>methods:</b> using a retrospective registry of 2,191 crt patients, ml models were implemented in 6 partially overlapping patient subsets all patients, females, or males with 1- or 3-year follow-up each cohort was randomly split into training 80% and test sets 20% after hyperparameter tuning in the training sets, the best performing algorithm was evaluated in the test sets model discrimination was quantified using the area under the receiver-operating characteristic curves auc the most important predictors were identified using the permutation feature importances method <b>results:</b> conditional inference random forest exhibited the best performance with aucs of 0728 0645-0802 and 0732 0681-0784 for the prediction of 1- and 3-year mortality, respectively etiology of heart failure, nyha class, left ventricular ejection fraction, and qrs morphology had higher predictive power, whereas hemoglobin was less important in females compared to males the importance of atrial fibrillation and age increased, while the importance of serum creatinine decreased from 1- to 3-year follow-up in both sexes <b>conclusions:</b> using ml techniques in combination with easily obtainable clinical features, our models effectively predicted 1- and 3-year all-cause mortality in crt patients sex-specific patterns of predictors were identified, showing a dynamic variation over time   \n",
       "31480                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         an enhanced random forests approach to predict heart failure from small imbalanced gene expression data myocardial infarctions and heart failure are the cause of more than 17 million deaths annually worldwide st-segment elevation myocardial infarctions stemi require timely treatment, because delays of minutes have serious clinical impacts machine learning can provide alternative ways to predict heart failure and identify genes invovled in heart failure for these scopes, we applied a random forests classifier enhanced with feature elimination to microarray gene expression of 111 patients diagnosed with stemi, and measured the classification performance through standard metrics such as the matthews correlation coefficient mcc and area under the receiver operating characteristic curve roc auc afterwards, we used the same approach to rank all genes by importance, and to detect the genes more strongly associated with heart failure we validated this ranking by literature review and gene set enrichment analysis our classifier achieved mcc = +087 and roc auc = 0918, and our analysis identified klhl22, wdr11, or4q3, gpatch3, and fah as top five protein-coding genes related to heart failure our results confirm the effectiveness of machine learning feature elimination in predicting heart failure from gene expression, and the top genes found by our approach will be able to help biologists and cardiologists further our understanding of heart failure   \n",
       "94489                                                                                                                                                                                                                                                                                                                                                                                                                                              differences in repolarization heterogeneity among heart failure with preserved ejection fraction phenotypic subgroups heart failure with preserved ejection fraction hfpef is a highly heterogeneous syndrome associated with multiple medical comorbidities and pathophysiologic pathways or phenotypes we recently developed a phenomapping method combining deep phenotyping with machine learning analysis to classify hfpef patients into 3 clinically distinct phenotypic subgroups phenogroups with different clinical outcomes phenogroup #1 was younger with lower b-type natriuretic peptide levels, phenogroup #2 had the highest prevalence of obesity and diabetes mellitus, and phenogroup #3 was the oldest with the most factors for chronic kidney disease, the most dysfunctional myocardial mechanics, and the highest adverse outcomes the pathophysiological differences between these phenogroups, however, remain incompletely described we sought to evaluate whether these 3 groups differ on the basis of repolarization heterogeneity, which has previously been linked to adverse outcomes in hfpef the t-peak to t-end tpte interval, a well-validated index of repolarization heterogeneity, was measured by 2 readers blinded to each other and all other clinical data on the electrocardiograms of 201 hfpef patients enrolled in a systematic observational study tpte duration was associated with higher b-type natriuretic peptide level p = 0006, increased qrs-t angle p = 0008, and lower septal evelocity p = 0007 tpte duration was greatest in phenogroup #3 1004 ± 245 ms compared with phenogroups #1 912 ± 173 ms and #2 902 ± 170 ms p = 00098 on multivariable analyses, increased tpte was independently associated with the high-risk phenogroup #3 classification in conclusion, repolarization heterogeneity is a marker of a specific subset of hfpef patients identified using unsupervised machine learning analysis and therefore may be a key pathophysiologic marker in this subset of hfpef patients   \n",
       "44365                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       dynamically constructed network with error correction for accurate ventricle volume estimation automated ventricle volume estimation avve on cardiac magnetic resonance cmr images is very important for clinical cardiac disease diagnosis however, current avve methods ignore the error correction for the estimated volume this results in clinically intolerable ventricle volume estimation error and further leads to wrong ejection fraction ef assessment, which significantly limits the application potential of avve methods the objective of this paper is to address this problem with avve and further make it more clinically applicable we proposed a dynamically constructed network to achieve accurate avve first, we introduced a novel dynamically constructed deep learning framework, that evolves a single model into a bi-model volume estimation network in this way, the ef correlation can be built directly based on the bi-model network second, we proposed an error correction strategy using dynamically created residual nodes, which is based on stochastic configurations with an ef correlation constraint finally, we formulated the proposed method into an end-to-end joint optimization framework for accurate ventricle volume estimation with effective error correction experiments and comparisons on large-scale cardiac magnetic resonance datasets were carried out results show that the proposed method outperforms state-of-the-art methods, and has good potential for clinical application besides, the proposed method is the first work to achieve error correction for avve and also has the potential to be extended to other medical index estimation tasks   \n",
       "6344    using deep learning to identify high-risk patients with heart failure with reduced ejection fraction <b>background:</b> deep learning dl has not been well-established as a method to identify high-risk patients among patients with heart failure hf <b>objectives:</b> this study aimed to use dl models to predict hospitalizations, worsening hf events, and 30-day and 90-day readmissions in patients with heart failure with reduced ejection fraction hfref <b>methods:</b> we analyzed the data of adult hfref patients from the ibm® marketscan® commercial and medicare supplement databases between january 1, 2015 and december 31, 2017 a sequential model architecture based on bi-directional long short-term memory bi-lstm layers was utilized for dl models to predict hf hospitalizations and worsening hf events, we utilized two study designs: with and without a buffer window for comparison, we also tested multiple traditional machine learning models including logistic regression, random forest, and extreme gradient boosting xgboost model performance was assessed by area under the curve auc values, precision, and recall on an independent testing dataset <b>results:</b> a total of 47 498 hfref patients were included; 9427 with at least one hf hospitalization the best aucs of dl models without a buffer window in predicting hf hospitalizations and worsening hf events in the total patient cohort were 0977 and 0972; with a 7-day buffer window the best aucs were 0573 and 0608, respectively the best aucs in predicting 30- and 90-day readmissions in all adult patients were 0597 and 0614, respectively an auc of 0861 was attained for prediction of 90-day readmission in patients aged 18-64 for all outcomes assessed, the dl approach outperformed traditional machine learning models <b>discussion:</b> the dl approach can automate feature engineering during the model learning, which can increase the clinical applicability and lead to comparable or better model performance however, the lack of granular clinical data, and sample size and imbalance issues may have limited the models performance <b>conclusions:</b> a dl approach using bi-lstm was shown to be a feasible and useful tool to predict hf-related outcomes this study can help inform the future development and deployment of predictive tools to identify high-risk hfref patients and ultimately facilitate targeted interventions in clinical practice   \n",
       "104818                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               data mining framework for identification of myocardial infarction stages in ultrasound: a hybrid feature extraction paradigm part 2 early expansion of infarcted zone after acute myocardial infarction ami has serious short and long-term consequences and contributes to increased mortality thus, identification of moderate and severe phases of ami before leading to other catastrophic post-mi medical condition is most important for aggressive treatment and management advanced image processing techniques together with robust classifier using two-dimensional 2d echocardiograms may aid for automated classification of the extent of infarcted myocardium therefore, this paper proposes novel algorithms namely curvelet transform ct and local configuration pattern lcp for an automated detection of normal, moderately infarcted and severely infarcted myocardium using 2d echocardiograms the methodology extracts the lcp features from ct coefficients of echocardiograms the obtained features are subjected to marginal fisher analysis mfa dimensionality reduction technique followed by fuzzy entropy based ranking method different classifiers are used to differentiate ranked features into three classes normal, moderate and severely infarcted based on the extent of damage to myocardium the developed algorithm has achieved an accuracy of 9899%, sensitivity of 9848% and specificity of 100% for support vector machine svm classifier using only six features furthermore, we have developed an integrated index called myocardial infarction risk index miri to detect the normal, moderately and severely infarcted myocardium using a single number the proposed system may aid the clinicians in faster identification and quantification of the extent of infarcted myocardium using 2d echocardiogram this system may also aid in identifying the person at risk of developing heart failure based on the extent of infarcted myocardium    \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "138429        0       0       0           0          0        0       0   \n",
       "36341         0       0       0           0          0        0       0   \n",
       "1670          0       0       0           0          0        0       0   \n",
       "65797         0       0       0           0          0        0       0   \n",
       "15921         0       0       0           0          0        0       0   \n",
       "20254         0       0       0           0          0        0       0   \n",
       "25997         0       0       0           0          0        0       0   \n",
       "80151         0       0       1           0          0        0       0   \n",
       "64369         0       0       0           0          0        0       0   \n",
       "44614         0       0       0           0          0        0       0   \n",
       "163963        0       0       1           0          0        0       1   \n",
       "84041         0       0       0           0          0        0       0   \n",
       "91984         0       0       0           0          0        0       0   \n",
       "60525         0       0       0           0          0        0       0   \n",
       "22193         0       0       0           0          0        0       0   \n",
       "31480         0       0       0           0          0        0       0   \n",
       "94489         0       0       0           0          0        0       0   \n",
       "44365         0       0       0           0          0        0       0   \n",
       "6344          0       0       0           0          0        0       0   \n",
       "104818        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "138429           0            0         0           0        0       0   \n",
       "36341            0            0         0           0        0       0   \n",
       "1670             0            0         0           0        0       0   \n",
       "65797            0            0         0           0        0       0   \n",
       "15921            0            0         0           0        0       0   \n",
       "20254            0            0         0           0        0       0   \n",
       "25997            0            0         0           0        0       0   \n",
       "80151            0            0         0           0        0       0   \n",
       "64369            0            0         0           0        0       0   \n",
       "44614            0            0         0           0        0       0   \n",
       "163963           0            0         0           0        1       0   \n",
       "84041            0            0         0           0        0       0   \n",
       "91984            0            0         0           0        0       0   \n",
       "60525            0            0         0           0        0       0   \n",
       "22193            0            0         0           0        0       0   \n",
       "31480            0            0         0           0        0       0   \n",
       "94489            0            0         0           0        0       0   \n",
       "44365            0            0         0           0        0       0   \n",
       "6344             0            0         0           0        0       0   \n",
       "104818           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "138429           0             0           0            0         0   \n",
       "36341            0             0           0            0         0   \n",
       "1670             0             0           0            0         0   \n",
       "65797            0             0           0            0         0   \n",
       "15921            0             0           0            0         0   \n",
       "20254            0             0           0            0         0   \n",
       "25997            0             0           0            0         0   \n",
       "80151            0             0           0            0         0   \n",
       "64369            0             0           0            0         0   \n",
       "44614            0             0           0            0         0   \n",
       "163963           0             0           0            0         0   \n",
       "84041            0             0           0            0         0   \n",
       "91984            0             0           0            0         0   \n",
       "60525            0             0           0            0         0   \n",
       "22193            0             0           0            0         0   \n",
       "31480            0             0           0            0         0   \n",
       "94489            0             0           0            0         0   \n",
       "44365            0             0           0            0         0   \n",
       "6344             0             0           0            0         0   \n",
       "104818           0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "138429          0            0           0            0           0   \n",
       "36341           0            0           0            0           0   \n",
       "1670            0            0           0            0           0   \n",
       "65797           0            0           0            0           0   \n",
       "15921           0            0           0            0           0   \n",
       "20254           0            0           0            0           0   \n",
       "25997           0            0           0            0           0   \n",
       "80151           0            0           0            0           0   \n",
       "64369           0            0           0            0           0   \n",
       "44614           0            0           0            0           0   \n",
       "163963          0            0           0            0           0   \n",
       "84041           0            0           0            0           0   \n",
       "91984           0            0           0            0           0   \n",
       "60525           0            0           0            0           0   \n",
       "22193           0            0           0            0           0   \n",
       "31480           0            0           0            0           0   \n",
       "94489           0            0           0            0           0   \n",
       "44365           0            0           0            0           0   \n",
       "6344            0            0           0            0           0   \n",
       "104818          0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "138429            0          0            0        0         0          0   \n",
       "36341             0          0            0        0         0          0   \n",
       "1670              0          0            0        0         0          0   \n",
       "65797             0          0            0        0         0          0   \n",
       "15921             0          0            0        0         0          0   \n",
       "20254             0          0            0        0         0          0   \n",
       "25997             0          0            0        0         0          0   \n",
       "80151             0          0            0        0         0          0   \n",
       "64369             0          0            0        0         0          0   \n",
       "44614             0          0            0        0         0          0   \n",
       "163963            0          0            0        0         0          0   \n",
       "84041             0          0            0        0         0          0   \n",
       "91984             0          0            0        0         0          0   \n",
       "60525             0          0            0        0         0          0   \n",
       "22193             0          0            0        0         0          0   \n",
       "31480             0          0            0        0         0          0   \n",
       "94489             0          0            0        0         0          0   \n",
       "44365             0          0            0        0         0          0   \n",
       "6344              0          0            0        0         0          0   \n",
       "104818            0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "138429       0        0         0          0        0       0         0   \n",
       "36341        0        0         0          0        0       0         0   \n",
       "1670         0        0         0          0        0       0         0   \n",
       "65797        0        0         0          0        0       0         0   \n",
       "15921        0        0         0          0        0       0         0   \n",
       "20254        0        0         0          0        0       0         0   \n",
       "25997        0        0         0          0        0       0         0   \n",
       "80151        0        0         0          0        0       0         0   \n",
       "64369        0        0         0          0        0       0         0   \n",
       "44614        0        0         0          0        0       0         0   \n",
       "163963       0        0         1          1        0       0         0   \n",
       "84041        0        0         0          0        0       0         0   \n",
       "91984        0        0         0          0        0       0         0   \n",
       "60525        0        0         0          0        0       0         0   \n",
       "22193        0        0         0          0        0       0         0   \n",
       "31480        0        0         0          0        0       0         0   \n",
       "94489        0        0         0          0        0       0         0   \n",
       "44365        0        0         0          0        0       0         0   \n",
       "6344         0        0         0          0        0       0         0   \n",
       "104818       0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \n",
       "138429          1        0           0         0        1        0       1  \n",
       "36341           1        0           0         0        1        1       1  \n",
       "1670            0        0           0         0        1        1       1  \n",
       "65797           0        0           0         0        1        0       1  \n",
       "15921           0        0           0         0        1        0       1  \n",
       "20254           0        0           0         0        1        0       1  \n",
       "25997           0        0           0         0        1        0       1  \n",
       "80151           1        0           0         1        1        0       1  \n",
       "64369           0        0           0         0        1        0       1  \n",
       "44614           0        0           0         0        1        0       1  \n",
       "163963          0        0           0         0        1        0       1  \n",
       "84041           0        0           0         0        1        0       1  \n",
       "91984           0        0           0         0        1        1       1  \n",
       "60525           0        0           0         0        1        0       1  \n",
       "22193           0        0           0         0        1        0       1  \n",
       "31480           0        0           0         0        1        1       1  \n",
       "94489           0        0           0         0        1        0       1  \n",
       "44365           0        0           0         0        1        0       1  \n",
       "6344            0        0           0         0        1        0       1  \n",
       "104818          0        0           0         0        1        1       1  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['hf_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e7466e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33397, '1': 782})\n"
     ]
    }
   ],
   "source": [
    "#### ARRHYTHMIA / arrhyt\n",
    "\n",
    "## text\n",
    "text = ['sinus node', 'sinoatrial', 'atrial tachy', 'atrial flutter', 'accessory pathway', 'long qt', 'holter',\n",
    "        'pacemaker', 'ventricular tachy', 'atrial fibrill', 'ventricular fibrill', 'supraventricular tachy',\n",
    "        'cardiover', 'defibrillat', 'heart block', 'degree block', 'av block', 'ventricular block', ' p-wave', ' p wave', 'pr interval',\n",
    "       'p-r interval', 'pr-interval', 'corrected qt', ' qtc ', ' qrs complex ', 'brugada', 'short qt', 'qt syndrome', 'long qt']\n",
    "\n",
    "spec['arrhyt_text'] = np.where(groups['text'].str.contains('arrhythmi'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['arrhyt_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['arrhyt_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"ablation\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"ablation\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"bradycardia\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"bradycardia\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"electrophys\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"electrophys\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"rhythm\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"rhythm\")) , \"1\", spec['arrhyt_text'])\n",
    "\n",
    "\n",
    "## outputs\n",
    "print('text counts:')\n",
    "print(Counter(spec['arrhyt_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "800af8f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>automated localization of focal ventricular tachycardia from simulated implanted device electrograms: a combined physics-ai approach &lt;b&gt;background:&lt;/b&gt; focal ventricular tachycardia vt is a life-threating arrhythmia, responsible for high morbidity rates and sudden cardiac death scd radiofrequency ablation is the only curative therapy against incessant vt; however, its success is dependent on accurate localization of its source, which is highly invasive and time-consuming &lt;b&gt;objective:&lt;/b&gt; the goal of our study is, as a proof of concept, to demonstrate the possibility of utilizing electrogram egm recordings from cardiac implantable electronic devices cieds to achieve this, we utilize fast and accurate whole torso electrophysiological ep simulations in conjunction with convolutional neural networks cnns to automate the localization of focal vts using simulated egms &lt;b&gt;materials and methods:&lt;/b&gt; a highly detailed 3d torso model was used to simulate ∼4000 focal vts, evenly distributed across the left ventricle lv, utilizing a rapid reaction-eikonal environment solutions were subsequently combined with lead field computations on the torso to derive accurate electrocardiograms ecgs and egm traces, which were used as inputs to cnns to localize focal sources we compared the localization performance of a previously developed cnn architecture cartesian probability-based with our novel cnn algorithm utilizing universal ventricular coordinates uvcs &lt;b&gt;results:&lt;/b&gt; implanted device egms successfully localized vt sources with localization error 874 mm comparable to ecg-based localization 669 mm our novel uvc cnn architecture outperformed the existing cartesian probability-based algorithm errors = 406 mm and 807 mm for ecgs and egms, respectively overall, localization was relatively insensitive to noise and changes in body compositions; however, displacements in ecg electrodes and cied leads caused performance to decrease errors 16-25 mm &lt;b&gt;conclusion:&lt;/b&gt; egm recordings from implanted devices may be used to successfully, and robustly, localize focal vt sources, and aid ablation planning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137756</th>\n",
       "      <td>field programmable gate array based fuzzy neural signal processing system for differential diagnosis of qrs complex tachycardia and tachyarrhythmia in noisy ecg signals the paper reports of a field programmable gate array fpga based embedded system for detection of qrs complex in a noisy electrocardiogram ecg signal and thereafter differential diagnosis of tachycardia and tachyarrhythmia the qrs complex has been detected after application of entropy measure of fuzziness to build a detection function of ecg signal, which has been previously filtered to remove power line interference and base line wander using the detected qrs complexes, differential diagnosis of tachycardia and tachyarrhythmia has been performed the entire algorithm has been realized in hardware on an fpga using the standard cse ecg database, the algorithm performed highly effectively the performance of the algorithm in respect of qrs detection with sensitivity se of 9974% and accuracy of 995% is achieved when tested using single channel ecg with entropy criteria the performance of the qrs detection system has been compared and found to be better than most of the qrs detection systems available in literature using the system, 200 patients have been diagnosed with an accuracy of 985%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111202</th>\n",
       "      <td>symmetrical compression distance for arrhythmia discrimination in cloud-based big-data services the current development of cloud computing is completely changing the paradigm of data knowledge extraction in huge databases an example of this technology in the cardiac arrhythmia field is the scoop platform, a national-level scientific cloud-based big data service for implantable cardioverter defibrillators in this scenario, we here propose a new methodology for automatic classification of intracardiac electrograms egms in a cloud computing system, designed for minimal signal preprocessing a new compression-based similarity measure csm is created for low computational burden, so-called weighted fast compression distance, which provides better performance when compared with other csms in the literature using simple machine learning techniques, a set of 6848 egms extracted from scoop platform were classified into seven cardiac arrhythmia classes and one noise class, reaching near to 90% accuracy when previous patient arrhythmia information was available and 63% otherwise, hence overcoming in all cases the classification provided by the majority class results show that this methodology can be used as a high-quality service of cloud computing, providing support to physicians for improving the knowledge on patient diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113277</th>\n",
       "      <td>variational bayesian electrophysiological imaging of myocardial infarction the presence, size, and distribution of ischemic tissue bear significant prognostic and therapeutic implication for ventricular arrhythmias while many approaches to 3d infarct detection have been developed via electrophysiological ep imaging from noninvasive electrocardiographic data, this ill-posed inverse problem remains challenging especially for septal infarcts that are hidden from body-surface data we propose a variational bayesian framework for ep imaging of 3d infarct using a total-variation prior the posterior distribution of intramural action potential and all regularization parameters are estimated from body-surface data by minimizing the kullback-leibler divergence because of the uncertainty introduced in prior models, we hypothesize that the solution uncertainty plays as important a role as the point estimate in interpreting the reconstruction this is verified in a set of phantom and real-data experiments, where regions of low confidence help to eliminate false-positives and to accurately identify infarcts of various locations including septum and distributions owing to the ability of total-variation prior in extracting the boundary between smooth regions, the presented method also has the potential to outline infarct border that is the most critical region responsible for ventricular arrhvthmias</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138791</th>\n",
       "      <td>determination of a new vlf band in hrv for ventricular tachyarrhythmia patients this study presents a new very low frequency vlf band range in ventricular tachyarrhythmia patients and involves an approach for estimation of effect of vlf band on ventricular tachyarrhythmia patients a model based on wavelet packets wp and multilayer perceptron neural network mlpnn is used for determination of effective vlf band in heart rate variability hrv signals hrv is decomposed into sub-bands including very low frequency parts and variations of energy are analyzed domination test is done using mlpnn and dominant band is determined as a result, a new vlf band was described in 00039063-003125 hz frequency range this method can be used for other bands or other arrhythmia patients especially, estimation of dominant band energy using this method can be helped to diagnose for applications where have important effect of characteristic band</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>multi-task deep learning for cardiac rhythm detection in wearable devices wearable devices enable theoretically continuous, longitudinal monitoring of physiological measurements such as step count, energy expenditure, and heart rate although the classification of abnormal cardiac rhythms such as atrial fibrillation from wearable devices has great potential, commercial algorithms remain proprietary and tend to focus on heart rate variability derived from green spectrum led sensors placed on the wrist, where noise remains an unsolved problem here we develop deepbeat, a multitask deep learning method to jointly assess signal quality and arrhythmia event detection in wearable photoplethysmography devices for real-time detection of atrial fibrillation the model is trained on approximately one million simulated unlabeled physiological signals and fine-tuned on a curated dataset of over 500 k labeled signals from over 100 individuals from 3 different wearable devices we demonstrate that, in comparison with a single-task model, our architecture using unsupervised transfer learning through convolutional denoising autoencoders dramatically improves the performance of atrial fibrillation detection from a f1 score of 054 to 096 we also include in our evaluation a prospectively derived replication cohort of ambulatory participants where the algorithm performed with high sensitivity 098, specificity 099, and f1 score 093 we show that two-stage training can help address the unbalanced data problem common to biomedical applications, where large-scale well-annotated datasets are hard to generate due to the expense of manual annotation, data acquisition, and participant privacy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58171</th>\n",
       "      <td>new artificial intelligence prediction model using serial prothrombin time international normalized ratio measurements in atrial fibrillation patients on vitamin k antagonists: garfield-af most clinical risk stratification models are based on measurement at a single time-point rather than serial measurements artificial intelligence ai is able to predict one-dimensional outcomes from multi-dimensional datasets using data from global anticoagulant registry in the field garfield-af registry, a new ai model was developed for predicting clinical outcomes in atrial fibrillation af patients up to 1 year based on sequential measures of prothrombin time international normalized ratio pt-inr within 30 days of enrolment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58654</th>\n",
       "      <td>a 1334 μw event-driven patient-specific ann cardiac arrhythmia classifier for wearable ecg sensors artificial neural network ann and its variants are favored algorithm in designing cardiac arrhythmia classifier cac for its high accuracy however, the implementation of ultralow power ann-cac is challenging due to the intensive computations moreover, the imbalanced mit-bih database limits the ann-cac performance several novel techniques are proposed to address the challenges in the low power implementation firstly, continuous-in-time discrete-in-amplitude ctda signal flow is adopted to reduce the multiplication operations secondly, conditional grouping scheme cgs in combination with biased training bt is proposed to handle the imbalanced training samples for better training convergency and evaluation accuracy thirdly, arithmetic unit sharing with customized high-performance multiplier improves the power efficiency verified in fpga and synthesized in 018 μm cmos process, the proposed ctda ann-cac can classify an arrhythmia within 252 μs at 25 mhz clock frequency with average power of 1334 μw for 75bpm heart rate evaluated on mit-bih database, it shows over 98% classification accuracy, 97% sensitivity, and 94% positive predictivity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60082</th>\n",
       "      <td>evaluation of risk prediction models of atrial fibrillation from the multi-ethnic study of atherosclerosis mesa atrial fibrillation af is prevalent and strongly associated with higher cardiovascular disease cvd risk machine learning is increasingly used to identify novel predictors of cvd risk, but prediction improvements beyond established risk scores are uncertain we evaluated improvements in predicting 5-year af risk when adding novel candidate variables identified by machine learning to the charge-af enriched score, which includes age, race/ethnicity, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and nt-probnp we included 3,534 participants mean age, 613 years; 520% female with complete data from the prospective multi-ethnic study of atherosclerosis incident af was defined based on study electrocardiograms and hospital discharge diagnosis icd-9 codes, supplemented by medicare claims prediction performance was evaluated using cox regression and a parsimonious model was selected using lasso within 5 years of baseline, 124 participants had incident af compared with the charge-af enriched model c-statistic, 0804, variables identified by machine learning, including biomarkers, cardiac magnetic resonance imaging variables, electrocardiogram variables, and subclinical cvd variables, did not significantly improve prediction a 23-item score derived by machine learning achieved a c-statistic of 0806, whereas a parsimonious model including the clinical risk factors age, weight, current smoking, nt-probnp, coronary artery calcium score, and cardiac troponin-t achieved a c-statistic of 0802 this analysis confirms that the charge-af enriched model and a parsimonious 6-item model performed similarly to a more extensive model derived by machine learning in conclusion, these simple models remain the gold standard for risk prediction of af, although addition of the coronary artery calcium score should be considered</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71650</th>\n",
       "      <td>a supervised approach to robust photoplethysmography quality assessment early detection of atrial fibrillation afib is crucial to prevent stroke recurrence new tools for monitoring cardiac rhythm are important for risk stratification and stroke prevention as many of new approaches to long-term afib detection are now based on photoplethysmogram ppg recordings from wearable devices, ensuring high ppg signal-to-noise ratios is a fundamental requirement for a robust detection of afib episodes traditionally, signal quality assessment is often based on the evaluation of similarity between pulses to derive signal quality indices there are limitations to using this approach for accurate assessment of ppg quality in the presence of arrhythmia, as in the case of afib, mainly due to substantial changes in pulse morphology in this paper, we first tested the performance of algorithms selected from a body of studies on ppg quality assessment using a dataset of ppg recordings from patients with afib we then propose machine learning approaches for ppg quality assessment in 30-s segments of ppg recording from 13 stroke patients admitted to the university of california san francisco ucsf neuro intensive care unit and another dataset of 3764 patients from one of the five ucsf general intensive care units we used data acquired from two systems, fingertip ppg fppg from a bedside monitor system, and radial ppg rppg measured using a wearable commercial wristband we compared various supervised machine learning techniques including k-nearest neighbors, decisions trees, and a two-class support vector machine svm svm provided the best performance fppg signals were used to build the model and achieved 09477 accuracy when tested on the data from the fppg exclusive to the test set, and 09589 accuracy when tested on the rppg data</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81936</th>\n",
       "      <td>ecg signal classification for the detection of cardiac arrhythmias using a convolutional recurrent neural network the electrocardiogram ecg provides an effective, non-invasive approach for clinical diagnosis in patients with cardiac diseases such as atrial fibrillation af af is the most common cardiac rhythm disturbance and affects ~2% of the general population in industrialized countries automatic af detection in clinics remains a challenging task due to the high inter-patient variability of ecgs, and unsatisfactory existing approaches for af diagnosis eg atrial or ventricular activity-based analyses</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66327</th>\n",
       "      <td>a novel irbf-rvm model for diagnosis of atrial fibrillation atrial fibrillation af is one of the common cardiovascular diseases, and electrocardiography ecg is a key indicator for the detection and diagnosis of af and other heart diseases in this study, an improved machine learning method is proposed for rapid modeling and accurate diagnosis of af</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55722</th>\n",
       "      <td>predicting electrical storm using episodesparameters from icd recorded data electrical storm es is a life-threatening heart condition for patients with implantable cardioverter defibrillators icds icd patients experienced episodes are at higher risk for es however, predicting es using previous episodesparameters recorded by icds have never been developed this study aims to predict es using machine learning models based on icd remote monitoring-summaries during episodes in the anonymized large number of patients episode icd-summaries from 16,022 patients were used to construct and evaluate two models, logistic regression and random forest, for predicting the short-term risk of es episode parameters in this study included the total number of sustained episodes, shocks delivered and the cycle length parameters the models evaluated on the data sections not used for model development random forest performed significantly better than logistic regression p &lt;; 001, achieving a test accuracy of 099 and an area under an roc curve auc of 093 vs an accuracy of 098 and an auc of 090 the total number of previous sustained episodes was the most relevant variables in the both models</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31169</th>\n",
       "      <td>remote atrial fibrillation burden estimation using deep recurrent neural network the atrial fibrillation burden afb is defined as the percentage of time spent in atrial fibrillation af over a long enough monitoring period recent research has suggested the added prognostic value of using the afb compared to a binary diagnosis we evaluate, for the first time, the ability to estimate the afb over long-term continuous recordings, using a deep recurrent neutral network drnn approach</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606</th>\n",
       "      <td>machine learning identification of pro-arrhythmic structures in cardiac fibrosis cardiac fibrosis and other scarring of the heart, arising from conditions ranging from myocardial infarction to ageing, promotes dangerous arrhythmias by blocking the healthy propagation of cardiac excitation owing to the complexity of the dynamics of electrical signalling in the heart, however, the connection between different arrangements of blockage and various arrhythmic consequences remains poorly understood where a mechanism defies traditional understanding, machine learning can be invaluable for enabling accurate prediction of quantities of interest measures of arrhythmic risk in terms of predictor variables such as the arrangement or pattern of obstructive scarring in this study, we simulate the propagation of the action potential ap in tissue affected by fibrotic changes and hence detect sites that initiate re-entrant activation patterns by separately considering multiple different stimulus regimes, we directly observe and quantify the sensitivity of re-entry formation to activation sequence in the fibrotic region then, by extracting the fibrotic structures around locations that both do and do not initiate re-entries, we use neural networks to determine to what extent re-entry initiation is predictable, and over what spatial scale conduction heterogeneities appear to act to produce this effect we find that structural information within about 05 mm of a given point is sufficient to predict structures that initiate re-entry with more than 90% accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148726</th>\n",
       "      <td>predicting termination of atrial fibrillation based on the structure and quantification of the recurrence plot predicting the spontaneous termination of the atrial fibrillation af leads to not only better understanding of mechanisms of the arrhythmia but also the improved treatment of the sustained af a novel method is proposed to characterize the af based on structure and the quantification of the recurrence plot rp to predict the termination of the af the rp of the electrocardiogram ecg signal is firstly obtained and eleven features are extracted to characterize its three basic patterns then the sequential forward search sfs algorithm and davies-bouldin criterion are utilized to select the feature subset which can predict the af termination effectively finally, the multilayer perceptron mlp neural network is applied to predict the af termination an af database which includes one training set and two testing sets a and b of holter ecg recordings is studied experiment results show that 97% of testing set a and 95% of testing set b are correctly classified it demonstrates that this algorithm has the ability to predict the spontaneous termination of the af effectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>application of a machine learning algorithm for detection of atrial fibrillation in secondary care atrial fibrillation af is the most common sustained heart arrhythmia and significantly increases risk of stroke opportunistic af testing in high-risk patients typically requires frequent electrocardiogram tests to capture the arrhythmia risk-prediction algorithms may help to more accurately identify people with undiagnosed af and machine learning ml may aid in the diagnosis of af here, we applied an af-risk prediction algorithm to secondary care data linked to primary care data in the discover database in order to evaluate changes in model performance, and identify patients not previously detected in primary care we identified an additional 5,444 patients who had an af diagnosis only in secondary care during the data extraction period 2,696 495% were accepted by the algorithm and the algorithm correctly assigned 2,637 978% patients to the af cohort using a risk threshold of 74% in patients aged ≥ 30 years, algorithm sensitivity and specificity was 38% and 95%, respectively approximately 15% of af patients assigned to the af cohort by the algorithm had a secondary care diagnosis with no record of af in primary care these additional patients did not substantially alter algorithm performance the additional detection of previously undiagnosed af patients in secondary care highlights unexpected potential utility of this ml algorithm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158473</th>\n",
       "      <td>clinical decision-support for diagnosing stress-related disorders by applying psychophysiological medical knowledge to an instance-based learning system an important procedure in diagnosing stress-related disorders caused by dysfunction in the interaction of the heart with breathing, ie, respiratory sinus arrhythmia rsa, is to analyse the breathing first and then the heart rate analysing these measurements is a time-consuming task for the diagnosing clinician a decision-support system in this area would reduce the analysis task of the clinician and enable him/her to give more attention to the patient we have created a decision-support system which contains a signal classifier and a pattern identifier the system performs an analysis of the physiological time series concerned which would otherwise be performed manually by the clinician</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92772</th>\n",
       "      <td>a deep convolutional neural network model to classify heartbeats the electrocardiogram ecg is a standard test used to monitor the activity of the heart many cardiac abnormalities will be manifested in the ecg including arrhythmia which is a general term that refers to an abnormal heart rhythm the basis of arrhythmia diagnosis is the identification of normal versus abnormal individual heart beats, and their correct classification into different diagnoses, based on ecg morphology heartbeats can be sub-divided into five categories namely non-ectopic, supraventricular ectopic, ventricular ectopic, fusion, and unknown beats it is challenging and time-consuming to distinguish these heartbeats on ecg as these signals are typically corrupted by noise we developed a 9-layer deep convolutional neural network cnn to automatically identify 5 different categories of heartbeats in ecg signals our experiment was conducted in original and noise attenuated sets of ecg signals derived from a publicly available database this set was artificially augmented to even out the number of instances the 5 classes of heartbeats and filtered to remove high-frequency noise the cnn was trained using the augmented data and achieved an accuracy of 9403% and 9347% in the diagnostic classification of heartbeats in original and noise free ecgs, respectively when the cnn was trained with highly imbalanced data original dataset, the accuracy of the cnn reduced to 8907%% and 893% in noisy and noise-free ecgs when properly trained, the proposed cnn model can serve as a tool for screening of ecg to quickly identify different types and frequency of arrhythmic heartbeats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81970</th>\n",
       "      <td>automatic recognition of arrhythmia based on principal component analysis network and linear support vector machine electrocardiogram ecg classification is an important process in identifying arrhythmia, and neural network models have been widely used in this field however, these models are often disrupted by heartbeat noise and are negatively affected by skewed data to address these problems, a novel heartbeat recognition method is presented the aim of this study is to apply a principal component analysis network pcanet for feature extraction based on a noisy ecg signal to improve the classification speed, a linear support vector machine svm was applied in our experiments, we identified five types of imbalanced original and noise-free ecgs in the mit-bih arrhythmia database to verify the effectiveness of our algorithm and achieved 9777% and 9708% accuracy, respectively the results show that our method has high recognition accuracy in the classification of skewed and noisy heartbeats, indicating that our method is a practical ecg recognition method with suitable noise robustness and skewed data applicability</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "9533    automated localization of focal ventricular tachycardia from simulated implanted device electrograms: a combined physics-ai approach <b>background:</b> focal ventricular tachycardia vt is a life-threating arrhythmia, responsible for high morbidity rates and sudden cardiac death scd radiofrequency ablation is the only curative therapy against incessant vt; however, its success is dependent on accurate localization of its source, which is highly invasive and time-consuming <b>objective:</b> the goal of our study is, as a proof of concept, to demonstrate the possibility of utilizing electrogram egm recordings from cardiac implantable electronic devices cieds to achieve this, we utilize fast and accurate whole torso electrophysiological ep simulations in conjunction with convolutional neural networks cnns to automate the localization of focal vts using simulated egms <b>materials and methods:</b> a highly detailed 3d torso model was used to simulate ∼4000 focal vts, evenly distributed across the left ventricle lv, utilizing a rapid reaction-eikonal environment solutions were subsequently combined with lead field computations on the torso to derive accurate electrocardiograms ecgs and egm traces, which were used as inputs to cnns to localize focal sources we compared the localization performance of a previously developed cnn architecture cartesian probability-based with our novel cnn algorithm utilizing universal ventricular coordinates uvcs <b>results:</b> implanted device egms successfully localized vt sources with localization error 874 mm comparable to ecg-based localization 669 mm our novel uvc cnn architecture outperformed the existing cartesian probability-based algorithm errors = 406 mm and 807 mm for ecgs and egms, respectively overall, localization was relatively insensitive to noise and changes in body compositions; however, displacements in ecg electrodes and cied leads caused performance to decrease errors 16-25 mm <b>conclusion:</b> egm recordings from implanted devices may be used to successfully, and robustly, localize focal vt sources, and aid ablation planning   \n",
       "137756                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           field programmable gate array based fuzzy neural signal processing system for differential diagnosis of qrs complex tachycardia and tachyarrhythmia in noisy ecg signals the paper reports of a field programmable gate array fpga based embedded system for detection of qrs complex in a noisy electrocardiogram ecg signal and thereafter differential diagnosis of tachycardia and tachyarrhythmia the qrs complex has been detected after application of entropy measure of fuzziness to build a detection function of ecg signal, which has been previously filtered to remove power line interference and base line wander using the detected qrs complexes, differential diagnosis of tachycardia and tachyarrhythmia has been performed the entire algorithm has been realized in hardware on an fpga using the standard cse ecg database, the algorithm performed highly effectively the performance of the algorithm in respect of qrs detection with sensitivity se of 9974% and accuracy of 995% is achieved when tested using single channel ecg with entropy criteria the performance of the qrs detection system has been compared and found to be better than most of the qrs detection systems available in literature using the system, 200 patients have been diagnosed with an accuracy of 985%   \n",
       "111202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     symmetrical compression distance for arrhythmia discrimination in cloud-based big-data services the current development of cloud computing is completely changing the paradigm of data knowledge extraction in huge databases an example of this technology in the cardiac arrhythmia field is the scoop platform, a national-level scientific cloud-based big data service for implantable cardioverter defibrillators in this scenario, we here propose a new methodology for automatic classification of intracardiac electrograms egms in a cloud computing system, designed for minimal signal preprocessing a new compression-based similarity measure csm is created for low computational burden, so-called weighted fast compression distance, which provides better performance when compared with other csms in the literature using simple machine learning techniques, a set of 6848 egms extracted from scoop platform were classified into seven cardiac arrhythmia classes and one noise class, reaching near to 90% accuracy when previous patient arrhythmia information was available and 63% otherwise, hence overcoming in all cases the classification provided by the majority class results show that this methodology can be used as a high-quality service of cloud computing, providing support to physicians for improving the knowledge on patient diagnosis    \n",
       "113277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   variational bayesian electrophysiological imaging of myocardial infarction the presence, size, and distribution of ischemic tissue bear significant prognostic and therapeutic implication for ventricular arrhythmias while many approaches to 3d infarct detection have been developed via electrophysiological ep imaging from noninvasive electrocardiographic data, this ill-posed inverse problem remains challenging especially for septal infarcts that are hidden from body-surface data we propose a variational bayesian framework for ep imaging of 3d infarct using a total-variation prior the posterior distribution of intramural action potential and all regularization parameters are estimated from body-surface data by minimizing the kullback-leibler divergence because of the uncertainty introduced in prior models, we hypothesize that the solution uncertainty plays as important a role as the point estimate in interpreting the reconstruction this is verified in a set of phantom and real-data experiments, where regions of low confidence help to eliminate false-positives and to accurately identify infarcts of various locations including septum and distributions owing to the ability of total-variation prior in extracting the boundary between smooth regions, the presented method also has the potential to outline infarct border that is the most critical region responsible for ventricular arrhvthmias   \n",
       "138791                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           determination of a new vlf band in hrv for ventricular tachyarrhythmia patients this study presents a new very low frequency vlf band range in ventricular tachyarrhythmia patients and involves an approach for estimation of effect of vlf band on ventricular tachyarrhythmia patients a model based on wavelet packets wp and multilayer perceptron neural network mlpnn is used for determination of effective vlf band in heart rate variability hrv signals hrv is decomposed into sub-bands including very low frequency parts and variations of energy are analyzed domination test is done using mlpnn and dominant band is determined as a result, a new vlf band was described in 00039063-003125 hz frequency range this method can be used for other bands or other arrhythmia patients especially, estimation of dominant band energy using this method can be helped to diagnose for applications where have important effect of characteristic band   \n",
       "4339                                                                                                                                                                                                                                                                                                                                                                                                                                         multi-task deep learning for cardiac rhythm detection in wearable devices wearable devices enable theoretically continuous, longitudinal monitoring of physiological measurements such as step count, energy expenditure, and heart rate although the classification of abnormal cardiac rhythms such as atrial fibrillation from wearable devices has great potential, commercial algorithms remain proprietary and tend to focus on heart rate variability derived from green spectrum led sensors placed on the wrist, where noise remains an unsolved problem here we develop deepbeat, a multitask deep learning method to jointly assess signal quality and arrhythmia event detection in wearable photoplethysmography devices for real-time detection of atrial fibrillation the model is trained on approximately one million simulated unlabeled physiological signals and fine-tuned on a curated dataset of over 500 k labeled signals from over 100 individuals from 3 different wearable devices we demonstrate that, in comparison with a single-task model, our architecture using unsupervised transfer learning through convolutional denoising autoencoders dramatically improves the performance of atrial fibrillation detection from a f1 score of 054 to 096 we also include in our evaluation a prospectively derived replication cohort of ambulatory participants where the algorithm performed with high sensitivity 098, specificity 099, and f1 score 093 we show that two-stage training can help address the unbalanced data problem common to biomedical applications, where large-scale well-annotated datasets are hard to generate due to the expense of manual annotation, data acquisition, and participant privacy   \n",
       "58171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  new artificial intelligence prediction model using serial prothrombin time international normalized ratio measurements in atrial fibrillation patients on vitamin k antagonists: garfield-af most clinical risk stratification models are based on measurement at a single time-point rather than serial measurements artificial intelligence ai is able to predict one-dimensional outcomes from multi-dimensional datasets using data from global anticoagulant registry in the field garfield-af registry, a new ai model was developed for predicting clinical outcomes in atrial fibrillation af patients up to 1 year based on sequential measures of prothrombin time international normalized ratio pt-inr within 30 days of enrolment   \n",
       "58654                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  a 1334 μw event-driven patient-specific ann cardiac arrhythmia classifier for wearable ecg sensors artificial neural network ann and its variants are favored algorithm in designing cardiac arrhythmia classifier cac for its high accuracy however, the implementation of ultralow power ann-cac is challenging due to the intensive computations moreover, the imbalanced mit-bih database limits the ann-cac performance several novel techniques are proposed to address the challenges in the low power implementation firstly, continuous-in-time discrete-in-amplitude ctda signal flow is adopted to reduce the multiplication operations secondly, conditional grouping scheme cgs in combination with biased training bt is proposed to handle the imbalanced training samples for better training convergency and evaluation accuracy thirdly, arithmetic unit sharing with customized high-performance multiplier improves the power efficiency verified in fpga and synthesized in 018 μm cmos process, the proposed ctda ann-cac can classify an arrhythmia within 252 μs at 25 mhz clock frequency with average power of 1334 μw for 75bpm heart rate evaluated on mit-bih database, it shows over 98% classification accuracy, 97% sensitivity, and 94% positive predictivity   \n",
       "60082                                                                                                          evaluation of risk prediction models of atrial fibrillation from the multi-ethnic study of atherosclerosis mesa atrial fibrillation af is prevalent and strongly associated with higher cardiovascular disease cvd risk machine learning is increasingly used to identify novel predictors of cvd risk, but prediction improvements beyond established risk scores are uncertain we evaluated improvements in predicting 5-year af risk when adding novel candidate variables identified by machine learning to the charge-af enriched score, which includes age, race/ethnicity, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and nt-probnp we included 3,534 participants mean age, 613 years; 520% female with complete data from the prospective multi-ethnic study of atherosclerosis incident af was defined based on study electrocardiograms and hospital discharge diagnosis icd-9 codes, supplemented by medicare claims prediction performance was evaluated using cox regression and a parsimonious model was selected using lasso within 5 years of baseline, 124 participants had incident af compared with the charge-af enriched model c-statistic, 0804, variables identified by machine learning, including biomarkers, cardiac magnetic resonance imaging variables, electrocardiogram variables, and subclinical cvd variables, did not significantly improve prediction a 23-item score derived by machine learning achieved a c-statistic of 0806, whereas a parsimonious model including the clinical risk factors age, weight, current smoking, nt-probnp, coronary artery calcium score, and cardiac troponin-t achieved a c-statistic of 0802 this analysis confirms that the charge-af enriched model and a parsimonious 6-item model performed similarly to a more extensive model derived by machine learning in conclusion, these simple models remain the gold standard for risk prediction of af, although addition of the coronary artery calcium score should be considered   \n",
       "71650                                                                                                                                                                                                                                                                                          a supervised approach to robust photoplethysmography quality assessment early detection of atrial fibrillation afib is crucial to prevent stroke recurrence new tools for monitoring cardiac rhythm are important for risk stratification and stroke prevention as many of new approaches to long-term afib detection are now based on photoplethysmogram ppg recordings from wearable devices, ensuring high ppg signal-to-noise ratios is a fundamental requirement for a robust detection of afib episodes traditionally, signal quality assessment is often based on the evaluation of similarity between pulses to derive signal quality indices there are limitations to using this approach for accurate assessment of ppg quality in the presence of arrhythmia, as in the case of afib, mainly due to substantial changes in pulse morphology in this paper, we first tested the performance of algorithms selected from a body of studies on ppg quality assessment using a dataset of ppg recordings from patients with afib we then propose machine learning approaches for ppg quality assessment in 30-s segments of ppg recording from 13 stroke patients admitted to the university of california san francisco ucsf neuro intensive care unit and another dataset of 3764 patients from one of the five ucsf general intensive care units we used data acquired from two systems, fingertip ppg fppg from a bedside monitor system, and radial ppg rppg measured using a wearable commercial wristband we compared various supervised machine learning techniques including k-nearest neighbors, decisions trees, and a two-class support vector machine svm svm provided the best performance fppg signals were used to build the model and achieved 09477 accuracy when tested on the data from the fppg exclusive to the test set, and 09589 accuracy when tested on the rppg data   \n",
       "81936                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ecg signal classification for the detection of cardiac arrhythmias using a convolutional recurrent neural network the electrocardiogram ecg provides an effective, non-invasive approach for clinical diagnosis in patients with cardiac diseases such as atrial fibrillation af af is the most common cardiac rhythm disturbance and affects ~2% of the general population in industrialized countries automatic af detection in clinics remains a challenging task due to the high inter-patient variability of ecgs, and unsatisfactory existing approaches for af diagnosis eg atrial or ventricular activity-based analyses   \n",
       "66327                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   a novel irbf-rvm model for diagnosis of atrial fibrillation atrial fibrillation af is one of the common cardiovascular diseases, and electrocardiography ecg is a key indicator for the detection and diagnosis of af and other heart diseases in this study, an improved machine learning method is proposed for rapid modeling and accurate diagnosis of af   \n",
       "55722                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               predicting electrical storm using episodesparameters from icd recorded data electrical storm es is a life-threatening heart condition for patients with implantable cardioverter defibrillators icds icd patients experienced episodes are at higher risk for es however, predicting es using previous episodesparameters recorded by icds have never been developed this study aims to predict es using machine learning models based on icd remote monitoring-summaries during episodes in the anonymized large number of patients episode icd-summaries from 16,022 patients were used to construct and evaluate two models, logistic regression and random forest, for predicting the short-term risk of es episode parameters in this study included the total number of sustained episodes, shocks delivered and the cycle length parameters the models evaluated on the data sections not used for model development random forest performed significantly better than logistic regression p <; 001, achieving a test accuracy of 099 and an area under an roc curve auc of 093 vs an accuracy of 098 and an auc of 090 the total number of previous sustained episodes was the most relevant variables in the both models   \n",
       "31169                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              remote atrial fibrillation burden estimation using deep recurrent neural network the atrial fibrillation burden afb is defined as the percentage of time spent in atrial fibrillation af over a long enough monitoring period recent research has suggested the added prognostic value of using the afb compared to a binary diagnosis we evaluate, for the first time, the ability to estimate the afb over long-term continuous recordings, using a deep recurrent neutral network drnn approach   \n",
       "4606                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      machine learning identification of pro-arrhythmic structures in cardiac fibrosis cardiac fibrosis and other scarring of the heart, arising from conditions ranging from myocardial infarction to ageing, promotes dangerous arrhythmias by blocking the healthy propagation of cardiac excitation owing to the complexity of the dynamics of electrical signalling in the heart, however, the connection between different arrangements of blockage and various arrhythmic consequences remains poorly understood where a mechanism defies traditional understanding, machine learning can be invaluable for enabling accurate prediction of quantities of interest measures of arrhythmic risk in terms of predictor variables such as the arrangement or pattern of obstructive scarring in this study, we simulate the propagation of the action potential ap in tissue affected by fibrotic changes and hence detect sites that initiate re-entrant activation patterns by separately considering multiple different stimulus regimes, we directly observe and quantify the sensitivity of re-entry formation to activation sequence in the fibrotic region then, by extracting the fibrotic structures around locations that both do and do not initiate re-entries, we use neural networks to determine to what extent re-entry initiation is predictable, and over what spatial scale conduction heterogeneities appear to act to produce this effect we find that structural information within about 05 mm of a given point is sufficient to predict structures that initiate re-entry with more than 90% accuracy   \n",
       "148726                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               predicting termination of atrial fibrillation based on the structure and quantification of the recurrence plot predicting the spontaneous termination of the atrial fibrillation af leads to not only better understanding of mechanisms of the arrhythmia but also the improved treatment of the sustained af a novel method is proposed to characterize the af based on structure and the quantification of the recurrence plot rp to predict the termination of the af the rp of the electrocardiogram ecg signal is firstly obtained and eleven features are extracted to characterize its three basic patterns then the sequential forward search sfs algorithm and davies-bouldin criterion are utilized to select the feature subset which can predict the af termination effectively finally, the multilayer perceptron mlp neural network is applied to predict the af termination an af database which includes one training set and two testing sets a and b of holter ecg recordings is studied experiment results show that 97% of testing set a and 95% of testing set b are correctly classified it demonstrates that this algorithm has the ability to predict the spontaneous termination of the af effectively   \n",
       "13604                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        application of a machine learning algorithm for detection of atrial fibrillation in secondary care atrial fibrillation af is the most common sustained heart arrhythmia and significantly increases risk of stroke opportunistic af testing in high-risk patients typically requires frequent electrocardiogram tests to capture the arrhythmia risk-prediction algorithms may help to more accurately identify people with undiagnosed af and machine learning ml may aid in the diagnosis of af here, we applied an af-risk prediction algorithm to secondary care data linked to primary care data in the discover database in order to evaluate changes in model performance, and identify patients not previously detected in primary care we identified an additional 5,444 patients who had an af diagnosis only in secondary care during the data extraction period 2,696 495% were accepted by the algorithm and the algorithm correctly assigned 2,637 978% patients to the af cohort using a risk threshold of 74% in patients aged ≥ 30 years, algorithm sensitivity and specificity was 38% and 95%, respectively approximately 15% of af patients assigned to the af cohort by the algorithm had a secondary care diagnosis with no record of af in primary care these additional patients did not substantially alter algorithm performance the additional detection of previously undiagnosed af patients in secondary care highlights unexpected potential utility of this ml algorithm   \n",
       "158473                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  clinical decision-support for diagnosing stress-related disorders by applying psychophysiological medical knowledge to an instance-based learning system an important procedure in diagnosing stress-related disorders caused by dysfunction in the interaction of the heart with breathing, ie, respiratory sinus arrhythmia rsa, is to analyse the breathing first and then the heart rate analysing these measurements is a time-consuming task for the diagnosing clinician a decision-support system in this area would reduce the analysis task of the clinician and enable him/her to give more attention to the patient we have created a decision-support system which contains a signal classifier and a pattern identifier the system performs an analysis of the physiological time series concerned which would otherwise be performed manually by the clinician   \n",
       "92772                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a deep convolutional neural network model to classify heartbeats the electrocardiogram ecg is a standard test used to monitor the activity of the heart many cardiac abnormalities will be manifested in the ecg including arrhythmia which is a general term that refers to an abnormal heart rhythm the basis of arrhythmia diagnosis is the identification of normal versus abnormal individual heart beats, and their correct classification into different diagnoses, based on ecg morphology heartbeats can be sub-divided into five categories namely non-ectopic, supraventricular ectopic, ventricular ectopic, fusion, and unknown beats it is challenging and time-consuming to distinguish these heartbeats on ecg as these signals are typically corrupted by noise we developed a 9-layer deep convolutional neural network cnn to automatically identify 5 different categories of heartbeats in ecg signals our experiment was conducted in original and noise attenuated sets of ecg signals derived from a publicly available database this set was artificially augmented to even out the number of instances the 5 classes of heartbeats and filtered to remove high-frequency noise the cnn was trained using the augmented data and achieved an accuracy of 9403% and 9347% in the diagnostic classification of heartbeats in original and noise free ecgs, respectively when the cnn was trained with highly imbalanced data original dataset, the accuracy of the cnn reduced to 8907%% and 893% in noisy and noise-free ecgs when properly trained, the proposed cnn model can serve as a tool for screening of ecg to quickly identify different types and frequency of arrhythmic heartbeats   \n",
       "81970                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           automatic recognition of arrhythmia based on principal component analysis network and linear support vector machine electrocardiogram ecg classification is an important process in identifying arrhythmia, and neural network models have been widely used in this field however, these models are often disrupted by heartbeat noise and are negatively affected by skewed data to address these problems, a novel heartbeat recognition method is presented the aim of this study is to apply a principal component analysis network pcanet for feature extraction based on a noisy ecg signal to improve the classification speed, a linear support vector machine svm was applied in our experiments, we identified five types of imbalanced original and noise-free ecgs in the mit-bih arrhythmia database to verify the effectiveness of our algorithm and achieved 9777% and 9708% accuracy, respectively the results show that our method has high recognition accuracy in the classification of skewed and noisy heartbeats, indicating that our method is a practical ecg recognition method with suitable noise robustness and skewed data applicability   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "9533          0       0       0           0          0        0       0   \n",
       "137756        0       0       0           0          0        0       0   \n",
       "111202        0       0       0           0          0        0       0   \n",
       "113277        0       0       0           0          0        0       0   \n",
       "138791        0       0       0           0          0        0       0   \n",
       "4339          0       0       0           0          0        0       0   \n",
       "58171         0       0       0           0          0        0       0   \n",
       "58654         0       0       0           0          0        0       0   \n",
       "60082         0       0       0           0          0        0       0   \n",
       "71650         1       0       0           0          0        0       0   \n",
       "81936         0       0       0           0          0        0       0   \n",
       "66327         0       0       0           0          0        0       0   \n",
       "55722         0       0       0           0          0        0       0   \n",
       "31169         0       0       0           0          0        0       0   \n",
       "4606          0       0       0           0          0        0       0   \n",
       "148726        0       0       0           0          0        0       0   \n",
       "13604         0       0       0           0          0        0       0   \n",
       "158473        0       0       0           0          0        0       0   \n",
       "92772         0       0       0           0          0        0       0   \n",
       "81970         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "9533             0            0         0           0        0       0   \n",
       "137756           0            0         0           0        0       0   \n",
       "111202           0            0         0           0        0       0   \n",
       "113277           0            0         0           0        0       0   \n",
       "138791           0            0         0           0        0       0   \n",
       "4339             0            0         0           0        0       0   \n",
       "58171            0            0         0           0        0       0   \n",
       "58654            0            0         0           0        0       0   \n",
       "60082            0            0         0           0        0       0   \n",
       "71650            0            0         0           0        0       0   \n",
       "81936            0            0         0           0        0       0   \n",
       "66327            0            0         0           0        0       0   \n",
       "55722            0            0         0           0        0       0   \n",
       "31169            0            0         0           0        0       0   \n",
       "4606             0            0         0           0        0       0   \n",
       "148726           0            0         0           0        0       0   \n",
       "13604            0            0         0           0        0       0   \n",
       "158473           0            0         0           0        0       0   \n",
       "92772            0            0         0           0        0       0   \n",
       "81970            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "9533             0             0           0            0         0   \n",
       "137756           0             0           0            0         0   \n",
       "111202           0             0           0            0         0   \n",
       "113277           0             0           0            0         0   \n",
       "138791           0             0           0            0         0   \n",
       "4339             0             0           0            0         0   \n",
       "58171            0             0           0            0         0   \n",
       "58654            0             0           0            0         0   \n",
       "60082            0             0           0            0         0   \n",
       "71650            0             0           0            0         0   \n",
       "81936            0             0           0            0         0   \n",
       "66327            0             0           0            0         0   \n",
       "55722            0             0           0            0         0   \n",
       "31169            0             0           0            0         0   \n",
       "4606             0             0           0            0         0   \n",
       "148726           0             0           0            0         0   \n",
       "13604            0             0           0            0         0   \n",
       "158473           0             0           0            0         0   \n",
       "92772            0             0           0            0         0   \n",
       "81970            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "9533            0            0           0            0           0   \n",
       "137756          0            0           0            0           0   \n",
       "111202          0            0           0            0           0   \n",
       "113277          0            0           0            0           0   \n",
       "138791          0            0           0            0           0   \n",
       "4339            0            0           0            0           0   \n",
       "58171           0            0           0            0           0   \n",
       "58654           0            0           0            0           0   \n",
       "60082           0            0           0            0           0   \n",
       "71650           0            0           0            0           0   \n",
       "81936           0            0           0            0           0   \n",
       "66327           0            0           0            0           0   \n",
       "55722           0            0           0            0           0   \n",
       "31169           0            0           0            0           0   \n",
       "4606            0            0           0            0           0   \n",
       "148726          0            0           0            0           0   \n",
       "13604           0            0           0            0           0   \n",
       "158473          0            0           0            0           0   \n",
       "92772           0            0           0            0           0   \n",
       "81970           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "9533              0          0            0        0         0          0   \n",
       "137756            0          0            0        0         0          0   \n",
       "111202            0          0            0        0         0          0   \n",
       "113277            0          0            0        0         0          0   \n",
       "138791            0          0            0        0         0          0   \n",
       "4339              0          0            0        0         0          0   \n",
       "58171             0          0            0        0         0          0   \n",
       "58654             0          0            0        0         0          0   \n",
       "60082             0          0            0        0         0          0   \n",
       "71650             0          0            0        0         0          0   \n",
       "81936             0          0            0        0         0          0   \n",
       "66327             0          0            0        0         0          0   \n",
       "55722             0          0            0        0         0          0   \n",
       "31169             0          0            0        0         0          0   \n",
       "4606              0          0            0        0         0          0   \n",
       "148726            0          0            0        0         0          0   \n",
       "13604             0          0            0        0         0          0   \n",
       "158473            0          1            0        0         0          0   \n",
       "92772             0          0            0        0         0          0   \n",
       "81970             0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "9533         0        0         0          0        0       0         0   \n",
       "137756       0        0         0          0        0       0         0   \n",
       "111202       0        0         0          0        0       0         0   \n",
       "113277       0        0         0          0        0       0         0   \n",
       "138791       0        0         0          0        0       0         0   \n",
       "4339         0        0         0          0        0       0         0   \n",
       "58171        0        0         0          0        0       0         0   \n",
       "58654        0        0         0          0        0       0         0   \n",
       "60082        0        0         0          0        0       0         0   \n",
       "71650        0        0         0          0        0       0         0   \n",
       "81936        0        0         0          0        0       0         0   \n",
       "66327        0        0         0          0        0       0         0   \n",
       "55722        0        0         0          0        0       0         0   \n",
       "31169        0        0         0          0        0       0         0   \n",
       "4606         0        0         0          0        0       0         0   \n",
       "148726       0        0         0          0        0       0         0   \n",
       "13604        0        0         0          0        0       0         0   \n",
       "158473       0        0         1          0        0       0         0   \n",
       "92772        0        0         0          0        0       0         0   \n",
       "81970        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "9533            0        0           0         0        1        0       0   \n",
       "137756          0        0           0         0        1        0       0   \n",
       "111202          0        0           0         0        1        0       0   \n",
       "113277          0        0           0         0        1        1       0   \n",
       "138791          0        0           0         0        1        0       0   \n",
       "4339            0        0           0         0        1        0       0   \n",
       "58171           0        0           0         0        1        0       0   \n",
       "58654           0        0           0         0        1        0       0   \n",
       "60082           0        0           0         0        1        1       0   \n",
       "71650           1        0           0         0        1        0       0   \n",
       "81936           0        0           0         0        1        0       0   \n",
       "66327           0        0           0         0        1        0       0   \n",
       "55722           0        0           0         0        0        0       0   \n",
       "31169           0        0           0         0        1        0       0   \n",
       "4606            0        0           0         0        1        1       0   \n",
       "148726          0        0           0         0        1        0       0   \n",
       "13604           0        0           0         0        1        0       0   \n",
       "158473          0        0           0         0        1        0       0   \n",
       "92772           0        0           0         0        1        0       0   \n",
       "81970           0        0           0         0        1        0       0   \n",
       "\n",
       "       arrhyt_text  \n",
       "9533             1  \n",
       "137756           1  \n",
       "111202           1  \n",
       "113277           1  \n",
       "138791           1  \n",
       "4339             1  \n",
       "58171            1  \n",
       "58654            1  \n",
       "60082            1  \n",
       "71650            1  \n",
       "81936            1  \n",
       "66327            1  \n",
       "55722            1  \n",
       "31169            1  \n",
       "4606             1  \n",
       "148726           1  \n",
       "13604            1  \n",
       "158473           1  \n",
       "92772            1  \n",
       "81970            1  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['arrhyt_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41254e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33695, '1': 484})\n"
     ]
    }
   ],
   "source": [
    "## ENDOCRINE [C19] - no dm / endo\n",
    "\n",
    "## text\n",
    "text = ['acromegaly', 'adrenal', 'addisons', 'conns syn', 'cushings synd', 'cushings disease', 'thyroid', 'graves disease',\n",
    "       'hashimoto', 'polycystic ovary', 'prolactin', 'pituitar', 'androgen', 'testosterone', 'gonadism', 'gonadal']\n",
    "\n",
    "spec['endo_text'] = np.where(groups['text'].str.contains('endocrin'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['endo_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['endo_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## outputs\n",
    "print('text counts:')\n",
    "print(Counter(spec['endo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "197c41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32897, '1': 1282})\n"
     ]
    }
   ],
   "source": [
    "#### DIABETES - all / dm\n",
    "\n",
    "## text\n",
    "text = ['diabet', 'mellitus', 'hypoglycemia', 'hypoglycaemi', 'hyperglycemi', 'hyperglycaemi', 'insulin', 'glucagon',\n",
    "        'islet cell'\n",
    "       ]\n",
    "\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains('diabetes'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['dm_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['dm_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains(\"insipidus\"), \"0\", spec['dm_text'])\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains('growth factor'), \"0\", spec['dm_text'])\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains(' igf'), \"1\", spec['dm_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['dm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d6ac8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34015, '1': 164})\n"
     ]
    }
   ],
   "source": [
    "#### DIABETES - insulin / insulin\n",
    "\n",
    "spec['insulin_text'] = np.where(groups['text'].str.contains('insulin'), \"1\", \"0\")\n",
    "\n",
    "\n",
    "spec['insulin_text'] = np.where(groups['text'].str.contains('growth factor'), \"0\", spec['insulin_text'])\n",
    "spec['insulin_text'] = np.where(groups['text'].str.contains(' igf'), \"0\", spec['insulin_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['insulin_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4f15d1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77216</th>\n",
       "      <td>predicting and understanding the response to short-term intensive insulin therapy in people with early type 2 diabetes short-term intensive insulin therapy iit early in the course of type 2 diabetes acutely improves beta-cell function with long-lasting effects on glycemic control however, conventional measures cannot determine which patients are better suited for iit, and little is known about the molecular mechanisms determining response therefore, this study aimed to develop a model that could accurately predict the response to iit and provide insight into molecular mechanisms driving such response in humans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>hippocampal volume reduction is associated with direct measure of insulin resistance in adults hippocampal integrity is highly susceptible to metabolic dysfunction, yet its mechanisms are not well defined we studied 126 healthy individuals aged 23-61 years insulin resistance ir was quantified by measuring steady-state plasma glucose sspg concentration during the insulin suppression test body mass index bmi, adiposity, fasting insulin, glucose, leptin as well as structural neuroimaing with automatic hippocampal subfield segmentation were performed data analysis using unsupervised machine learning k-means clustering identified two subgroups reflecting a pattern of more pronounced hippocampal volume reduction being concurrently associated with greater adiposity and insulin resistance; the hippocampal volume reductions were uniform across subfields individuals in the most deviant subgroup were predominantly women 79 versus 42 % with higher bmi 279 25 versus 305 46 kg/m&lt;sup&gt;2&lt;/sup&gt;, ir sspg concentration, 156 61 versus 123 70 mg/dl and leptinemia 217 170 versus 445 304 μg/l the use of person-based modeling in healthy individuals suggests that adiposity, insulin resistance and compromised structural hippocampal integrity behave as a composite phenotype; female sex emerged as risk factor for this phenotype</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32600</th>\n",
       "      <td>a novel cgm metric-gradient and combining mean sensor glucose enable to improve the prediction of nocturnal hypoglycemic events in patients with diabetes nocturnal hypoglycemia is a serious complication of insulin-treated diabetes, and it is often asymptomatic a novel cgm metric-gradient was proposed in this paper, and a method of combining mean sensor glucose msg and gradient was presented for the prediction of nocturnal hypoglycemia for this purpose, the data from continuous glucose monitoring cgm encompassing 1,921 patients with diabetes were analyzed, and a total of 302 nocturnal hypoglycemic events were recorded the msg and gradient values were calculated, respectively, and then combined as a new metric &lt;i&gt;ie&lt;/i&gt;, msg+gradient in addition, the prediction was conducted by four algorithms, namely, logistic regression, support vector machine, random forest, and long short-term memory the results revealed that the gradient of cgm showed a downward trend before hypoglycemic events happened additionally, the results indicated that the specificity and sensitivity based on the proposed method were better than the conventional metrics of low blood glucose index lbgi, coefficient of variation cv, mean absolute glucose mag, lability index li, &lt;i&gt;etc&lt;/i&gt;, and the complex metrics of msg+lbgi, msg+cv, msg+mag, and msg+li, &lt;i&gt;etc&lt;/i&gt; specifically, the specificity and sensitivity were greater than 9607% and 9603% at the prediction horizon of 15 minutes and greater than 8779% and 9007% at the prediction horizon of 30 minutes when the proposed method was adopted to predict nocturnal hypoglycemic events in the aforementioned four algorithms therefore, the proposed method of combining msg and gradient may enable to improve the prediction of nocturnal hypoglycemic events future studies are warranted to confirm the validity of this metric</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168634</th>\n",
       "      <td>a simple prediction rule and a neural network model to predict pancreatic beta-cell reserve in young adults with diabetes mellitus in the present study we developed and assessed the performance of a simple prediction rule and a neural network model to predict beta-cell reserve in young adults with diabetes eighty three young adults with diabetes were included in the study all were less than 40 years old and without apparent secondary causes of diabetes the subjects were randomly allocated to 2 groups; group 1 n = 59 for developing a prediction rule and training a neural network, group 2 n = 24 for validation purpose the prediction rule was developed by using stepwise logistic regression using stepwise logistic regression and modification of the derived equation, the patient would be insulin deficient if 3waist circumference in cm + 4age at diagnosis &lt; 340 in the absence of previous diabetic ketoacidosis dka or &lt; 400 in the presence of previous dka when tested in the validation set, the prediction rule had positive and negative predictive values of 867 per cent and 778 per cent respectively with 833 per cent accuracy while the ann model had a positive predictive value of 882 per cent and a negative predictive value of 100 per cent with 917 per cent accuracy when testing the performance of the prediction rule and the ann model compared to the assessment of 23 internists in a subgroup of 9 diabetics whose age at onset was less than 30 years and without a history of dka, the ann had the highest ability to predict beta-cell reserve accuracy = 889, followed by the prediction rule accuracy = 778% and assessments by internists accuracy = 609% we concluded that beta-cell reserve in young adults with diabetes mellitus could be predicted by a simple prediction rule or a neural network model the prediction rule and the neural network model can be helpful clinically in patients with mixed clinical features of type 1 and type 2 diabetes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65522</th>\n",
       "      <td>glunet: a deep learning framework for accurate glucose forecasting for people with type 1 diabetes t1d, forecasting of blood glucose bg can be used to effectively avoid hyperglycemia, hypoglycemia and associated complications the latest continuous glucose monitoring cgm technology allows people to observe glucose in real-time however, an accurate glucose forecast remains a challenge in this work, we introduce glunet, a framework that leverages on a personalized deep neural network to predict the probabilistic distribution of short-term 30-60 minutes future cgm measurements for subjects with t1d based on their historical data including glucose measurements, meal information, insulin doses, and other factors it adopts the latest deep learning techniques consisting of four components: data pre-processing, label transform/recover, multi-layers of dilated convolution neural network cnn, and post-processing the method is evaluated in-silico for both adult and adolescent subjects the results show significant improvements over existing methods in the literature through a comprehensive comparison in terms of root mean square error rmse formula: see text mg/dl with short time lag formula: see text minutes for prediction horizons ph = 30 mins minutes, and rmse formula: see text mg/dl with time lag formula: see text mins for ph = 60 mins for virtual adult subjects in addition, glunet is also tested on two clinical data sets results show that it achieves an rmse formula: see text mg/dl with time lag formula: see text mins for ph = 30 mins and an rmse formula: see text mg/dl with time lag formula: see text mins for ph = 60 mins these are the best reported results for glucose forecasting when compared with other methods including the neural network for predicting glucose nnpg, the support vector regression svr, the latent variable with exogenous input lvx, and the auto regression with exogenous input arx algorithm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150895</th>\n",
       "      <td>neural network based glucose - insulin metabolism models for children with type 1 diabetes in this paper two models for the simulation of glucose-insulin metabolism of children with type 1 diabetes are presented the models are based on the combined use of compartmental models cms and artificial neural networks nns data from children with type 1 diabetes, stored in a database, have been used as input to the models the data are taken from four children with type 1 diabetes and contain information about glucose levels taken from continuous glucose monitoring system, insulin intake and food intake, along with corresponding time the influences of taken insulin on plasma insulin concentration, as well as the effect of food intake on glucose input into the blood from the gut, are estimated from the cms the outputs of cms, along with previous glucose measurements, are fed to a nn, which provides short-term prediction of glucose values for comparative reasons two different nn architectures have been tested: a feed-forward nn ffnn trained with the back-propagation algorithm with adaptive learning rate and momentum, and a recurrent nn rnn, trained with the real time recurrent learning rtrl algorithm the results indicate that the best prediction performance can be achieved by the use of rnn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68229</th>\n",
       "      <td>prediction and prevention of hypoglycaemic events in type-1 diabetic patients using machine learning tight blood glucose control reduces the risk of microvascular and macrovascular complications in patients with type 1 diabetes however, this is very difficult due to the large intra-individual variability and other factors that affect glycaemic control the main limiting factor to achieve strict control of glucose levels in patients on intensive insulin therapy is the risk of severe hypoglycaemia therefore, hypoglycaemia is the main safety problem in the treatment of type 1 diabetes, negatively affecting the quality of life of patients suffering from this disease decision support tools based on machine learning methods have become a viable way to enhance patient safety by anticipating adverse glycaemic events this study proposes the application of four machine learning algorithms to tackle the problem of safety in diabetes management: 1 grammatical evolution for the mid-term continuous prediction of blood glucose levels, 2 support vector machines to predict hypoglycaemic events during postprandial periods, 3 artificial neural networks to predict hypoglycaemic episodes overnight, and 4 data mining to profile diabetes management scenarios the proposal consists of the combination of prediction and classification capabilities of the implemented approaches the resulting system significantly reduces the number of episodes of hypoglycaemia, improving safety and providing patients with greater confidence in decision-making</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128784</th>\n",
       "      <td>natural occurrence of nocturnal hypoglycemia detection using hybrid particle swarm optimized fuzzy reasoning model low blood glucose hypoglycemia is a common and serious side effect of insulin therapy in patients with diabetes this paper will make a contribution to knowledge in the modeling and design of a non-invasive hypoglycemia monitor for patients with type 1 diabetes mellitus t1dm using a fuzzy-reasoning system</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136417</th>\n",
       "      <td>evolved fuzzy reasoning model for hypoglycaemic detection hypoglycaemia is a serious side effect of insulin therapy in patients with diabetes we measure physiological parameters heart rate, corrected qt interval of the electrocardiogram ecg signal continuously to provide early detection of hypoglycemic episodes in type 1 diabetes mellitus t1dm patients based on the physiological parameters, an evolved fuzzy reasoning model frm to recognize the presence of hypoglycaemic episodes is developed to optimize the fuzzy rules and the fuzzy membership functions of frm, an evolutionary algorithm called hybrid particle swarm optimization with wavelet mutation operation is investigated all data sets are collected from department of health, government of western australia for a clinical study the results show that the proposed algorithm performs well in terms of the clinical sensitivity and specificity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66252</th>\n",
       "      <td>classification of postprandial glycemic status with application to insulin dosing in type 1 diabetes-an in silico proof-of-concept in the daily management of type 1 diabetes t1d, determining the correct insulin dose to be injected at meal-time is fundamental to achieve optimal glycemic control wearable sensors, such as continuous glucose monitoring cgm devices, are instrumental to achieve this purpose in this paper, we show how cgm data, together with commonly recorded inputs carbohydrate intake and bolus insulin, can be used to develop an algorithm that allows classifying, at meal-time, the post-prandial glycemic status ie, blood glucose concentration being too low, too high, or within target range such an outcome can then be used to improve the efficacy of insulin therapy by reducing or increasing the corresponding meal bolus dose a state-of-the-art t1d simulation environment, including intraday variability and a behavioral model, was used to generate a rich in silico dataset corresponding to 100 subjects over a two-month scenario then, an extreme gradient-boosted tree xgb algorithm was employed to classify the post-prandial glycemic status finally, we demonstrate how the xgb algorithm outcome can be exploited to improve glycemic control in t1d through real-time adjustment of the meal insulin bolus the proposed xgb algorithm obtained good accuracy at classifying post-prandial glycemic status auroc = 084 078, 087 consequently, when used to adjust, in real-time, meal insulin boluses obtained with a bolus calculator, the proposed approach improves glycemic control when compared to the baseline bolus calculator in particular, percentage time in target 70, 180 mg/dl was improved from 6198 ± 1389 to 6700 ± 1154; p &lt; 001 without increasing hypoglycemia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8647</th>\n",
       "      <td>a multitask learning approach to personalised blood glucose prediction blood glucose prediction algorithms are key tools in the development of decision support systems and closed-loop insulin delivery systems for blood glucose control in diabetes deep learning models have provided leading results among machine learning algorithms to date in glucose prediction however these models typically require large amounts of data to obtain best personalised glucose prediction results multitask learning facilitates an approach for leveraging data from multiple subjects while still learning accurate personalised models in this work we present results comparing the effectiveness of multitask learning over sequential transfer learning, and learning only on subject-specific data with neural networks and support vector regression the multitask learning approach shows consistent leading performance in predictive metrics at both short-term and long-term prediction horizons we obtain a predictive accuracy rmse of 188 23, 253 29, 318 39, 412 45, 472 46 mg/dl at 30, 45, 60, 90, and 120 min prediction horizons respectively, with at least 93\\% clinically acceptable predictions using the clarke error grid ega at each prediction horizon we also identify relevant prior information such as glycaemic variability that can be incorporated to improve predictive performance at long-term prediction horizons furthermore, we demonstrate consistent performance - 5% change in both rmse and ega zone a - in rare cases of adverse glycaemic events with 1-6 weeks of training data in conclusion, a multitask approach can allow for deploying personalised models even with significantly less subject-specific data without compromising performance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70561</th>\n",
       "      <td>risk-based postprandial hypoglycemia forecasting using supervised learning predicting insulin-induced postprandial hypoglycemic events is critical for the safety of type 1 diabetes patients because an early warning of hypoglycemia facilitates correction of the insulin bolus before its administration the postprandial hypoglycemic event counts can be lowered by reducing the size of the bolus based on a reliable prediction but at the cost of increasing the average blood glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132523</th>\n",
       "      <td>real-time adaptive models for the personalized prediction of glycemic profile in type 1 diabetes patients prediction of glycemic profile is an important task for both early recognition of hypoglycemia and enhancement of the control algorithms for optimization of insulin infusion rate adaptive models for glucose prediction and recognition of hypoglycemia based on statistical and artificial intelligence techniques are presented</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125368</th>\n",
       "      <td>detection of correct and incorrect measurements in real-time continuous glucose monitoring systems by applying a postprocessing support vector machine support vector machines svms are an attractive option for detecting correct and incorrect measurements in real-time continuous glucose monitoring systems rtcgmss, because their learning mechanism can introduce a postprocessing strategy for imbalanced datasets the proposed svm considers the geometric mean to obtain a more balanced performance between sensitivity and specificity to test this approach, 23 critically ill patients receiving insulin therapy were monitored over 72 h using an rtcgms, and a dataset of 537 samples, classified according to international standards organization iso criteria 372 correct and 165 incorrect measurements, was obtained the results obtained were promising for patients with septic shock or with sepsis, for which the proposed system can be considered as reliable however, this approach cannot be considered suitable for patients without sepsis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98363</th>\n",
       "      <td>non-invasive blood glucose detection system based on conservation of energy method the most common method used for minimizing the occurrence of diabetes complications is frequent glucose testing to adjust the insulin dose however, using blood glucose bg meters presents a risk of infection it is of great importance to develop non-invasive bg detection techniques to realize high-accuracy, low-cost and continuous glucose monitoring, we have developed a non-invasive bg detection system using a mixed signal processor 430 msp430 microcontroller this method is based on the combination of the conservation-of-energy method with a sensor integration module, which collects physiological parameters, such as the blood oxygen saturation spo&lt;sub&gt;2&lt;/sub&gt;, blood flow velocity and heart rate new methods to detect the basal metabolic rate bmr and bv are proposed, which combine the human body heat balance and characteristic signals of photoplethysmography as well dual elastic chambers theory four hundred clinical trials on real-time non-invasive bg monitoring under suitable experiment conditions were performed on different individuals, including diabetic patients, senior citizens and healthy adults a multisensory information fusion model was applied to process these samples the algorithm we defined it as dcbpn algorithm applied in the model combines a decision tree and back propagation neural network, which classifies the physiological and environmental parameters into three categories, and then establishes a corresponding prediction model for the three categories the dcbpn algorithm provides an accuracy of 8853% in predicting the bg of new samples thus, this system demonstrates a great potential to reliably detect bg values in a non-invasive setting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49883</th>\n",
       "      <td>predicting nocturnal hypoglycemia from continuous glucose monitoring data with extended prediction horizon nocturnal hypoglycemia is a serious complication of insulin-treated diabetes, which commonly goes undetected continuous glucose monitoring cgm devices have enabled prediction of impending nocturnal hypoglycemia, however, prior efforts have been limited to a short prediction horizon ~ 30 minutes to this end, a nocturnal hypoglycemia prediction model with a 6-hour horizon midnight-6 am was developed using a random forest machine- learning model based on data from 10,000 users with more than 1 million nights of cgm data the model demonstrated an overall nighttime hypoglycemia prediction performance of roc auc = 084, with auc = 090 for early night midnight-3 am and auc = 075 for late night prediction at midnight, looking at 3-6 am window while instabilities and the absence of late-night blood glucose patterns introduce predictability challenges, this 6-hour horizon model demonstrates good performance in predicting nocturnal hypoglycemia additional study and specific patient-specific features will provide refinements that further ensure safe overnight management of glycemia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100248</th>\n",
       "      <td>ranking factors involved in diabetes remission after bariatric surgery using machine-learning integrating clinical and genomic biomarkers as weight-loss surgery is an effective treatment for the glycaemic control of type 2 diabetes in obese patients, yet not all patients benefit, it is valuable to find predictive factors for this diabetic remission this will help elucidating possible mechanistic insights and form the basis for prioritising obese patients with dysregulated diabetes for surgery where diabetes remission is of interest in this study, we combine both clinical and genomic factors using heuristic methods, informed by prior biological knowledge in order to rank factors that would have a role in predicting diabetes remission, and indeed in identifying patients who may have low likelihood in responding to bariatric surgery for improved glycaemic control genetic variants from the illumina cardiometabochip were prioritised through single-association tests and then seeded a larger selection from protein-protein interaction networks artificial neural networks allowing nonlinear correlations were trained to discriminate patients with and without surgery-induced diabetes remission, and the importance of each clinical and genetic parameter was evaluated the approach highlighted insulin treatment, baseline hba1c levels, use of insulin-sensitising agents and baseline serum insulin levels, as the most informative variables with a decent internal validation performance 74% accuracy, area under the curve auc 081 adding information for the eight top-ranked single nucleotide polymorphisms snps significantly boosted classification performance to 84% accuracy auc 092 the eight snps mapped to eight genes - &lt;i&gt;abca1, arhgef12, ctnnbl1, gli3, prok2, rybp, smug1&lt;/i&gt; and &lt;i&gt;stxbp5&lt;/i&gt; - three of which are known to have a role in insulin secretion, insulin sensitivity or obesity, but have not been indicated for diabetes remission after bariatric surgery before</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53157</th>\n",
       "      <td>use of a k-nearest neighbors model to predict the development of type 2 diabetes within 2 years in an obese, hypertensive population prediabetes is a type of hyperglycemia in which patients have blood glucose levels above normal but below the threshold for type 2 diabetes mellitus t2dm prediabetic patients are considered to be at high risk for developing t2dm, but not all will eventually do so because it is difficult to identify which patients have an increased risk of developing t2dm, we developed a model of several clinical and laboratory features to predict the development of t2dm within a 2-year period we used a supervised machine learning algorithm to identify at-risk patients from among 1647 obese, hypertensive patients the study period began in 2005 and ended in 2018 we constrained data up to 2 years before the development of t2dm then, using a time series analysis with the features of every patient, we calculated one linear regression line and one slope per feature features were then included in a k-nearest neighbors classification model feature importance was assessed using the random forest algorithm the k-nearest neighbors model accurately classified patients in 96% of cases, with a sensitivity of 99%, specificity of 78%, positive predictive value of 96%, and negative predictive value of 94% the random forest algorithm selected the homeostatic model assessment-estimated insulin resistance, insulin levels, and body mass index as the most important factors, which in combination with knn had an accuracy of 99% with a sensitivity of 99% and specificity of 97% we built a prognostic model that accurately identified obese, hypertensive patients at risk for developing t2dm within a 2-year period clinicians may use machine learning approaches to better assess risk for t2dm and better manage hypertensive patients machine learning algorithms may help health care providers make more informed decisions</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32618</th>\n",
       "      <td>application of artificial intelligence techniques for the estimation of basal insulin in patients with type i diabetes artificial intelligence techniques have been positioned in the resolution of problems in various areas of healthcare clinical decision support systems developed from this technology have optimized the healthcare of patients with chronic diseases through mobile applications in this study, several models based on this methodology have been developed to calculate the basal insulin dose in patients with type i diabetes using subcutaneous insulin infusion pumps &lt;i&gt;methods&lt;/i&gt; a pilot experimental study was performed with data from 56 patients with type 1 diabetes who used insulin infusion pumps and underwent continuous glucose monitoring several models based on artificial intelligence techniques were developed to analyze glycemic patterns based on continuous glucose monitoring and clinical variables in order to estimate the basal insulin dose we used neural networks nns, bayesian networks bns, support vector machines svms, and random forests rf we then evaluated the agreement between predicted and actual values using several statistical error measurements: mean absolute error mae, mean square error mse, root-mean-square error rmse, pearsons correlation coefficient &lt;i&gt;r&lt;/i&gt;, and determination coefficient &lt;i&gt;r&lt;/i&gt; &lt;sup&gt;2&lt;/sup&gt; &lt;i&gt;results&lt;/i&gt; twenty-four different models were obtained, one for each hour of the day, with each chosen technique correlation coefficients obtained with rf, svms, nns, and bns were 09999, 09921, 00303, and 07754, respectively the error increased between 06:00 and 07:00 and between 13:00 and 17:00 &lt;i&gt;conclusions&lt;/i&gt; the performance of the rf technique was excellent and got very close to the actual values intelligence techniques could be used to predict basal insulin dose however, it is necessary to explore the validity of the results and select the target population models that allow for more accurate levels of prediction should be further explored</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121145</th>\n",
       "      <td>combining genetic algorithm and levenberg-marquardt algorithm in training neural network for hypoglycemia detection using eeg signals hypoglycemia is the most common but highly feared complication induced by the intensive insulin therapy in patients with type 1 diabetes mellitus t1dm nocturnal hypoglycemia is dangerous because sleep obscures early symptoms and potentially leads to severe episodes which can cause seizure, coma, or even death it is shown that the hypoglycemia onset induces early changes in electroencephalography eeg signals which can be detected non-invasively in our research, eeg signals from five t1dm patients during an overnight clamp study were measured and analyzed by applying a method of feature extraction using fast fourier transform fft and classification using neural networks, we establish that hypoglycemia can be detected efficiently using eeg signals from only two channels this paper demonstrates that by implementing a training process of combining genetic algorithm and levenberg-marquardt algorithm, the classification results are improved markedly up to 75% sensitivity and 60% specificity on a separate testing set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "77216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          predicting and understanding the response to short-term intensive insulin therapy in people with early type 2 diabetes short-term intensive insulin therapy iit early in the course of type 2 diabetes acutely improves beta-cell function with long-lasting effects on glycemic control however, conventional measures cannot determine which patients are better suited for iit, and little is known about the molecular mechanisms determining response therefore, this study aimed to develop a model that could accurately predict the response to iit and provide insight into molecular mechanisms driving such response in humans   \n",
       "7760                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            hippocampal volume reduction is associated with direct measure of insulin resistance in adults hippocampal integrity is highly susceptible to metabolic dysfunction, yet its mechanisms are not well defined we studied 126 healthy individuals aged 23-61 years insulin resistance ir was quantified by measuring steady-state plasma glucose sspg concentration during the insulin suppression test body mass index bmi, adiposity, fasting insulin, glucose, leptin as well as structural neuroimaing with automatic hippocampal subfield segmentation were performed data analysis using unsupervised machine learning k-means clustering identified two subgroups reflecting a pattern of more pronounced hippocampal volume reduction being concurrently associated with greater adiposity and insulin resistance; the hippocampal volume reductions were uniform across subfields individuals in the most deviant subgroup were predominantly women 79 versus 42 % with higher bmi 279 25 versus 305 46 kg/m<sup>2</sup>, ir sspg concentration, 156 61 versus 123 70 mg/dl and leptinemia 217 170 versus 445 304 μg/l the use of person-based modeling in healthy individuals suggests that adiposity, insulin resistance and compromised structural hippocampal integrity behave as a composite phenotype; female sex emerged as risk factor for this phenotype   \n",
       "32600                                                                                                                                                                      a novel cgm metric-gradient and combining mean sensor glucose enable to improve the prediction of nocturnal hypoglycemic events in patients with diabetes nocturnal hypoglycemia is a serious complication of insulin-treated diabetes, and it is often asymptomatic a novel cgm metric-gradient was proposed in this paper, and a method of combining mean sensor glucose msg and gradient was presented for the prediction of nocturnal hypoglycemia for this purpose, the data from continuous glucose monitoring cgm encompassing 1,921 patients with diabetes were analyzed, and a total of 302 nocturnal hypoglycemic events were recorded the msg and gradient values were calculated, respectively, and then combined as a new metric <i>ie</i>, msg+gradient in addition, the prediction was conducted by four algorithms, namely, logistic regression, support vector machine, random forest, and long short-term memory the results revealed that the gradient of cgm showed a downward trend before hypoglycemic events happened additionally, the results indicated that the specificity and sensitivity based on the proposed method were better than the conventional metrics of low blood glucose index lbgi, coefficient of variation cv, mean absolute glucose mag, lability index li, <i>etc</i>, and the complex metrics of msg+lbgi, msg+cv, msg+mag, and msg+li, <i>etc</i> specifically, the specificity and sensitivity were greater than 9607% and 9603% at the prediction horizon of 15 minutes and greater than 8779% and 9007% at the prediction horizon of 30 minutes when the proposed method was adopted to predict nocturnal hypoglycemic events in the aforementioned four algorithms therefore, the proposed method of combining msg and gradient may enable to improve the prediction of nocturnal hypoglycemic events future studies are warranted to confirm the validity of this metric   \n",
       "168634                                                              a simple prediction rule and a neural network model to predict pancreatic beta-cell reserve in young adults with diabetes mellitus in the present study we developed and assessed the performance of a simple prediction rule and a neural network model to predict beta-cell reserve in young adults with diabetes eighty three young adults with diabetes were included in the study all were less than 40 years old and without apparent secondary causes of diabetes the subjects were randomly allocated to 2 groups; group 1 n = 59 for developing a prediction rule and training a neural network, group 2 n = 24 for validation purpose the prediction rule was developed by using stepwise logistic regression using stepwise logistic regression and modification of the derived equation, the patient would be insulin deficient if 3waist circumference in cm + 4age at diagnosis < 340 in the absence of previous diabetic ketoacidosis dka or < 400 in the presence of previous dka when tested in the validation set, the prediction rule had positive and negative predictive values of 867 per cent and 778 per cent respectively with 833 per cent accuracy while the ann model had a positive predictive value of 882 per cent and a negative predictive value of 100 per cent with 917 per cent accuracy when testing the performance of the prediction rule and the ann model compared to the assessment of 23 internists in a subgroup of 9 diabetics whose age at onset was less than 30 years and without a history of dka, the ann had the highest ability to predict beta-cell reserve accuracy = 889, followed by the prediction rule accuracy = 778% and assessments by internists accuracy = 609% we concluded that beta-cell reserve in young adults with diabetes mellitus could be predicted by a simple prediction rule or a neural network model the prediction rule and the neural network model can be helpful clinically in patients with mixed clinical features of type 1 and type 2 diabetes   \n",
       "65522                                                                                       glunet: a deep learning framework for accurate glucose forecasting for people with type 1 diabetes t1d, forecasting of blood glucose bg can be used to effectively avoid hyperglycemia, hypoglycemia and associated complications the latest continuous glucose monitoring cgm technology allows people to observe glucose in real-time however, an accurate glucose forecast remains a challenge in this work, we introduce glunet, a framework that leverages on a personalized deep neural network to predict the probabilistic distribution of short-term 30-60 minutes future cgm measurements for subjects with t1d based on their historical data including glucose measurements, meal information, insulin doses, and other factors it adopts the latest deep learning techniques consisting of four components: data pre-processing, label transform/recover, multi-layers of dilated convolution neural network cnn, and post-processing the method is evaluated in-silico for both adult and adolescent subjects the results show significant improvements over existing methods in the literature through a comprehensive comparison in terms of root mean square error rmse formula: see text mg/dl with short time lag formula: see text minutes for prediction horizons ph = 30 mins minutes, and rmse formula: see text mg/dl with time lag formula: see text mins for ph = 60 mins for virtual adult subjects in addition, glunet is also tested on two clinical data sets results show that it achieves an rmse formula: see text mg/dl with time lag formula: see text mins for ph = 30 mins and an rmse formula: see text mg/dl with time lag formula: see text mins for ph = 60 mins these are the best reported results for glucose forecasting when compared with other methods including the neural network for predicting glucose nnpg, the support vector regression svr, the latent variable with exogenous input lvx, and the auto regression with exogenous input arx algorithm   \n",
       "150895                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               neural network based glucose - insulin metabolism models for children with type 1 diabetes in this paper two models for the simulation of glucose-insulin metabolism of children with type 1 diabetes are presented the models are based on the combined use of compartmental models cms and artificial neural networks nns data from children with type 1 diabetes, stored in a database, have been used as input to the models the data are taken from four children with type 1 diabetes and contain information about glucose levels taken from continuous glucose monitoring system, insulin intake and food intake, along with corresponding time the influences of taken insulin on plasma insulin concentration, as well as the effect of food intake on glucose input into the blood from the gut, are estimated from the cms the outputs of cms, along with previous glucose measurements, are fed to a nn, which provides short-term prediction of glucose values for comparative reasons two different nn architectures have been tested: a feed-forward nn ffnn trained with the back-propagation algorithm with adaptive learning rate and momentum, and a recurrent nn rnn, trained with the real time recurrent learning rtrl algorithm the results indicate that the best prediction performance can be achieved by the use of rnn   \n",
       "68229                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 prediction and prevention of hypoglycaemic events in type-1 diabetic patients using machine learning tight blood glucose control reduces the risk of microvascular and macrovascular complications in patients with type 1 diabetes however, this is very difficult due to the large intra-individual variability and other factors that affect glycaemic control the main limiting factor to achieve strict control of glucose levels in patients on intensive insulin therapy is the risk of severe hypoglycaemia therefore, hypoglycaemia is the main safety problem in the treatment of type 1 diabetes, negatively affecting the quality of life of patients suffering from this disease decision support tools based on machine learning methods have become a viable way to enhance patient safety by anticipating adverse glycaemic events this study proposes the application of four machine learning algorithms to tackle the problem of safety in diabetes management: 1 grammatical evolution for the mid-term continuous prediction of blood glucose levels, 2 support vector machines to predict hypoglycaemic events during postprandial periods, 3 artificial neural networks to predict hypoglycaemic episodes overnight, and 4 data mining to profile diabetes management scenarios the proposal consists of the combination of prediction and classification capabilities of the implemented approaches the resulting system significantly reduces the number of episodes of hypoglycaemia, improving safety and providing patients with greater confidence in decision-making   \n",
       "128784                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              natural occurrence of nocturnal hypoglycemia detection using hybrid particle swarm optimized fuzzy reasoning model low blood glucose hypoglycemia is a common and serious side effect of insulin therapy in patients with diabetes this paper will make a contribution to knowledge in the modeling and design of a non-invasive hypoglycemia monitor for patients with type 1 diabetes mellitus t1dm using a fuzzy-reasoning system   \n",
       "136417                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            evolved fuzzy reasoning model for hypoglycaemic detection hypoglycaemia is a serious side effect of insulin therapy in patients with diabetes we measure physiological parameters heart rate, corrected qt interval of the electrocardiogram ecg signal continuously to provide early detection of hypoglycemic episodes in type 1 diabetes mellitus t1dm patients based on the physiological parameters, an evolved fuzzy reasoning model frm to recognize the presence of hypoglycaemic episodes is developed to optimize the fuzzy rules and the fuzzy membership functions of frm, an evolutionary algorithm called hybrid particle swarm optimization with wavelet mutation operation is investigated all data sets are collected from department of health, government of western australia for a clinical study the results show that the proposed algorithm performs well in terms of the clinical sensitivity and specificity   \n",
       "66252                                                                                                                                                                                                                                                  classification of postprandial glycemic status with application to insulin dosing in type 1 diabetes-an in silico proof-of-concept in the daily management of type 1 diabetes t1d, determining the correct insulin dose to be injected at meal-time is fundamental to achieve optimal glycemic control wearable sensors, such as continuous glucose monitoring cgm devices, are instrumental to achieve this purpose in this paper, we show how cgm data, together with commonly recorded inputs carbohydrate intake and bolus insulin, can be used to develop an algorithm that allows classifying, at meal-time, the post-prandial glycemic status ie, blood glucose concentration being too low, too high, or within target range such an outcome can then be used to improve the efficacy of insulin therapy by reducing or increasing the corresponding meal bolus dose a state-of-the-art t1d simulation environment, including intraday variability and a behavioral model, was used to generate a rich in silico dataset corresponding to 100 subjects over a two-month scenario then, an extreme gradient-boosted tree xgb algorithm was employed to classify the post-prandial glycemic status finally, we demonstrate how the xgb algorithm outcome can be exploited to improve glycemic control in t1d through real-time adjustment of the meal insulin bolus the proposed xgb algorithm obtained good accuracy at classifying post-prandial glycemic status auroc = 084 078, 087 consequently, when used to adjust, in real-time, meal insulin boluses obtained with a bolus calculator, the proposed approach improves glycemic control when compared to the baseline bolus calculator in particular, percentage time in target 70, 180 mg/dl was improved from 6198 ± 1389 to 6700 ± 1154; p < 001 without increasing hypoglycemia   \n",
       "8647                                                                                                                                                                                                                                                                                                     a multitask learning approach to personalised blood glucose prediction blood glucose prediction algorithms are key tools in the development of decision support systems and closed-loop insulin delivery systems for blood glucose control in diabetes deep learning models have provided leading results among machine learning algorithms to date in glucose prediction however these models typically require large amounts of data to obtain best personalised glucose prediction results multitask learning facilitates an approach for leveraging data from multiple subjects while still learning accurate personalised models in this work we present results comparing the effectiveness of multitask learning over sequential transfer learning, and learning only on subject-specific data with neural networks and support vector regression the multitask learning approach shows consistent leading performance in predictive metrics at both short-term and long-term prediction horizons we obtain a predictive accuracy rmse of 188 23, 253 29, 318 39, 412 45, 472 46 mg/dl at 30, 45, 60, 90, and 120 min prediction horizons respectively, with at least 93\\% clinically acceptable predictions using the clarke error grid ega at each prediction horizon we also identify relevant prior information such as glycaemic variability that can be incorporated to improve predictive performance at long-term prediction horizons furthermore, we demonstrate consistent performance - 5% change in both rmse and ega zone a - in rare cases of adverse glycaemic events with 1-6 weeks of training data in conclusion, a multitask approach can allow for deploying personalised models even with significantly less subject-specific data without compromising performance   \n",
       "70561                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    risk-based postprandial hypoglycemia forecasting using supervised learning predicting insulin-induced postprandial hypoglycemic events is critical for the safety of type 1 diabetes patients because an early warning of hypoglycemia facilitates correction of the insulin bolus before its administration the postprandial hypoglycemic event counts can be lowered by reducing the size of the bolus based on a reliable prediction but at the cost of increasing the average blood glucose   \n",
       "132523                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     real-time adaptive models for the personalized prediction of glycemic profile in type 1 diabetes patients prediction of glycemic profile is an important task for both early recognition of hypoglycemia and enhancement of the control algorithms for optimization of insulin infusion rate adaptive models for glucose prediction and recognition of hypoglycemia based on statistical and artificial intelligence techniques are presented   \n",
       "125368                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        detection of correct and incorrect measurements in real-time continuous glucose monitoring systems by applying a postprocessing support vector machine support vector machines svms are an attractive option for detecting correct and incorrect measurements in real-time continuous glucose monitoring systems rtcgmss, because their learning mechanism can introduce a postprocessing strategy for imbalanced datasets the proposed svm considers the geometric mean to obtain a more balanced performance between sensitivity and specificity to test this approach, 23 critically ill patients receiving insulin therapy were monitored over 72 h using an rtcgms, and a dataset of 537 samples, classified according to international standards organization iso criteria 372 correct and 165 incorrect measurements, was obtained the results obtained were promising for patients with septic shock or with sepsis, for which the proposed system can be considered as reliable however, this approach cannot be considered suitable for patients without sepsis    \n",
       "98363                                                                                                                                                                                                                                                                   non-invasive blood glucose detection system based on conservation of energy method the most common method used for minimizing the occurrence of diabetes complications is frequent glucose testing to adjust the insulin dose however, using blood glucose bg meters presents a risk of infection it is of great importance to develop non-invasive bg detection techniques to realize high-accuracy, low-cost and continuous glucose monitoring, we have developed a non-invasive bg detection system using a mixed signal processor 430 msp430 microcontroller this method is based on the combination of the conservation-of-energy method with a sensor integration module, which collects physiological parameters, such as the blood oxygen saturation spo<sub>2</sub>, blood flow velocity and heart rate new methods to detect the basal metabolic rate bmr and bv are proposed, which combine the human body heat balance and characteristic signals of photoplethysmography as well dual elastic chambers theory four hundred clinical trials on real-time non-invasive bg monitoring under suitable experiment conditions were performed on different individuals, including diabetic patients, senior citizens and healthy adults a multisensory information fusion model was applied to process these samples the algorithm we defined it as dcbpn algorithm applied in the model combines a decision tree and back propagation neural network, which classifies the physiological and environmental parameters into three categories, and then establishes a corresponding prediction model for the three categories the dcbpn algorithm provides an accuracy of 8853% in predicting the bg of new samples thus, this system demonstrates a great potential to reliably detect bg values in a non-invasive setting   \n",
       "49883                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           predicting nocturnal hypoglycemia from continuous glucose monitoring data with extended prediction horizon nocturnal hypoglycemia is a serious complication of insulin-treated diabetes, which commonly goes undetected continuous glucose monitoring cgm devices have enabled prediction of impending nocturnal hypoglycemia, however, prior efforts have been limited to a short prediction horizon ~ 30 minutes to this end, a nocturnal hypoglycemia prediction model with a 6-hour horizon midnight-6 am was developed using a random forest machine- learning model based on data from 10,000 users with more than 1 million nights of cgm data the model demonstrated an overall nighttime hypoglycemia prediction performance of roc auc = 084, with auc = 090 for early night midnight-3 am and auc = 075 for late night prediction at midnight, looking at 3-6 am window while instabilities and the absence of late-night blood glucose patterns introduce predictability challenges, this 6-hour horizon model demonstrates good performance in predicting nocturnal hypoglycemia additional study and specific patient-specific features will provide refinements that further ensure safe overnight management of glycemia   \n",
       "100248                                       ranking factors involved in diabetes remission after bariatric surgery using machine-learning integrating clinical and genomic biomarkers as weight-loss surgery is an effective treatment for the glycaemic control of type 2 diabetes in obese patients, yet not all patients benefit, it is valuable to find predictive factors for this diabetic remission this will help elucidating possible mechanistic insights and form the basis for prioritising obese patients with dysregulated diabetes for surgery where diabetes remission is of interest in this study, we combine both clinical and genomic factors using heuristic methods, informed by prior biological knowledge in order to rank factors that would have a role in predicting diabetes remission, and indeed in identifying patients who may have low likelihood in responding to bariatric surgery for improved glycaemic control genetic variants from the illumina cardiometabochip were prioritised through single-association tests and then seeded a larger selection from protein-protein interaction networks artificial neural networks allowing nonlinear correlations were trained to discriminate patients with and without surgery-induced diabetes remission, and the importance of each clinical and genetic parameter was evaluated the approach highlighted insulin treatment, baseline hba1c levels, use of insulin-sensitising agents and baseline serum insulin levels, as the most informative variables with a decent internal validation performance 74% accuracy, area under the curve auc 081 adding information for the eight top-ranked single nucleotide polymorphisms snps significantly boosted classification performance to 84% accuracy auc 092 the eight snps mapped to eight genes - <i>abca1, arhgef12, ctnnbl1, gli3, prok2, rybp, smug1</i> and <i>stxbp5</i> - three of which are known to have a role in insulin secretion, insulin sensitivity or obesity, but have not been indicated for diabetes remission after bariatric surgery before   \n",
       "53157                                                                                      use of a k-nearest neighbors model to predict the development of type 2 diabetes within 2 years in an obese, hypertensive population prediabetes is a type of hyperglycemia in which patients have blood glucose levels above normal but below the threshold for type 2 diabetes mellitus t2dm prediabetic patients are considered to be at high risk for developing t2dm, but not all will eventually do so because it is difficult to identify which patients have an increased risk of developing t2dm, we developed a model of several clinical and laboratory features to predict the development of t2dm within a 2-year period we used a supervised machine learning algorithm to identify at-risk patients from among 1647 obese, hypertensive patients the study period began in 2005 and ended in 2018 we constrained data up to 2 years before the development of t2dm then, using a time series analysis with the features of every patient, we calculated one linear regression line and one slope per feature features were then included in a k-nearest neighbors classification model feature importance was assessed using the random forest algorithm the k-nearest neighbors model accurately classified patients in 96% of cases, with a sensitivity of 99%, specificity of 78%, positive predictive value of 96%, and negative predictive value of 94% the random forest algorithm selected the homeostatic model assessment-estimated insulin resistance, insulin levels, and body mass index as the most important factors, which in combination with knn had an accuracy of 99% with a sensitivity of 99% and specificity of 97% we built a prognostic model that accurately identified obese, hypertensive patients at risk for developing t2dm within a 2-year period clinicians may use machine learning approaches to better assess risk for t2dm and better manage hypertensive patients machine learning algorithms may help health care providers make more informed decisions   \n",
       "32618   application of artificial intelligence techniques for the estimation of basal insulin in patients with type i diabetes artificial intelligence techniques have been positioned in the resolution of problems in various areas of healthcare clinical decision support systems developed from this technology have optimized the healthcare of patients with chronic diseases through mobile applications in this study, several models based on this methodology have been developed to calculate the basal insulin dose in patients with type i diabetes using subcutaneous insulin infusion pumps <i>methods</i> a pilot experimental study was performed with data from 56 patients with type 1 diabetes who used insulin infusion pumps and underwent continuous glucose monitoring several models based on artificial intelligence techniques were developed to analyze glycemic patterns based on continuous glucose monitoring and clinical variables in order to estimate the basal insulin dose we used neural networks nns, bayesian networks bns, support vector machines svms, and random forests rf we then evaluated the agreement between predicted and actual values using several statistical error measurements: mean absolute error mae, mean square error mse, root-mean-square error rmse, pearsons correlation coefficient <i>r</i>, and determination coefficient <i>r</i> <sup>2</sup> <i>results</i> twenty-four different models were obtained, one for each hour of the day, with each chosen technique correlation coefficients obtained with rf, svms, nns, and bns were 09999, 09921, 00303, and 07754, respectively the error increased between 06:00 and 07:00 and between 13:00 and 17:00 <i>conclusions</i> the performance of the rf technique was excellent and got very close to the actual values intelligence techniques could be used to predict basal insulin dose however, it is necessary to explore the validity of the results and select the target population models that allow for more accurate levels of prediction should be further explored   \n",
       "121145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           combining genetic algorithm and levenberg-marquardt algorithm in training neural network for hypoglycemia detection using eeg signals hypoglycemia is the most common but highly feared complication induced by the intensive insulin therapy in patients with type 1 diabetes mellitus t1dm nocturnal hypoglycemia is dangerous because sleep obscures early symptoms and potentially leads to severe episodes which can cause seizure, coma, or even death it is shown that the hypoglycemia onset induces early changes in electroencephalography eeg signals which can be detected non-invasively in our research, eeg signals from five t1dm patients during an overnight clamp study were measured and analyzed by applying a method of feature extraction using fast fourier transform fft and classification using neural networks, we establish that hypoglycemia can be detected efficiently using eeg signals from only two channels this paper demonstrates that by implementing a training process of combining genetic algorithm and levenberg-marquardt algorithm, the classification results are improved markedly up to 75% sensitivity and 60% specificity on a separate testing set    \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "77216         0       0       0           0          0        0       0   \n",
       "7760          0       0       0           0          0        0       0   \n",
       "32600         0       0       0           0          0        0       0   \n",
       "168634        0       0       0           0          0        0       0   \n",
       "65522         0       0       0           0          0        0       0   \n",
       "150895        0       0       0           0          0        0       0   \n",
       "68229         0       0       0           0          0        0       0   \n",
       "128784        0       0       0           0          0        0       0   \n",
       "136417        0       0       0           0          0        0       0   \n",
       "66252         0       0       0           0          0        0       0   \n",
       "8647          0       0       0           0          0        0       0   \n",
       "70561         0       0       0           0          0        0       0   \n",
       "132523        0       0       0           0          0        0       0   \n",
       "125368        0       0       1           1          0        0       0   \n",
       "98363         0       0       1           0          0        0       0   \n",
       "49883         0       0       0           0          0        0       0   \n",
       "100248        0       0       0           0          0        0       0   \n",
       "53157         0       0       0           0          0        0       0   \n",
       "32618         0       0       0           0          0        0       0   \n",
       "121145        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "77216            0            0         0           0        0       0   \n",
       "7760             0            0         0           0        0       0   \n",
       "32600            0            0         0           0        0       0   \n",
       "168634           0            0         0           0        0       0   \n",
       "65522            0            0         0           0        0       0   \n",
       "150895           0            0         0           0        0       0   \n",
       "68229            0            0         0           0        0       0   \n",
       "128784           0            0         0           0        0       0   \n",
       "136417           0            0         0           0        0       0   \n",
       "66252            0            0         0           0        0       0   \n",
       "8647             0            0         0           0        0       0   \n",
       "70561            0            0         0           0        0       0   \n",
       "132523           0            0         0           0        0       0   \n",
       "125368           0            0         0           0        0       0   \n",
       "98363            0            0         0           0        0       0   \n",
       "49883            0            0         0           0        0       0   \n",
       "100248           0            0         0           0        0       0   \n",
       "53157            0            0         0           0        0       0   \n",
       "32618            0            0         1           0        0       0   \n",
       "121145           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "77216            0             0           0            0         0   \n",
       "7760             0             0           0            0         0   \n",
       "32600            0             0           0            0         0   \n",
       "168634           0             0           0            0         0   \n",
       "65522            0             0           0            0         0   \n",
       "150895           0             0           0            0         0   \n",
       "68229            0             0           0            0         0   \n",
       "128784           0             0           0            0         0   \n",
       "136417           0             0           0            0         0   \n",
       "66252            0             0           0            0         0   \n",
       "8647             0             0           0            0         0   \n",
       "70561            0             0           0            0         0   \n",
       "132523           0             0           0            0         0   \n",
       "125368           0             0           0            0         0   \n",
       "98363            0             0           0            0         0   \n",
       "49883            0             0           0            0         0   \n",
       "100248           0             0           0            0         0   \n",
       "53157            0             0           0            0         0   \n",
       "32618            0             0           0            0         0   \n",
       "121145           0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "77216           0            0           0            0           0   \n",
       "7760            0            0           0            0           0   \n",
       "32600           0            0           0            0           0   \n",
       "168634          0            0           0            0           0   \n",
       "65522           0            0           0            0           0   \n",
       "150895          0            0           0            0           0   \n",
       "68229           0            0           0            0           0   \n",
       "128784          0            0           0            0           0   \n",
       "136417          0            0           0            0           0   \n",
       "66252           0            0           0            0           0   \n",
       "8647            0            0           0            0           0   \n",
       "70561           0            0           0            0           0   \n",
       "132523          0            0           0            0           0   \n",
       "125368          0            0           0            0           0   \n",
       "98363           0            0           0            0           0   \n",
       "49883           0            0           0            0           0   \n",
       "100248          0            0           0            0           0   \n",
       "53157           0            0           0            0           0   \n",
       "32618           0            0           0            0           0   \n",
       "121145          0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "77216             0          0            0        0         0          0   \n",
       "7760              0          0            0        0         0          0   \n",
       "32600             0          0            0        0         0          0   \n",
       "168634            0          0            0        0         0          0   \n",
       "65522             0          0            0        0         0          0   \n",
       "150895            0          0            0        0         0          0   \n",
       "68229             0          0            0        0         0          0   \n",
       "128784            0          0            0        0         0          0   \n",
       "136417            0          0            0        0         0          0   \n",
       "66252             0          0            0        0         0          0   \n",
       "8647              0          0            0        0         0          0   \n",
       "70561             0          0            0        0         0          0   \n",
       "132523            0          0            0        0         0          0   \n",
       "125368            0          0            0        0         0          0   \n",
       "98363             0          0            0        0         0          0   \n",
       "49883             0          0            0        0         0          0   \n",
       "100248            0          0            0        0         0          0   \n",
       "53157             0          0            0        0         0          0   \n",
       "32618             0          0            0        0         0          0   \n",
       "121145            0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "77216        0        0         0          0        0       0         0   \n",
       "7760         0        0         0          0        0       0         0   \n",
       "32600        0        0         0          0        0       0         0   \n",
       "168634       0        1         0          0        0       0         0   \n",
       "65522        0        0         0          0        0       0         0   \n",
       "150895       0        0         0          0        0       0         0   \n",
       "68229        0        0         0          0        0       0         0   \n",
       "128784       0        0         0          0        0       0         0   \n",
       "136417       0        0         0          0        0       0         0   \n",
       "66252        0        0         0          0        0       0         0   \n",
       "8647         0        0         0          0        0       0         0   \n",
       "70561        0        0         0          0        0       0         0   \n",
       "132523       0        0         0          0        0       0         0   \n",
       "125368       0        0         0          0        0       0         0   \n",
       "98363        0        0         0          0        0       0         0   \n",
       "49883        0        0         0          0        0       0         0   \n",
       "100248       0        0         0          0        0       0         0   \n",
       "53157        0        0         0          0        0       0         0   \n",
       "32618        0        0         0          0        0       0         0   \n",
       "121145       0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "77216           0        0           0         0        0        0       0   \n",
       "7760            1        0           0         0        0        0       0   \n",
       "32600           0        0           0         0        0        0       0   \n",
       "168634          0        0           0         0        0        0       0   \n",
       "65522           0        0           0         0        0        0       0   \n",
       "150895          0        0           0         0        0        0       0   \n",
       "68229           0        0           0         0        0        0       0   \n",
       "128784          0        0           0         0        0        0       0   \n",
       "136417          0        0           0         0        1        0       0   \n",
       "66252           0        0           0         0        0        0       0   \n",
       "8647            0        0           0         0        0        0       0   \n",
       "70561           0        0           0         0        0        0       0   \n",
       "132523          0        0           0         0        0        0       0   \n",
       "125368          0        0           0         0        0        0       0   \n",
       "98363           0        0           0         0        0        0       0   \n",
       "49883           0        0           0         0        0        0       0   \n",
       "100248          0        0           0         0        0        0       0   \n",
       "53157           0        0           0         0        0        0       0   \n",
       "32618           0        0           0         0        0        0       0   \n",
       "121145          0        0           1         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text  \n",
       "77216            0         0       1            1  \n",
       "7760             0         0       1            1  \n",
       "32600            0         0       1            1  \n",
       "168634           0         0       1            1  \n",
       "65522            0         0       1            1  \n",
       "150895           0         0       1            1  \n",
       "68229            0         0       1            1  \n",
       "128784           0         0       1            1  \n",
       "136417           1         0       1            1  \n",
       "66252            0         0       1            1  \n",
       "8647             0         0       1            1  \n",
       "70561            0         0       1            1  \n",
       "132523           0         0       1            1  \n",
       "125368           0         0       1            1  \n",
       "98363            0         0       1            1  \n",
       "49883            0         0       1            1  \n",
       "100248           0         0       1            1  \n",
       "53157            0         0       1            1  \n",
       "32618            0         0       1            1  \n",
       "121145           0         0       1            1  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['insulin_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a3ff1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33794, '1': 385})\n"
     ]
    }
   ],
   "source": [
    "#### DM RETINOPATHIES / retina\n",
    "\n",
    "## text\n",
    "spec['retina_text'] = np.where(groups['text'].str.contains('diabetic retin'), \"1\", \"0\")\n",
    "\n",
    "spec['retina_text'] = np.where((groups['text'].str.contains(\"diabet\")) &\n",
    "                             (groups['text'].str.contains(\"retina\")) , \"1\", spec['retina_text'])\n",
    "spec['retina_text'] = np.where((groups['text'].str.contains(\"diabet\")) &\n",
    "                             (groups['text'].str.contains(\"retino\")) , \"1\", spec['retina_text'])\n",
    "spec['retina_text'] = np.where((groups['text'].str.contains(\"diabet\")) &\n",
    "                             (groups['text'].str.contains(\"eye\")) , \"1\", spec['retina_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['retina_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f408a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80853</th>\n",
       "      <td>feature selection and parameters optimization of support vector machines based on hybrid glowworm swarm optimization for classification of diabetic retinopathy diabetic retinopathy dr has been a leading cause of blindness in case of human beings falling between the ages of 20 and 74 years this will have a major influence on both the patient and the society as it can normally influence the humans in their gainful years an early dr detection is quite challenging as it may not be detected by humans there are several techniques and algorithms that have been established for detecting the dr these techniques have been facing problems to achieve effective sensitivity, accuracy, and specificity in order to overcome all these problems, the work has proposed one more such effective algorithm for image processing in order to increase the efficiency and also identify easily the dr diseases a major challenge in the task is the automatic detection of the microaneurysms in this work, the support vector machine svm parameters optimized with glowworm swarm optimization gso and genetic algorithm ga is used to classify the dr because the svm parameter c and γ to control the performance of the classifier for this work, the svms get fused with the hybrid gso-ga along with the feature chromosomes that are generated that will thereby direct the ga search to a straight line of the error of optimal generalization in their super parameter space this gso algorithm will not have memory and the glow worms will not retain any information in memory the results of the experiment prove that this method had achieved a better performance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92430</th>\n",
       "      <td>an automated system for the detection and classification of retinal changes due to red lesions in longitudinal fundus images people with diabetes mellitus need annual screening to check for the development of diabetic retinopathy dr tracking small retinal changes due to early diabetic retinopathy lesions in longitudinal fundus image sets is challenging due to intra- and intervisit variability in illumination and image quality, the required high registration accuracy, and the subtle appearance of retinal lesions compared to other retinal features this paper presents a robust and flexible approach for automated detection of longitudinal retinal changes due to small red lesions by exploiting normalized fundus images that significantly reduce illumination variations and improve the contrast of small retinal features to detect spatio-temporal retinal changes, the absolute difference between the extremes of the multiscale blobness responses of fundus images from two time points is proposed as a simple and effective blobness measure dr related changes are then identified based on several intensity and shape features by a support vector machine classifier the proposed approach was evaluated in the context of a regular diabetic retinopathy screening program involving subjects ranging from healthy no retinal lesion to moderate with clinically relevant retinal lesions dr levels evaluation shows that the system is able to detect retinal changes due to small red lesions with a sensitivity of at an average false positive rate of 1 and 25 lesions per eye on small and large fields-of-view of the retina, respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29064</th>\n",
       "      <td>using artificial intelligence as an initial triage strategy in diabetic retinopathy screening program in china &lt;b&gt;objective:&lt;/b&gt; to investigate the diagnostic accuracy and efficiency of an artificial intelligence ai triaging model in a diabetic retinopathy dr screening program &lt;b&gt;methods:&lt;/b&gt; a dr screening program was conducted in kashi city and kizilsu kirghiz autonomous prefecture of the xinjiang uyur autonomous region from may to july 2018, and 8 005 patients with diabetes mellitus were included fundus images, one centered at optic disc and one centered at macula, were taken for both eyes a previously validated ai algorithm was applied as the first step to identify the patients with all 4 images if the images were classified as gradable and negative dr, an ai-generated report was immediately provided without sending to manual grading, and 1/3 of these patients were randomly sampled for manual grading and quality control group a for the patients with at least one image classified as ungradable or positive for any dr, all images were sent for manual grading group b finally, 300 patients were randomly selected from group a and group b respectively for accuracy assessment, where the patients and their images were classified by a specialist panel for referral dr pre-proliferative dr, or proliferative dr, and/or diabetic macular edema &lt;b&gt;results:&lt;/b&gt; among 8 005 patients for dr screening including 3 220 males and 4 785 females, aged 583±106 years, after ai triaging, 5 267 658% potentially received reports from ai system and 2 738 342% required manual grading in group a, the accuracy and specificity of ai classification and manual grading on referral dr were all 100% in group b, the accuracy of ai and manual grading were 758% and 903%, respectively, while the sensitivity of ai and manual grading was 100% and 791%, respectively &lt;b&gt;conclusion:&lt;/b&gt; ai alleviates 60% of the workload of manual grading without missing any referral patients with the aid of the current ai triaging model</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34564</th>\n",
       "      <td>hybrid model structure for diabetic retinopathy classification diabetic retinopathy dr is one of the most common complications of diabetes and the main cause of blindness the progression of the disease can be prevented by early diagnosis of dr due to differences in the distribution of medical conditions and low labor efficiency, the best time for diagnosis and treatment was missed, which results in impaired vision using neural network models to classify and diagnose dr can improve efficiency and reduce costs in this work, an improved loss function and three hybrid model structures hybrid-a, hybrid-f, and hybrid-c were proposed to improve the performance of dr classification models efficientnetb4, efficientnetb5, nasnetlarge, xception, and inceptionresnetv2 cnns were chosen as the basic models these basic models were trained using enhance cross-entropy loss and cross-entropy loss, respectively the output of the basic models was used to train the hybrid model structures experiments showed that enhance cross-entropy loss can effectively accelerate the training process of the basic models and improve the performance of the models under various evaluation metrics the proposed hybrid model structures can also improve dr classification performance compared with the best-performing results in the basic models, the accuracy of dr classification was improved from 8544% to 8634%, the sensitivity was improved from 9848% to 9877%, the specificity was improved from 7182% to 7476%, the precision was improved from 9027% to 9137%, and the f1 score was improved from 9362% to 939% by using hybrid model structures</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90440</th>\n",
       "      <td>deep convolutional neural network-based early automated detection of diabetic retinopathy using fundus image the automatic detection of diabetic retinopathy is of vital importance, as it is the main cause of irreversible vision loss in the working-age population in the developed world the early detection of diabetic retinopathy occurrence can be very helpful for clinical treatment; although several different feature extraction approaches have been proposed, the classification task for retinal images is still tedious even for those trained clinicians recently, deep convolutional neural networks have manifested superior performance in image classification compared to previous handcrafted feature-based image classification methods thus, in this paper, we explored the use of deep convolutional neural network methodology for the automatic classification of diabetic retinopathy using color fundus image, and obtained an accuracy of 945% on our dataset, outperforming the results obtained by using classical approaches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84398</th>\n",
       "      <td>automated detection of diabetic retinopathy using deep learning diabetic retinopathy is a leading cause of blindness among working-age adults early detection of this condition is critical for good prognosis in this paper, we demonstrate the use of convolutional neural networks cnns on color fundus images for the recognition task of diabetic retinopathy staging our network models achieved test metric performance comparable to baseline literature results, with validation sensitivity of 95% we additionally explored multinomial classification models, and demonstrate that errors primarily occur in the misclassification of mild disease as normal due to the cnns inability to detect subtle disease features we discovered that preprocessing with contrast limited adaptive histogram equalization and ensuring dataset fidelity by expert verification of class labels improves recognition of subtle features transfer learning on pretrained googlenet and alexnet models from imagenet improved peak test set accuracies to 745%, 688%, and 572% on 2-ary, 3-ary, and 4-ary classification models, respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74763</th>\n",
       "      <td>artificial intelligence for the detection of diabetic retinopathy in primary care: protocol for algorithm development diabetic retinopathy dr is one of the most important causes of blindness worldwide, especially in developed countries in diabetic patients, periodic examination of the back of the eye using a nonmydriatic camera has been widely demonstrated to be an effective system to control and prevent the onset of dr convolutional neural networks have been used to detect dr, achieving very high sensitivities and specificities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156702</th>\n",
       "      <td>robust detection and classification of longitudinal changes in color retinal fundus images for monitoring diabetic retinopathy a fully automated approach is presented for robust detection and classification of changes in longitudinal time-series of color retinal fundus images of diabetic retinopathy the method is robust to: 1 spatial variations in illumination resulting from instrument limitations and changes both within, and between patient visits; 2 imaging artifacts such as dust particles; 3 outliers in the training data; 4 segmentation and alignment errors robustness to illumination variation is achieved by a novel iterative algorithm to estimate the reflectance of the retina exploiting automatically extracted segmentations of the retinal vasculature, optic disk, fovea, and pathologies robustness to dust artifacts is achieved by exploiting their spectral characteristics, enabling application to film-based, as well as digital imaging systems false changes from alignment errors are minimized by subpixel accuracy registration using a 12-parameter transformation that accounts for unknown retinal curvature and camera parameters bayesian detection and classification algorithms are used to generate a color-coded output that is readily inspected a multiobserver validation on 43 image pairs from 22 eyes involving nonproliferative and proliferative diabetic retinopathies, showed a 97% change detection rate, a 3% miss rate, and a 10% false alarm rate the performance in correctly classifying the changes was 993% a self-consistency metric, and an error factor were developed to measure performance over more than two periods the average self consistency was 94% and the error factor was 006% although this study focuses on diabetic changes, the proposed techniques have broader applicability in ophthalmology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37508</th>\n",
       "      <td>dcardnet: diabetic retinopathy classification at multiple levels based on structural and angiographic optical coherence tomography optical coherence tomography oct and its angiography octa have several advantages for the early detection and diagnosis of diabetic retinopathy dr however, automated, complete dr classification frameworks based on both oct and octa data have not been proposed in this study, a convolutional neural network cnn based method is proposed to fulfill a dr classification framework using en face oct and octa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51248</th>\n",
       "      <td>automated quantification of photoreceptor alteration in macular disease using optical coherence tomography and deep learning diabetic macular edema dme and retina vein occlusion rvo are macular diseases in which central photoreceptors are affected due to pathological accumulation of fluid optical coherence tomography allows to visually assess and evaluate photoreceptor integrity, whose alteration has been observed as an important biomarker of both diseases however, the manual quantification of this layered structure is challenging, tedious and time-consuming in this paper we introduce a deep learning approach for automatically segmenting and characterising photoreceptor alteration the photoreceptor layer is segmented using an ensemble of four different convolutional neural networks en-face representations of the layer thickness are produced to characterize the photoreceptors the pixel-wise standard deviation of the score maps produced by the individual models is also taken to indicate areas of photoreceptor abnormality or ambiguous results experimental results showed that our ensemble is able to produce results in pair with a human expert, outperforming each of its constitutive models no statistically significant differences were observed between mean thickness estimates obtained from automated and manually generated annotations therefore, our model is able to reliable quantify photoreceptors, which can be used to improve prognosis and managment of macular diseases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146565</th>\n",
       "      <td>optimal wavelet transform for the detection of microaneurysms in retina photographs in this paper, we propose an automatic method to detect microaneurysms in retina photographs microaneurysms are the most frequent and usually the first lesions to appear as a consequence of diabetic retinopathy so, their detection is necessary for both screening the pathology and follow up progression measurement automating this task, which is currently performed manually, would bring more objectivity and reproducibility we propose to detect them by locally matching a lesion template in subbands of wavelet transformed images to improve the method performance, we have searched for the best adapted wavelet within the lifting scheme framework the optimization process is based on a genetic algorithm followed by powells direction set descent results are evaluated on 120 retinal images analyzed by an expert and the optimal wavelet is compared to different conventional mother wavelets these images are of three different modalities: there are color photographs, green filtered photographs, and angiographs depending on the imaging modality, microaneurysms were detected with a sensitivity of respectively 8962%, 9024%, and 9374% and a positive predictive value of respectively 8950%, 8975%, and 9167%, which is better than previously published methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39928</th>\n",
       "      <td>leveraging multimodal deep learning architecture with retina lesion information to detect diabetic retinopathy to improve disease severity classification from fundus images using a hybrid architecture with symptom awareness for diabetic retinopathy dr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130996</th>\n",
       "      <td>automatic detection of retina disease: robustness to image quality and localization of anatomy structure the automated detection of diabetic retinopathy and other eye diseases in images of the retina has great promise as a low-cost method for broad-based screening many systems in the literature which perform automated detection include a quality estimation step and physiological feature detection, including the vascular tree and the optic nerve / macula location in this work, we study the robustness of an automated disease detection method with respect to the accuracy of the optic nerve location and the quality of the images obtained as judged by a quality estimation algorithm the detection algorithm features microaneurysm and exudate detection followed by feature extraction on the detected population to describe the overall retina image labeled images of retinas ground-truthed to disease states are used to train a supervised learning algorithm to identify the disease state of the retina image and exam set under the restrictions of high confidence optic nerve detections and good quality imagery, the system achieves a sensitivity and specificity of 948% and 787% with area-under-curve of 953% analysis of the effect of constraining quality and the distinction between mild non-proliferative diabetic retinopathy, normal retina images, and more severe disease states is included</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5793</th>\n",
       "      <td>testing a deep learning algorithm for detection of diabetic retinopathy in a spanish diabetic population and with messidor database the aim of the present study was to test our deep learning algorithm dla by reading the retinographies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56600</th>\n",
       "      <td>deep and densely connected networks for classification of diabetic retinopathy diabetes has recently emerged as a worldwide problem, and diabetic retinopathy is an abnormal state associated with the human retina due to the increase in daily screen-related activities of modern human beings, diabetic retinopathy is more prevalent among adults, leading to minor and major blindness doctors and clinicians are unable to perform early diagnoses due to the large number of patients to solve this problem, this study introduces a classification model for retinal images that distinguishes between the various stages of diabetic retinopathy this work involves deploying deep and densely connected networks for retinal image analysis with training from scratch dense connections between the convolutional layers of the network are an essential factor to enhance accuracy owing to the deeper supervision between layers another factor is the growth rate that further assists our model in learning more sophisticated feature maps regarding retinal images from every stage of the network we compute the area under the curve, sensitivity, and specificity, particularly for messidor-2 and eyepacs compared to existing approaches, our method achieved better results, with an approximate rise rate of 001, 003, and 001, respectively therefore, computer-aided programs can help in diagnostic centers as automated detection systems</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53281</th>\n",
       "      <td>hard exudate detection based on deep model learned information and multi-feature joint representation for diabetic retinopathy screening diabetic retinopathy dr, which is generally diagnosed by the presence of hemorrhages and hard exudates, is one of the most prevalent causes of visual impairment and blindness early detection of hard exudates hes in color fundus photographs can help in preventing such destructive damage however, this is a challenging task due to high intra-class diversity and high similarity with other structures in the fundus images most of the existing methods for detecting hes are based on characterizing hes using hand crafted features hcfs only, which can not characterize hes accurately deep learning methods are scarce in this domain because they require large-scale sample sets for training which are not generally available for most routine medical imaging research</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68628</th>\n",
       "      <td>a data-driven approach to referable diabetic retinopathy detection prior art on automated screening of diabetic retinopathy and direct referral decision shows promising performance; yet most methods build upon complex hand-crafted features whose performance often fails to generalize</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66592</th>\n",
       "      <td>deep learning in estimating prevalence and systemic risk factors for diabetic retinopathy: a multi-ethnic study in any community, the key to understanding the burden of a specific condition is to conduct an epidemiological study the deep learning system dls recently showed promising diagnostic performance for diabetic retinopathy dr this study aims to use dls as the grading tool, instead of human assessors, to determine the prevalence and the systemic cardiovascular risk factors for dr on fundus photographs, in patients with diabetes this is a multi-ethnic 5 races, multi-site 8 datasets from singapore, usa, hong kong, china and australia, cross-sectional study involving 18,912 patients &lt;i&gt;n&lt;/i&gt; = 93,293 images we compared these results and the time taken for dr assessment by dls versus 17 human assessors - 10 retinal specialists/ophthalmologists and 7 professional graders the estimation of dr prevalence between dls and human assessors is comparable for any dr, referable dr and vision-threatening dr vtdr human assessors: 159, 65% and 41%; dls: 161%, 64%, 37% both assessment methods identified similar risk factors with comparable aucs, including younger age, longer diabetes duration, increased hba1c and systolic blood pressure, for any dr, referable dr and vtdr &lt;i&gt;p&lt;/i&gt; &gt; 005 the total time taken for dls to evaluate dr from 93,293 fundus photographs was ~1 month compared to 2 years for human assessors in conclusion, the prevalence and systemic risk factors for dr in multi-ethnic population could be determined accurately using a dls, in significantly less time than human assessors this study highlights the potential use of ai for future epidemiology or clinical trials for dr grading in the global communities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37735</th>\n",
       "      <td>application of non-mydriatic fundus examination and artificial intelligence to promote the screening of diabetic retinopathy in the endocrine clinic: an observational study of t2dm patients in tianjin, china we aimed to determine the role of non-mydriatic fundus examination and artificial intelligence ai in screening diabetic retinopathy dr in patients with diabetes in the metabolic disease management center mmc in tianjin, china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37755</th>\n",
       "      <td>coarse-to-fine classification for diabetic retinopathy grading using convolutional neural network diabetic retinopathy dr is the most common eye complication of diabetes and one of the leading causes of blindness and vision impairment automated and accurate dr grading is of great significance for the timely and effective treatment of fundus diseases current clinical methods remain subject to potential time-consumption and high-risk in this paper, a hierarchically coarse-to-fine network cf-drnet is proposed as an automatic clinical tool to classify five stages of dr severity grades using convolutional neural networks cnns the cf-drnet conforms to the hierarchical characteristic of dr grading and effectively improves the classification performance of five-class dr grading, which consists of the following: 1 the coarse network performs two-class classification including no dr and dr, where the attention gate module highlights the salient lesion features and suppresses irrelevant background information 2 the fine network is proposed to classify four stages of dr severity grades of the grade dr from the coarse network including mild, moderate, severe non-proliferative dr npdr and proliferative dr pdr experimental results show that proposed cf-drnet outperforms some state-of-art methods in the publicly available idrid and kaggle fundus image datasets these results indicate our method enables an efficient and reliable dr grading diagnosis in clinic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "80853                                                                                                                                                                                                                                                                                                                                                                                               feature selection and parameters optimization of support vector machines based on hybrid glowworm swarm optimization for classification of diabetic retinopathy diabetic retinopathy dr has been a leading cause of blindness in case of human beings falling between the ages of 20 and 74 years this will have a major influence on both the patient and the society as it can normally influence the humans in their gainful years an early dr detection is quite challenging as it may not be detected by humans there are several techniques and algorithms that have been established for detecting the dr these techniques have been facing problems to achieve effective sensitivity, accuracy, and specificity in order to overcome all these problems, the work has proposed one more such effective algorithm for image processing in order to increase the efficiency and also identify easily the dr diseases a major challenge in the task is the automatic detection of the microaneurysms in this work, the support vector machine svm parameters optimized with glowworm swarm optimization gso and genetic algorithm ga is used to classify the dr because the svm parameter c and γ to control the performance of the classifier for this work, the svms get fused with the hybrid gso-ga along with the feature chromosomes that are generated that will thereby direct the ga search to a straight line of the error of optimal generalization in their super parameter space this gso algorithm will not have memory and the glow worms will not retain any information in memory the results of the experiment prove that this method had achieved a better performance   \n",
       "92430                                                                                                                                                                                                                                                                                                                                                                                                  an automated system for the detection and classification of retinal changes due to red lesions in longitudinal fundus images people with diabetes mellitus need annual screening to check for the development of diabetic retinopathy dr tracking small retinal changes due to early diabetic retinopathy lesions in longitudinal fundus image sets is challenging due to intra- and intervisit variability in illumination and image quality, the required high registration accuracy, and the subtle appearance of retinal lesions compared to other retinal features this paper presents a robust and flexible approach for automated detection of longitudinal retinal changes due to small red lesions by exploiting normalized fundus images that significantly reduce illumination variations and improve the contrast of small retinal features to detect spatio-temporal retinal changes, the absolute difference between the extremes of the multiscale blobness responses of fundus images from two time points is proposed as a simple and effective blobness measure dr related changes are then identified based on several intensity and shape features by a support vector machine classifier the proposed approach was evaluated in the context of a regular diabetic retinopathy screening program involving subjects ranging from healthy no retinal lesion to moderate with clinically relevant retinal lesions dr levels evaluation shows that the system is able to detect retinal changes due to small red lesions with a sensitivity of at an average false positive rate of 1 and 25 lesions per eye on small and large fields-of-view of the retina, respectively   \n",
       "29064   using artificial intelligence as an initial triage strategy in diabetic retinopathy screening program in china <b>objective:</b> to investigate the diagnostic accuracy and efficiency of an artificial intelligence ai triaging model in a diabetic retinopathy dr screening program <b>methods:</b> a dr screening program was conducted in kashi city and kizilsu kirghiz autonomous prefecture of the xinjiang uyur autonomous region from may to july 2018, and 8 005 patients with diabetes mellitus were included fundus images, one centered at optic disc and one centered at macula, were taken for both eyes a previously validated ai algorithm was applied as the first step to identify the patients with all 4 images if the images were classified as gradable and negative dr, an ai-generated report was immediately provided without sending to manual grading, and 1/3 of these patients were randomly sampled for manual grading and quality control group a for the patients with at least one image classified as ungradable or positive for any dr, all images were sent for manual grading group b finally, 300 patients were randomly selected from group a and group b respectively for accuracy assessment, where the patients and their images were classified by a specialist panel for referral dr pre-proliferative dr, or proliferative dr, and/or diabetic macular edema <b>results:</b> among 8 005 patients for dr screening including 3 220 males and 4 785 females, aged 583±106 years, after ai triaging, 5 267 658% potentially received reports from ai system and 2 738 342% required manual grading in group a, the accuracy and specificity of ai classification and manual grading on referral dr were all 100% in group b, the accuracy of ai and manual grading were 758% and 903%, respectively, while the sensitivity of ai and manual grading was 100% and 791%, respectively <b>conclusion:</b> ai alleviates 60% of the workload of manual grading without missing any referral patients with the aid of the current ai triaging model   \n",
       "34564                                                                                                                                                                                                                                                                                                                                                                                                        hybrid model structure for diabetic retinopathy classification diabetic retinopathy dr is one of the most common complications of diabetes and the main cause of blindness the progression of the disease can be prevented by early diagnosis of dr due to differences in the distribution of medical conditions and low labor efficiency, the best time for diagnosis and treatment was missed, which results in impaired vision using neural network models to classify and diagnose dr can improve efficiency and reduce costs in this work, an improved loss function and three hybrid model structures hybrid-a, hybrid-f, and hybrid-c were proposed to improve the performance of dr classification models efficientnetb4, efficientnetb5, nasnetlarge, xception, and inceptionresnetv2 cnns were chosen as the basic models these basic models were trained using enhance cross-entropy loss and cross-entropy loss, respectively the output of the basic models was used to train the hybrid model structures experiments showed that enhance cross-entropy loss can effectively accelerate the training process of the basic models and improve the performance of the models under various evaluation metrics the proposed hybrid model structures can also improve dr classification performance compared with the best-performing results in the basic models, the accuracy of dr classification was improved from 8544% to 8634%, the sensitivity was improved from 9848% to 9877%, the specificity was improved from 7182% to 7476%, the precision was improved from 9027% to 9137%, and the f1 score was improved from 9362% to 939% by using hybrid model structures   \n",
       "90440                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             deep convolutional neural network-based early automated detection of diabetic retinopathy using fundus image the automatic detection of diabetic retinopathy is of vital importance, as it is the main cause of irreversible vision loss in the working-age population in the developed world the early detection of diabetic retinopathy occurrence can be very helpful for clinical treatment; although several different feature extraction approaches have been proposed, the classification task for retinal images is still tedious even for those trained clinicians recently, deep convolutional neural networks have manifested superior performance in image classification compared to previous handcrafted feature-based image classification methods thus, in this paper, we explored the use of deep convolutional neural network methodology for the automatic classification of diabetic retinopathy using color fundus image, and obtained an accuracy of 945% on our dataset, outperforming the results obtained by using classical approaches   \n",
       "84398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  automated detection of diabetic retinopathy using deep learning diabetic retinopathy is a leading cause of blindness among working-age adults early detection of this condition is critical for good prognosis in this paper, we demonstrate the use of convolutional neural networks cnns on color fundus images for the recognition task of diabetic retinopathy staging our network models achieved test metric performance comparable to baseline literature results, with validation sensitivity of 95% we additionally explored multinomial classification models, and demonstrate that errors primarily occur in the misclassification of mild disease as normal due to the cnns inability to detect subtle disease features we discovered that preprocessing with contrast limited adaptive histogram equalization and ensuring dataset fidelity by expert verification of class labels improves recognition of subtle features transfer learning on pretrained googlenet and alexnet models from imagenet improved peak test set accuracies to 745%, 688%, and 572% on 2-ary, 3-ary, and 4-ary classification models, respectively   \n",
       "74763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       artificial intelligence for the detection of diabetic retinopathy in primary care: protocol for algorithm development diabetic retinopathy dr is one of the most important causes of blindness worldwide, especially in developed countries in diabetic patients, periodic examination of the back of the eye using a nonmydriatic camera has been widely demonstrated to be an effective system to control and prevent the onset of dr convolutional neural networks have been used to detect dr, achieving very high sensitivities and specificities   \n",
       "156702                                                                                                                                                                                           robust detection and classification of longitudinal changes in color retinal fundus images for monitoring diabetic retinopathy a fully automated approach is presented for robust detection and classification of changes in longitudinal time-series of color retinal fundus images of diabetic retinopathy the method is robust to: 1 spatial variations in illumination resulting from instrument limitations and changes both within, and between patient visits; 2 imaging artifacts such as dust particles; 3 outliers in the training data; 4 segmentation and alignment errors robustness to illumination variation is achieved by a novel iterative algorithm to estimate the reflectance of the retina exploiting automatically extracted segmentations of the retinal vasculature, optic disk, fovea, and pathologies robustness to dust artifacts is achieved by exploiting their spectral characteristics, enabling application to film-based, as well as digital imaging systems false changes from alignment errors are minimized by subpixel accuracy registration using a 12-parameter transformation that accounts for unknown retinal curvature and camera parameters bayesian detection and classification algorithms are used to generate a color-coded output that is readily inspected a multiobserver validation on 43 image pairs from 22 eyes involving nonproliferative and proliferative diabetic retinopathies, showed a 97% change detection rate, a 3% miss rate, and a 10% false alarm rate the performance in correctly classifying the changes was 993% a self-consistency metric, and an error factor were developed to measure performance over more than two periods the average self consistency was 94% and the error factor was 006% although this study focuses on diabetic changes, the proposed techniques have broader applicability in ophthalmology   \n",
       "37508                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        dcardnet: diabetic retinopathy classification at multiple levels based on structural and angiographic optical coherence tomography optical coherence tomography oct and its angiography octa have several advantages for the early detection and diagnosis of diabetic retinopathy dr however, automated, complete dr classification frameworks based on both oct and octa data have not been proposed in this study, a convolutional neural network cnn based method is proposed to fulfill a dr classification framework using en face oct and octa   \n",
       "51248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            automated quantification of photoreceptor alteration in macular disease using optical coherence tomography and deep learning diabetic macular edema dme and retina vein occlusion rvo are macular diseases in which central photoreceptors are affected due to pathological accumulation of fluid optical coherence tomography allows to visually assess and evaluate photoreceptor integrity, whose alteration has been observed as an important biomarker of both diseases however, the manual quantification of this layered structure is challenging, tedious and time-consuming in this paper we introduce a deep learning approach for automatically segmenting and characterising photoreceptor alteration the photoreceptor layer is segmented using an ensemble of four different convolutional neural networks en-face representations of the layer thickness are produced to characterize the photoreceptors the pixel-wise standard deviation of the score maps produced by the individual models is also taken to indicate areas of photoreceptor abnormality or ambiguous results experimental results showed that our ensemble is able to produce results in pair with a human expert, outperforming each of its constitutive models no statistically significant differences were observed between mean thickness estimates obtained from automated and manually generated annotations therefore, our model is able to reliable quantify photoreceptors, which can be used to improve prognosis and managment of macular diseases   \n",
       "146565                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               optimal wavelet transform for the detection of microaneurysms in retina photographs in this paper, we propose an automatic method to detect microaneurysms in retina photographs microaneurysms are the most frequent and usually the first lesions to appear as a consequence of diabetic retinopathy so, their detection is necessary for both screening the pathology and follow up progression measurement automating this task, which is currently performed manually, would bring more objectivity and reproducibility we propose to detect them by locally matching a lesion template in subbands of wavelet transformed images to improve the method performance, we have searched for the best adapted wavelet within the lifting scheme framework the optimization process is based on a genetic algorithm followed by powells direction set descent results are evaluated on 120 retinal images analyzed by an expert and the optimal wavelet is compared to different conventional mother wavelets these images are of three different modalities: there are color photographs, green filtered photographs, and angiographs depending on the imaging modality, microaneurysms were detected with a sensitivity of respectively 8962%, 9024%, and 9374% and a positive predictive value of respectively 8950%, 8975%, and 9167%, which is better than previously published methods   \n",
       "39928                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  leveraging multimodal deep learning architecture with retina lesion information to detect diabetic retinopathy to improve disease severity classification from fundus images using a hybrid architecture with symptom awareness for diabetic retinopathy dr   \n",
       "130996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          automatic detection of retina disease: robustness to image quality and localization of anatomy structure the automated detection of diabetic retinopathy and other eye diseases in images of the retina has great promise as a low-cost method for broad-based screening many systems in the literature which perform automated detection include a quality estimation step and physiological feature detection, including the vascular tree and the optic nerve / macula location in this work, we study the robustness of an automated disease detection method with respect to the accuracy of the optic nerve location and the quality of the images obtained as judged by a quality estimation algorithm the detection algorithm features microaneurysm and exudate detection followed by feature extraction on the detected population to describe the overall retina image labeled images of retinas ground-truthed to disease states are used to train a supervised learning algorithm to identify the disease state of the retina image and exam set under the restrictions of high confidence optic nerve detections and good quality imagery, the system achieves a sensitivity and specificity of 948% and 787% with area-under-curve of 953% analysis of the effect of constraining quality and the distinction between mild non-proliferative diabetic retinopathy, normal retina images, and more severe disease states is included   \n",
       "5793                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    testing a deep learning algorithm for detection of diabetic retinopathy in a spanish diabetic population and with messidor database the aim of the present study was to test our deep learning algorithm dla by reading the retinographies   \n",
       "56600                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       deep and densely connected networks for classification of diabetic retinopathy diabetes has recently emerged as a worldwide problem, and diabetic retinopathy is an abnormal state associated with the human retina due to the increase in daily screen-related activities of modern human beings, diabetic retinopathy is more prevalent among adults, leading to minor and major blindness doctors and clinicians are unable to perform early diagnoses due to the large number of patients to solve this problem, this study introduces a classification model for retinal images that distinguishes between the various stages of diabetic retinopathy this work involves deploying deep and densely connected networks for retinal image analysis with training from scratch dense connections between the convolutional layers of the network are an essential factor to enhance accuracy owing to the deeper supervision between layers another factor is the growth rate that further assists our model in learning more sophisticated feature maps regarding retinal images from every stage of the network we compute the area under the curve, sensitivity, and specificity, particularly for messidor-2 and eyepacs compared to existing approaches, our method achieved better results, with an approximate rise rate of 001, 003, and 001, respectively therefore, computer-aided programs can help in diagnostic centers as automated detection systems   \n",
       "53281                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           hard exudate detection based on deep model learned information and multi-feature joint representation for diabetic retinopathy screening diabetic retinopathy dr, which is generally diagnosed by the presence of hemorrhages and hard exudates, is one of the most prevalent causes of visual impairment and blindness early detection of hard exudates hes in color fundus photographs can help in preventing such destructive damage however, this is a challenging task due to high intra-class diversity and high similarity with other structures in the fundus images most of the existing methods for detecting hes are based on characterizing hes using hand crafted features hcfs only, which can not characterize hes accurately deep learning methods are scarce in this domain because they require large-scale sample sets for training which are not generally available for most routine medical imaging research   \n",
       "68628                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  a data-driven approach to referable diabetic retinopathy detection prior art on automated screening of diabetic retinopathy and direct referral decision shows promising performance; yet most methods build upon complex hand-crafted features whose performance often fails to generalize   \n",
       "66592                                                                                                                                                                                                                                                                                       deep learning in estimating prevalence and systemic risk factors for diabetic retinopathy: a multi-ethnic study in any community, the key to understanding the burden of a specific condition is to conduct an epidemiological study the deep learning system dls recently showed promising diagnostic performance for diabetic retinopathy dr this study aims to use dls as the grading tool, instead of human assessors, to determine the prevalence and the systemic cardiovascular risk factors for dr on fundus photographs, in patients with diabetes this is a multi-ethnic 5 races, multi-site 8 datasets from singapore, usa, hong kong, china and australia, cross-sectional study involving 18,912 patients <i>n</i> = 93,293 images we compared these results and the time taken for dr assessment by dls versus 17 human assessors - 10 retinal specialists/ophthalmologists and 7 professional graders the estimation of dr prevalence between dls and human assessors is comparable for any dr, referable dr and vision-threatening dr vtdr human assessors: 159, 65% and 41%; dls: 161%, 64%, 37% both assessment methods identified similar risk factors with comparable aucs, including younger age, longer diabetes duration, increased hba1c and systolic blood pressure, for any dr, referable dr and vtdr <i>p</i> > 005 the total time taken for dls to evaluate dr from 93,293 fundus photographs was ~1 month compared to 2 years for human assessors in conclusion, the prevalence and systemic risk factors for dr in multi-ethnic population could be determined accurately using a dls, in significantly less time than human assessors this study highlights the potential use of ai for future epidemiology or clinical trials for dr grading in the global communities   \n",
       "37735                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            application of non-mydriatic fundus examination and artificial intelligence to promote the screening of diabetic retinopathy in the endocrine clinic: an observational study of t2dm patients in tianjin, china we aimed to determine the role of non-mydriatic fundus examination and artificial intelligence ai in screening diabetic retinopathy dr in patients with diabetes in the metabolic disease management center mmc in tianjin, china   \n",
       "37755                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    coarse-to-fine classification for diabetic retinopathy grading using convolutional neural network diabetic retinopathy dr is the most common eye complication of diabetes and one of the leading causes of blindness and vision impairment automated and accurate dr grading is of great significance for the timely and effective treatment of fundus diseases current clinical methods remain subject to potential time-consumption and high-risk in this paper, a hierarchically coarse-to-fine network cf-drnet is proposed as an automatic clinical tool to classify five stages of dr severity grades using convolutional neural networks cnns the cf-drnet conforms to the hierarchical characteristic of dr grading and effectively improves the classification performance of five-class dr grading, which consists of the following: 1 the coarse network performs two-class classification including no dr and dr, where the attention gate module highlights the salient lesion features and suppresses irrelevant background information 2 the fine network is proposed to classify four stages of dr severity grades of the grade dr from the coarse network including mild, moderate, severe non-proliferative dr npdr and proliferative dr pdr experimental results show that proposed cf-drnet outperforms some state-of-art methods in the publicly available idrid and kaggle fundus image datasets these results indicate our method enables an efficient and reliable dr grading diagnosis in clinic   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "80853         0       0       0           0          0        0       0   \n",
       "92430         0       0       0           0          0        0       0   \n",
       "29064         0       0       0           0          0        0       0   \n",
       "34564         0       0       0           0          0        0       0   \n",
       "90440         0       0       0           0          0        0       0   \n",
       "84398         0       0       0           0          0        0       0   \n",
       "74763         0       0       0           0          0        0       0   \n",
       "156702        0       0       0           0          0        0       0   \n",
       "37508         0       0       0           0          0        0       0   \n",
       "51248         0       0       0           0          0        0       0   \n",
       "146565        0       0       0           0          0        0       0   \n",
       "39928         0       0       0           0          0        0       0   \n",
       "130996        0       0       0           0          0        0       0   \n",
       "5793          0       0       0           0          0        0       0   \n",
       "56600         0       0       0           0          0        0       0   \n",
       "53281         0       0       0           0          0        0       0   \n",
       "68628         0       0       0           0          0        0       0   \n",
       "66592         0       0       0           0          0        0       0   \n",
       "37735         0       0       0           0          0        0       0   \n",
       "37755         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "80853            0            0         0           0        0       0   \n",
       "92430            0            0         0           0        0       0   \n",
       "29064            0            0         0           0        0       0   \n",
       "34564            0            0         0           0        0       0   \n",
       "90440            0            0         0           0        0       0   \n",
       "84398            0            0         0           0        0       0   \n",
       "74763            0            0         0           0        0       0   \n",
       "156702           0            0         0           0        0       0   \n",
       "37508            0            0         0           0        0       0   \n",
       "51248            0            0         0           0        0       0   \n",
       "146565           0            0         0           0        0       0   \n",
       "39928            0            0         0           0        0       0   \n",
       "130996           0            0         0           0        0       0   \n",
       "5793             0            0         0           0        0       0   \n",
       "56600            0            0         0           0        0       0   \n",
       "53281            0            0         0           0        0       0   \n",
       "68628            0            0         0           0        0       0   \n",
       "66592            0            0         0           0        0       0   \n",
       "37735            0            0         0           0        0       0   \n",
       "37755            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "80853            0             0           0            0         0   \n",
       "92430            0             0           0            0         0   \n",
       "29064            0             0           0            0         0   \n",
       "34564            0             0           0            0         0   \n",
       "90440            0             0           0            0         0   \n",
       "84398            0             0           0            0         0   \n",
       "74763            0             0           0            0         0   \n",
       "156702           0             0           0            0         0   \n",
       "37508            0             0           0            0         0   \n",
       "51248            0             0           0            0         0   \n",
       "146565           0             0           0            0         0   \n",
       "39928            0             0           0            0         0   \n",
       "130996           0             0           0            0         0   \n",
       "5793             0             0           0            0         0   \n",
       "56600            0             0           0            0         0   \n",
       "53281            0             0           0            0         0   \n",
       "68628            0             0           0            0         0   \n",
       "66592            0             0           0            0         0   \n",
       "37735            0             0           0            0         0   \n",
       "37755            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "80853           0            0           0            0           0   \n",
       "92430           0            0           0            0           0   \n",
       "29064           0            0           0            0           0   \n",
       "34564           0            0           0            0           0   \n",
       "90440           0            0           0            0           0   \n",
       "84398           0            0           0            0           0   \n",
       "74763           0            0           0            0           0   \n",
       "156702          0            0           0            0           0   \n",
       "37508           0            0           0            0           0   \n",
       "51248           0            0           0            0           0   \n",
       "146565          0            0           0            0           0   \n",
       "39928           0            0           0            0           0   \n",
       "130996          0            0           0            0           0   \n",
       "5793            0            0           0            0           0   \n",
       "56600           0            0           0            0           0   \n",
       "53281           0            0           0            0           0   \n",
       "68628           0            0           0            0           0   \n",
       "66592           0            0           0            0           0   \n",
       "37735           0            0           0            0           0   \n",
       "37755           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "80853             0          0            0        0         0          0   \n",
       "92430             0          0            0        0         0          0   \n",
       "29064             0          0            0        0         0          0   \n",
       "34564             0          0            0        0         0          0   \n",
       "90440             0          0            0        0         0          0   \n",
       "84398             0          0            0        0         0          0   \n",
       "74763             0          0            0        0         0          0   \n",
       "156702            0          0            0        0         0          0   \n",
       "37508             0          0            0        0         0          0   \n",
       "51248             0          0            0        0         0          0   \n",
       "146565            0          0            0        0         0          0   \n",
       "39928             0          0            0        0         0          0   \n",
       "130996            0          0            0        0         0          0   \n",
       "5793              0          0            0        0         0          0   \n",
       "56600             0          0            0        0         0          0   \n",
       "53281             0          0            0        0         0          0   \n",
       "68628             0          0            0        0         0          0   \n",
       "66592             0          0            0        0         0          0   \n",
       "37735             0          0            0        0         0          0   \n",
       "37755             0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "80853        0        0         0          0        0       0         0   \n",
       "92430        0        0         0          0        0       0         0   \n",
       "29064        0        0         0          0        0       0         0   \n",
       "34564        0        0         0          0        0       0         0   \n",
       "90440        0        0         0          0        0       0         0   \n",
       "84398        0        0         0          0        0       0         0   \n",
       "74763        0        0         0          0        0       0         0   \n",
       "156702       0        0         0          0        0       0         0   \n",
       "37508        0        0         0          0        0       0         0   \n",
       "51248        0        0         0          0        0       0         0   \n",
       "146565       0        0         0          0        0       0         0   \n",
       "39928        0        0         0          0        0       0         0   \n",
       "130996       0        0         0          0        0       0         0   \n",
       "5793         0        0         0          0        0       0         0   \n",
       "56600        0        0         0          0        0       0         0   \n",
       "53281        0        0         0          0        0       0         0   \n",
       "68628        0        0         0          0        0       0         0   \n",
       "66592        0        0         0          0        0       0         0   \n",
       "37735        0        0         0          0        0       0         0   \n",
       "37755        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "80853           0        0           0         0        0        0       0   \n",
       "92430           0        0           0         0        0        0       0   \n",
       "29064           0        0           0         0        0        0       0   \n",
       "34564           0        0           0         0        0        0       0   \n",
       "90440           0        0           0         0        0        0       0   \n",
       "84398           0        0           0         0        0        0       0   \n",
       "74763           0        0           0         0        0        0       0   \n",
       "156702          0        0           0         0        0        0       0   \n",
       "37508           0        0           0         0        0        0       0   \n",
       "51248           0        0           0         0        0        0       0   \n",
       "146565          0        0           0         0        0        0       0   \n",
       "39928           0        0           0         0        0        0       0   \n",
       "130996          0        0           0         0        0        0       0   \n",
       "5793            0        0           0         0        0        0       0   \n",
       "56600           0        0           0         0        0        0       0   \n",
       "53281           0        0           0         0        0        0       0   \n",
       "68628           0        0           0         0        0        0       0   \n",
       "66592           0        0           0         0        1        1       0   \n",
       "37735           0        0           0         0        0        0       0   \n",
       "37755           0        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text  \n",
       "80853            0         0       1            0           1  \n",
       "92430            0         0       1            0           1  \n",
       "29064            0         0       1            0           1  \n",
       "34564            0         0       1            0           1  \n",
       "90440            0         0       1            0           1  \n",
       "84398            0         0       1            0           1  \n",
       "74763            0         0       1            0           1  \n",
       "156702           0         0       1            0           1  \n",
       "37508            0         0       1            0           1  \n",
       "51248            0         0       1            0           1  \n",
       "146565           0         0       1            0           1  \n",
       "39928            0         0       1            0           1  \n",
       "130996           0         0       1            0           1  \n",
       "5793             0         0       1            0           1  \n",
       "56600            0         0       1            0           1  \n",
       "53281            0         0       1            0           1  \n",
       "68628            0         0       1            0           1  \n",
       "66592            0         0       1            0           1  \n",
       "37735            0         1       1            0           1  \n",
       "37755            0         0       1            0           1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['retina_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd07100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32742, '1': 1437})\n"
     ]
    }
   ],
   "source": [
    "## OPHTHALMOLOGY [C11] / eye\n",
    "\n",
    "## text\n",
    "text = ['ophth', 'retina', 'retino', 'retinitis', 'eye disease', 'uveitis', 'iritis', 'conjunctiv', 'cornea', 'blephar',\n",
    "       'optic nerve', 'optic atrophy', 'optic disk', 'optic disc', 'optic neuropathy', 'choroid', 'blindness', 'macular',\n",
    "       'strabismus', 'ocular', 'glaucoma', 'keratoconus']\n",
    "\n",
    "spec['eye_text'] = np.where(groups['text'].str.contains('eye disease'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['eye_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['eye_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['eye_text'] = np.where((groups['text'].str.contains(\"eye\")) &\n",
    "                             (groups['text'].str.contains(\"optic\")) , \"1\", spec['eye_text'])\n",
    "spec['eye_text'] = np.where((groups['text'].str.contains(\"eye\")) &\n",
    "                             (groups['text'].str.contains(\"fundus\")) , \"1\", spec['eye_text'])\n",
    "spec['eye_text'] = np.where((groups['text'].str.contains(\"eye\")) &\n",
    "                             (groups['text'].str.contains(\"fundal\")) , \"1\", spec['eye_text'])\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['eye_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb7f15c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55449</th>\n",
       "      <td>medios- an offline, smartphone-based artificial intelligence algorithm for the diagnosis of diabetic retinopathy an observational study to assess the sensitivity and specificity of the medios smartphone-based offline deep learning artificial intelligence ai software to detect diabetic retinopathy dr compared with the image diagnosis of ophthalmologists</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55944</th>\n",
       "      <td>deep learning based sub-retinal fluid segmentation in central serous chorioretinopathy optical coherence tomography scans development of an automated sub-retinal fluid segmentation technique from optical coherence tomography oct scans is faced with challenges such as noise and motion artifacts present in oct images, variation in size, shape and location of fluid pockets within the retina the ability of a fully convolutional neural network to automatically learn significant low level features to differentiate subtle spatial variations makes it suitable for retinal fluid segmentation task hence, a fully convolutional neural network has been proposed in this work for the automatic segmentation of sub-retinal fluid in oct scans of central serous chorioretinopathy csc pathology the proposed method has been evaluated on a dataset of 15 oct volumes and an average dice rate, precision and recall of 091, 093 and 089 respectively has been achieved over the test set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87247</th>\n",
       "      <td>microaneurysm detection using fully convolutional neural networks diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough microaneurysms are the earliest clinical signs of diabetic retinopathy this paper presents an automatic method for detecting microaneurysms in fundus photographies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159000</th>\n",
       "      <td>optical coherence tomography machine learning classifiers for glaucoma detection: a preliminary study machine-learning classifiers are trained computerized systems with the ability to detect the relationship between multiple input parameters and a diagnosis the present study investigated whether the use of machine-learning classifiers improves optical coherence tomography oct glaucoma detection</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159396</th>\n",
       "      <td>a mixture of experts network structure for modelling doppler ultrasound blood flow signals mixture of experts me is a modular neural network architecture for supervised learning this paper illustrates the use of me network structure to guide modelling doppler ultrasound blood flow signals expectation-maximization em algorithm was used for training the me so that the learning process is decoupled in a manner that fits well with the modular structure the ophthalmic and internal carotid arterial doppler signals were decomposed into time-frequency representations using discrete wavelet transform and statistical features were calculated to depict their distribution the me network structures were implemented for diagnosis of ophthalmic and internal carotid arterial disorders using the statistical features as inputs to improve diagnostic accuracy, the outputs of expert networks were combined by a gating network simultaneously trained in order to stochastically select the expert that is performing the best at solving the problem the me network structure achieved accuracy rates which were higher than that of the stand-alone neural network models</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "55449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   medios- an offline, smartphone-based artificial intelligence algorithm for the diagnosis of diabetic retinopathy an observational study to assess the sensitivity and specificity of the medios smartphone-based offline deep learning artificial intelligence ai software to detect diabetic retinopathy dr compared with the image diagnosis of ophthalmologists   \n",
       "55944                                                                                                                                                                                            deep learning based sub-retinal fluid segmentation in central serous chorioretinopathy optical coherence tomography scans development of an automated sub-retinal fluid segmentation technique from optical coherence tomography oct scans is faced with challenges such as noise and motion artifacts present in oct images, variation in size, shape and location of fluid pockets within the retina the ability of a fully convolutional neural network to automatically learn significant low level features to differentiate subtle spatial variations makes it suitable for retinal fluid segmentation task hence, a fully convolutional neural network has been proposed in this work for the automatic segmentation of sub-retinal fluid in oct scans of central serous chorioretinopathy csc pathology the proposed method has been evaluated on a dataset of 15 oct volumes and an average dice rate, precision and recall of 091, 093 and 089 respectively has been achieved over the test set   \n",
       "87247                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       microaneurysm detection using fully convolutional neural networks diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough microaneurysms are the earliest clinical signs of diabetic retinopathy this paper presents an automatic method for detecting microaneurysms in fundus photographies   \n",
       "159000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       optical coherence tomography machine learning classifiers for glaucoma detection: a preliminary study machine-learning classifiers are trained computerized systems with the ability to detect the relationship between multiple input parameters and a diagnosis the present study investigated whether the use of machine-learning classifiers improves optical coherence tomography oct glaucoma detection   \n",
       "159396  a mixture of experts network structure for modelling doppler ultrasound blood flow signals mixture of experts me is a modular neural network architecture for supervised learning this paper illustrates the use of me network structure to guide modelling doppler ultrasound blood flow signals expectation-maximization em algorithm was used for training the me so that the learning process is decoupled in a manner that fits well with the modular structure the ophthalmic and internal carotid arterial doppler signals were decomposed into time-frequency representations using discrete wavelet transform and statistical features were calculated to depict their distribution the me network structures were implemented for diagnosis of ophthalmic and internal carotid arterial disorders using the statistical features as inputs to improve diagnostic accuracy, the outputs of expert networks were combined by a gating network simultaneously trained in order to stochastically select the expert that is performing the best at solving the problem the me network structure achieved accuracy rates which were higher than that of the stand-alone neural network models   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "55449         0       0       0           0          0        0       0   \n",
       "55944         0       0       0           0          0        0       0   \n",
       "87247         0       0       0           0          0        0       0   \n",
       "159000        0       0       0           0          0        0       0   \n",
       "159396        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "55449            0            0         0           0        0       0   \n",
       "55944            0            0         0           0        0       0   \n",
       "87247            0            0         0           0        0       0   \n",
       "159000           0            0         0           0        0       0   \n",
       "159396           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "55449            0             0           0            0         0   \n",
       "55944            0             0           0            0         0   \n",
       "87247            0             0           0            0         0   \n",
       "159000           0             0           0            0         0   \n",
       "159396           0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "55449           0            0           0            0           0   \n",
       "55944           0            0           0            0           0   \n",
       "87247           0            0           0            0           0   \n",
       "159000          0            0           0            0           0   \n",
       "159396          0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "55449             0          0            0        0         0          0   \n",
       "55944             0          0            0        0         0          0   \n",
       "87247             0          0            0        0         0          0   \n",
       "159000            0          0            0        0         0          0   \n",
       "159396            0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "55449        0        0         0          0        0       0         0   \n",
       "55944        0        0         0          0        0       0         0   \n",
       "87247        0        0         0          0        0       0         0   \n",
       "159000       0        0         0          0        0       0         0   \n",
       "159396       0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "55449           0        0           0         0        0        0       0   \n",
       "55944           0        0           0         0        0        0       0   \n",
       "87247           0        0           0         0        0        0       0   \n",
       "159000          0        0           0         0        0        0       0   \n",
       "159396          0        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \n",
       "55449            0         0       1            0           1        1  \n",
       "55944            0         0       0            0           0        1  \n",
       "87247            0         0       1            0           1        1  \n",
       "159000           0         0       0            0           0        1  \n",
       "159396           0         0       0            0           0        1  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['eye_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f2a93c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33435, '1': 744})\n"
     ]
    }
   ],
   "source": [
    "## HAEMATOLOGIC [C15] / haem\n",
    "\n",
    "## text\n",
    "text = ['haematological cancer', 'hematological cancer', 'haematological malig', 'hematological malig', 'myelodysplas',\n",
    "       'myeloprolif', 'lymphoprolif', 'leukaemia', 'leukemia', 'myelofibro', 'thrombocythemia', 'polycythemia vera',\n",
    "       'polycythemia rubra vera', 'thrombocythaemia', 'polycythaemia vera', 'polycythaemia rubra vera', 'lymphoma',\n",
    "       'myeloma', ' gvhd', 'stem cell transpl', 'bone marrow aspirate',\n",
    "       'haematolog', 'anemia', 'anaemia', 'hemoglobin', 'haemoglobin', 'sickle cell', 'thalassemia', 'thalassaemia',\n",
    "       'sickle crisis', 'clotting disorder', 'coagulation disorder', 'coagulopathy', 'hemophilia', 'haemophilia',\n",
    "       'von willebrand', 'disseminated intrasvascular', 'thrombocytopeni', 'hemoly', 'haemoly', 'cryoglob', 'thrombim',\n",
    "       'bone marrow', 'coagulation']\n",
    "\n",
    "spec['haem_text'] = np.where(groups['text'].str.contains('hematolog'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['haem_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['haem_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['haem_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf21fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33097, '1': 1082})\n"
     ]
    }
   ],
   "source": [
    "## GYNAE/OBSTETRIC [C13] / obs\n",
    "\n",
    "## text\n",
    "text = ['obstetric', 'fetal', 'foetal', 'foetus', 'fetus', 'gestation', 'pregnan', 'endometriosis', 'ovarian', 'gynecolog', 'uterine', 'uterus'\n",
    "       'cervix', 'pap smear', 'cervical cancer', 'cervical carcinoma', ' vagina ', 'vaginal', 'vaginosis', 'macrosomia', 'colposcop',\n",
    "       'gynaecolog', 'menopaus', 'eclamp', ' iugr ', 'caesarean', 'endometrial']\n",
    "\n",
    "spec['obs_text'] = np.where(groups['text'].str.contains('cesarean'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['obs_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['obs_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['obs_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "901255f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33427, '1': 752})\n"
     ]
    }
   ],
   "source": [
    "## NEPHROLOGY [C12] / renal\n",
    "\n",
    "## text\n",
    "text = [' renal ', 'kidney', 'hemodialysis', 'haemodialysis', 'hemofilt', 'haemofilt', 'nephro', 'nephrit', 'glomerulus']\n",
    "\n",
    "spec['renal_text'] = np.where(groups['text'].str.contains('renovasc'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['renal_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['renal_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['renal_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f8a4f628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46837</th>\n",
       "      <td>predictive modeling of blood pressure during hemodialysis: a comparison of linear model, random forest, support vector regression, xgboost, lasso regression and ensemble method intradialytic hypotension idh is commonly occurred and links to higher mortality among patients undergoing hemodialysis hd its early prediction and prevention will dramatically improve the quality of life however, predicting the occurrence of idh clinically is not simple the aims of this study are to develop an intelligent system with capability of predicting blood pressure bp during hd, and to further compare different machine learning algorithms for next systolic bp sbp prediction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82297</th>\n",
       "      <td>modeling the covariates effects on the hazard function by piecewise exponential artificial neural networks: an application to a controlled clinical trial on renal carcinoma in exploring the time course of a disease to support or generate biological hypotheses, the shape of the hazard function provides relevant information for long follow-ups the shape of hazard function may be complex, with the presence of multiple peaks in this paper we present the use of a neural network extension of the piecewise exponential model to study the shape of the hazard function in time in dependence of covariates the technique is applied to a dataset of 247 renal cell carcinoma patients from a randomized clinical trial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26362</th>\n",
       "      <td>a phenotypic risk score for predicting mortality in sickle cell disease risk assessment for patients with sickle cell disease scd remains challenging as it depends on an individual physicians experience and ability to integrate a variety of test results we aimed to provide a new risk score that combines clinical, laboratory, and imaging data in a prospective cohort of 600 adult patients with scd, we assessed the relationship of 70 baseline covariates to all-cause mortality random survival forest and regularised cox regression machine learning ml methods were used to select top predictors multivariable models and a risk score were developed and internally validated over a median follow-up of 4·3 years, 131 deaths were recorded multivariable models were developed using nine independent predictors of mortality: tricuspid regurgitant velocity, estimated right atrial pressure, mitral e velocity, left ventricular septal thickness, body mass index, blood urea nitrogen, alkaline phosphatase, heart rate and age our prognostic risk score had superior performance with a bias-corrected c-statistic of 0·763 our model stratified patients into four groups with significantly different 4-year mortality rates 3%, 11%, 35% and 75% respectively using readily available variables from patients with scd, we applied ml techniques to develop and validate a mortality risk scoring method that reflects the summation of cardiopulmonary, renal and liver end-organ damage trial registration: clinicaltrialsgov identifier: nct#00011648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101239</th>\n",
       "      <td>electronic medical record-based predictive model for acute kidney injury in an acute care hospital patients with acute kidney injury aki are at risk for increased morbidity and mortality lack of specific treatment has meant that efforts have focused on early diagnosis and timely treatment advanced algorithms for clinical assistance including aki prediction models have potential to provide accurate risk estimates in this project, we aim to provide a clinical decision supporting system cdss based on a self-learning predictive model for aki in patients of an acute care hospital data of all in-patient episodes in adults admitted will be analysed using data mining techniques to build a prediction model the subsequent machine-learning process including two algorithms for data stream and concept drift will refine the predictive ability of the model simulation studies on the model will be used to quantify the expected impact of several scenarios of change in factors that influence aki incidence the proposed dynamic cdss will apply to future in-hospital aki surveillance in clinical practice</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23028</th>\n",
       "      <td>predicting the appearance of hypotension during hemodialysis sessions using machine learning classifiers a patient suffering from advanced chronic renal disease undergoes several dialysis sessions on different dates several clinical parameters are monitored during the different hours of any of these sessions these parameters, together with the information provided by other parameters of analytical nature, can be very useful to determine the probability that a patient may suffer from hypotension during the session, which should be specially watched since it represents a proven factor of possible mortality however, the analytical information is not always available to the healthcare personnel, or it is far in time, so the clinical parameters monitored during the session become key to the prevention of hypotension this article presents an investigation to predict the appearance of hypotension during a dialysis session, using predictive models trained from a large dialysis database, which contains the clinical information of 98,015 sessions corresponding to 758 patients the prediction model takes into account up to 22 clinical parameters measured five times during the session, as well as the gender and age of the patient this model was trained by means of machine learning classifiers, providing a success in the prediction higher than 80%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "46837                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  predictive modeling of blood pressure during hemodialysis: a comparison of linear model, random forest, support vector regression, xgboost, lasso regression and ensemble method intradialytic hypotension idh is commonly occurred and links to higher mortality among patients undergoing hemodialysis hd its early prediction and prevention will dramatically improve the quality of life however, predicting the occurrence of idh clinically is not simple the aims of this study are to develop an intelligent system with capability of predicting blood pressure bp during hd, and to further compare different machine learning algorithms for next systolic bp sbp prediction   \n",
       "82297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      modeling the covariates effects on the hazard function by piecewise exponential artificial neural networks: an application to a controlled clinical trial on renal carcinoma in exploring the time course of a disease to support or generate biological hypotheses, the shape of the hazard function provides relevant information for long follow-ups the shape of hazard function may be complex, with the presence of multiple peaks in this paper we present the use of a neural network extension of the piecewise exponential model to study the shape of the hazard function in time in dependence of covariates the technique is applied to a dataset of 247 renal cell carcinoma patients from a randomized clinical trial   \n",
       "26362   a phenotypic risk score for predicting mortality in sickle cell disease risk assessment for patients with sickle cell disease scd remains challenging as it depends on an individual physicians experience and ability to integrate a variety of test results we aimed to provide a new risk score that combines clinical, laboratory, and imaging data in a prospective cohort of 600 adult patients with scd, we assessed the relationship of 70 baseline covariates to all-cause mortality random survival forest and regularised cox regression machine learning ml methods were used to select top predictors multivariable models and a risk score were developed and internally validated over a median follow-up of 4·3 years, 131 deaths were recorded multivariable models were developed using nine independent predictors of mortality: tricuspid regurgitant velocity, estimated right atrial pressure, mitral e velocity, left ventricular septal thickness, body mass index, blood urea nitrogen, alkaline phosphatase, heart rate and age our prognostic risk score had superior performance with a bias-corrected c-statistic of 0·763 our model stratified patients into four groups with significantly different 4-year mortality rates 3%, 11%, 35% and 75% respectively using readily available variables from patients with scd, we applied ml techniques to develop and validate a mortality risk scoring method that reflects the summation of cardiopulmonary, renal and liver end-organ damage trial registration: clinicaltrialsgov identifier: nct#00011648   \n",
       "101239                                                                                                                                                                                                                                                                                                                                                                                                                                              electronic medical record-based predictive model for acute kidney injury in an acute care hospital patients with acute kidney injury aki are at risk for increased morbidity and mortality lack of specific treatment has meant that efforts have focused on early diagnosis and timely treatment advanced algorithms for clinical assistance including aki prediction models have potential to provide accurate risk estimates in this project, we aim to provide a clinical decision supporting system cdss based on a self-learning predictive model for aki in patients of an acute care hospital data of all in-patient episodes in adults admitted will be analysed using data mining techniques to build a prediction model the subsequent machine-learning process including two algorithms for data stream and concept drift will refine the predictive ability of the model simulation studies on the model will be used to quantify the expected impact of several scenarios of change in factors that influence aki incidence the proposed dynamic cdss will apply to future in-hospital aki surveillance in clinical practice    \n",
       "23028                                                                                                                                                                               predicting the appearance of hypotension during hemodialysis sessions using machine learning classifiers a patient suffering from advanced chronic renal disease undergoes several dialysis sessions on different dates several clinical parameters are monitored during the different hours of any of these sessions these parameters, together with the information provided by other parameters of analytical nature, can be very useful to determine the probability that a patient may suffer from hypotension during the session, which should be specially watched since it represents a proven factor of possible mortality however, the analytical information is not always available to the healthcare personnel, or it is far in time, so the clinical parameters monitored during the session become key to the prevention of hypotension this article presents an investigation to predict the appearance of hypotension during a dialysis session, using predictive models trained from a large dialysis database, which contains the clinical information of 98,015 sessions corresponding to 758 patients the prediction model takes into account up to 22 clinical parameters measured five times during the session, as well as the gender and age of the patient this model was trained by means of machine learning classifiers, providing a success in the prediction higher than 80%   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "46837         0       0       0           0          0        0       0   \n",
       "82297         0       0       0           0          0        0       0   \n",
       "26362         0       0       0           0          0        0       0   \n",
       "101239        0       0       0           0          0        0       0   \n",
       "23028         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "46837            0            0         0           0        0       0   \n",
       "82297            0            0         0           0        1       0   \n",
       "26362            0            0         0           0        0       0   \n",
       "101239           0            0         0           0        0       0   \n",
       "23028            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "46837            0             0           0            0         0   \n",
       "82297            0             0           0            0         0   \n",
       "26362            0             0           0            0         0   \n",
       "101239           0             0           0            0         0   \n",
       "23028            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "46837           0            0           0            0           0   \n",
       "82297           0            0           0            1           0   \n",
       "26362           0            0           0            0           0   \n",
       "101239          0            0           0            0           0   \n",
       "23028           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "46837             0          0            0        0         0          0   \n",
       "82297             0          0            0        0         0          0   \n",
       "26362             0          0            0        0         0          0   \n",
       "101239            0          0            0        0         0          0   \n",
       "23028             0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "46837        0        0         0          0        0       0         0   \n",
       "82297        0        0         0          0        0       0         0   \n",
       "26362        0        1         1          0        0       0         0   \n",
       "101239       0        0         0          0        0       0         0   \n",
       "23028        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "46837           0        0           0         0        0        0       0   \n",
       "82297           0        0           0         0        0        0       0   \n",
       "26362           0        0           0         0        1        0       0   \n",
       "101239          0        0           0         0        0        0       0   \n",
       "23028           0        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \\\n",
       "46837            0         0       0            0           0        0   \n",
       "82297            0         0       0            0           0        0   \n",
       "26362            0         0       0            0           0        0   \n",
       "101239           0         0       0            0           0        0   \n",
       "23028            0         0       0            0           0        0   \n",
       "\n",
       "       haem_text obs_text renal_text  \n",
       "46837          0        0          1  \n",
       "82297          0        0          1  \n",
       "26362          1        0          1  \n",
       "101239         0        0          1  \n",
       "23028          0        0          1  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['renal_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c49679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33917, '1': 262})\n"
     ]
    }
   ],
   "source": [
    "## ACUTE & CHRONIC KIDNEY DISEASE / ACKD\n",
    "\n",
    "## text\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"acute kidney\"), \"1\", \"0\")\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"acute renal\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"kidney failure\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"renal failure\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"chronic kidney disease\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"chronic renal disease\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"stage kidney\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"stage renal\"), \"1\", spec['ackd_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['ackd_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "64e9093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32370, '1': 1809})\n"
     ]
    }
   ],
   "source": [
    "## PAEDIATRICS / paeds\n",
    "\n",
    "## text\n",
    "text = ['paedia', 'pedia', 'neonate', 'neonatal', 'teenage', 'youth', 'children', 'childhood', 'infant', \n",
    "       'newborn', 'baby', 'babies', 'toddler']\n",
    "\n",
    "spec['paeds_text'] = np.where(groups['text'].str.contains(' child '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['paeds_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['paeds_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['paeds_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6f602e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70919</th>\n",
       "      <td>leveraging human microbiome features to diagnose and stratify children with irritable bowel syndrome accurate diagnosis and stratification of children with irritable bowel syndrome ibs remain challenging given the central role of recurrent abdominal pain in ibs, we evaluated the relationships of pediatric ibs and abdominal pain with intestinal microbes and fecal metabolites using a comprehensive clinical characterization and multiomics strategy using rigorous clinical phenotyping, we identified preadolescent children aged 7 to 12 years with rome iii ibs n = 23 and healthy controls n = 22 and characterized their fecal microbial communities using whole-genome shotgun metagenomics and global unbiased fecal metabolomic profiling correlation-based approaches and machine learning algorithms identified associations between microbes, metabolites, and abdominal pain ibs cases differed from controls with respect to key bacterial taxa eg, flavonifractor plautii and lachnospiraceae bacterium 7_1_58faa, metagenomic functions eg, carbohydrate metabolism and amino acid metabolism, and higher-order metabolites eg, secondary bile acids, sterols, and steroid-like compounds significant associations between abdominal pain frequency and severity and intestinal microbial features were identified a random forest classifier built on metagenomic and metabolic markers successfully distinguished ibs cases from controls area under the curve, 093 leveraging multiple lines of evidence, intestinal microbes, genes/pathways, and metabolites were associated with ibs, and these features were capable of distinguishing children with ibs from healthy children these multi-omics features, and their links to childhood ibs coupled with nutritional interventions, may lead to new microbiome-guided diagnostic and therapeutic strategies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57127</th>\n",
       "      <td>predicting the serum digoxin concentrations of infants in the neonatal intensive care unit through an artificial neural network given its narrow therapeutic range, digoxins pharmacokinetic parameters in infants are difficult to predict due to variation in birth weight and gestational age, especially for critically ill newborns there is limited evidence to support the safety and dosage requirements of digoxin, let alone to predict its concentrations in infants this study aimed to compare the concentrations of digoxin predicted by traditional regression modeling and artificial neural network ann modeling for newborn infants given digoxin for clinically significant patent ductus arteriosus pda</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80810</th>\n",
       "      <td>a machine learning approach to estimating preterm infants survival: development of the preterm infants survival assessment pisa predictor estimation of mortality risk of very preterm neonates is carried out in clinical and research settings we aimed at elaborating a prediction tool using machine learning methods we developed models on a cohort of 23747 neonates &lt;30 weeks gestational age, or &lt;1501 g birth weight, enrolled in the italian neonatal network in 2008-2014 development set, using 12 easily collected perinatal variables we used a cohort from 2015-2016 n = 5810 as a test set among several machine learning methods we chose artificial neural networks nn the resulting predictor was compared with logistic regression models in the test cohort, nn had a slightly better discrimination than logistic regression p &lt; 0002 the differences were greater in subgroups of neonates at various gestational age or birth weight intervals, singletons using a cutoff of death probability of 05, logistic regression misclassified 67/5810 neonates 12 percent more than nn in conclusion our study - the largest published so far - shows that even in this very simplified scenario, using only limited information available up to 5 minutes after birth, a nn approach had a small but significant advantage over current approaches the software implementing the predictor is made freely available to the community</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132500</th>\n",
       "      <td>voxelwise multivariate statistics and brain-wide machine learning using the full diffusion tensor in this paper, we propose to use the full diffusion tensor to perform brain-wide score prediction on diffusion tensor imaging dti using the log-euclidean framework, rather than the commonly used fractional anisotropy fa indeed, scalar values such as the fa do not capture all the information contained in the diffusion tensor additionally, full tensor information is included in every step of the pre-processing pipeline: registration, smoothing and feature selection using voxelwise multivariate regression analysis this approach was tested on data obtained from 30 children and adolescents with autism spectrum disorder and showed some improvement over the fa-only analysis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77612</th>\n",
       "      <td>validation of a salivary rna test for childhood autism spectrum disorder &lt;b&gt;background:&lt;/b&gt; the diagnosis of autism spectrum disorder asd relies on behavioral assessment efforts to define biomarkers of asd have not resulted in an objective, reliable test studies of rna levels in asd have demonstrated potential utility, but have been limited by a focus on single rna types, small sample sizes, and lack of developmental delay controls we hypothesized that a saliva-based poly-omic rna panel could objectively distinguish children with asd from their neurotypical peers and children with non-asd developmental delay &lt;b&gt;methods:&lt;/b&gt; this multi-center cross-sectional study included 456 children, ages 19-83 months children were either neurotypical &lt;i&gt;n&lt;/i&gt; = 134 or had a diagnosis of asd &lt;i&gt;n&lt;/i&gt; = 238, or non-asd developmental delay &lt;i&gt;n&lt;/i&gt; = 84 comprehensive human and microbial rna abundance was measured in the saliva of all participants using unbiased next generation sequencing prior to analysis, the sample was randomly divided into a training set 82% of subjects and an independent validation test set 18% of subjects the training set was used to develop an rna-based algorithm that distinguished asd and non-asd children the validation set was not used in model development feature selection or training but served only to validate empirical accuracy &lt;b&gt;results:&lt;/b&gt; in the training set &lt;i&gt;n&lt;/i&gt; = 372; mean age 51 months; 75% male; 51% asd, a set of 32 rna features controlled for demographic and medical characteristics, identified asd status with a cross-validated area under the curve auc of 087 95% ci: 086-088 in the completely separate validation test set &lt;i&gt;n&lt;/i&gt; = 84; mean age 50 months; 85% male; 60% asd, the algorithm maintained an auc of 088 82% sensitivity and 88% specificity notably, the rna features were implicated in physiologic processes related to asd axon guidance, neurotrophic signaling &lt;b&gt;conclusion:&lt;/b&gt; salivary poly-omic rna measurement represents a novel, non-invasive approach that can accurately identify children with asd this technology could improve the specificity of referrals for asd evaluation or provide objective support for asd diagnoses</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "70919                                                                                                                                                                                                                                                                                                                                                                                   leveraging human microbiome features to diagnose and stratify children with irritable bowel syndrome accurate diagnosis and stratification of children with irritable bowel syndrome ibs remain challenging given the central role of recurrent abdominal pain in ibs, we evaluated the relationships of pediatric ibs and abdominal pain with intestinal microbes and fecal metabolites using a comprehensive clinical characterization and multiomics strategy using rigorous clinical phenotyping, we identified preadolescent children aged 7 to 12 years with rome iii ibs n = 23 and healthy controls n = 22 and characterized their fecal microbial communities using whole-genome shotgun metagenomics and global unbiased fecal metabolomic profiling correlation-based approaches and machine learning algorithms identified associations between microbes, metabolites, and abdominal pain ibs cases differed from controls with respect to key bacterial taxa eg, flavonifractor plautii and lachnospiraceae bacterium 7_1_58faa, metagenomic functions eg, carbohydrate metabolism and amino acid metabolism, and higher-order metabolites eg, secondary bile acids, sterols, and steroid-like compounds significant associations between abdominal pain frequency and severity and intestinal microbial features were identified a random forest classifier built on metagenomic and metabolic markers successfully distinguished ibs cases from controls area under the curve, 093 leveraging multiple lines of evidence, intestinal microbes, genes/pathways, and metabolites were associated with ibs, and these features were capable of distinguishing children with ibs from healthy children these multi-omics features, and their links to childhood ibs coupled with nutritional interventions, may lead to new microbiome-guided diagnostic and therapeutic strategies   \n",
       "57127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      predicting the serum digoxin concentrations of infants in the neonatal intensive care unit through an artificial neural network given its narrow therapeutic range, digoxins pharmacokinetic parameters in infants are difficult to predict due to variation in birth weight and gestational age, especially for critically ill newborns there is limited evidence to support the safety and dosage requirements of digoxin, let alone to predict its concentrations in infants this study aimed to compare the concentrations of digoxin predicted by traditional regression modeling and artificial neural network ann modeling for newborn infants given digoxin for clinically significant patent ductus arteriosus pda   \n",
       "80810                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a machine learning approach to estimating preterm infants survival: development of the preterm infants survival assessment pisa predictor estimation of mortality risk of very preterm neonates is carried out in clinical and research settings we aimed at elaborating a prediction tool using machine learning methods we developed models on a cohort of 23747 neonates <30 weeks gestational age, or <1501 g birth weight, enrolled in the italian neonatal network in 2008-2014 development set, using 12 easily collected perinatal variables we used a cohort from 2015-2016 n = 5810 as a test set among several machine learning methods we chose artificial neural networks nn the resulting predictor was compared with logistic regression models in the test cohort, nn had a slightly better discrimination than logistic regression p < 0002 the differences were greater in subgroups of neonates at various gestational age or birth weight intervals, singletons using a cutoff of death probability of 05, logistic regression misclassified 67/5810 neonates 12 percent more than nn in conclusion our study - the largest published so far - shows that even in this very simplified scenario, using only limited information available up to 5 minutes after birth, a nn approach had a small but significant advantage over current approaches the software implementing the predictor is made freely available to the community   \n",
       "132500                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           voxelwise multivariate statistics and brain-wide machine learning using the full diffusion tensor in this paper, we propose to use the full diffusion tensor to perform brain-wide score prediction on diffusion tensor imaging dti using the log-euclidean framework, rather than the commonly used fractional anisotropy fa indeed, scalar values such as the fa do not capture all the information contained in the diffusion tensor additionally, full tensor information is included in every step of the pre-processing pipeline: registration, smoothing and feature selection using voxelwise multivariate regression analysis this approach was tested on data obtained from 30 children and adolescents with autism spectrum disorder and showed some improvement over the fa-only analysis   \n",
       "77612   validation of a salivary rna test for childhood autism spectrum disorder <b>background:</b> the diagnosis of autism spectrum disorder asd relies on behavioral assessment efforts to define biomarkers of asd have not resulted in an objective, reliable test studies of rna levels in asd have demonstrated potential utility, but have been limited by a focus on single rna types, small sample sizes, and lack of developmental delay controls we hypothesized that a saliva-based poly-omic rna panel could objectively distinguish children with asd from their neurotypical peers and children with non-asd developmental delay <b>methods:</b> this multi-center cross-sectional study included 456 children, ages 19-83 months children were either neurotypical <i>n</i> = 134 or had a diagnosis of asd <i>n</i> = 238, or non-asd developmental delay <i>n</i> = 84 comprehensive human and microbial rna abundance was measured in the saliva of all participants using unbiased next generation sequencing prior to analysis, the sample was randomly divided into a training set 82% of subjects and an independent validation test set 18% of subjects the training set was used to develop an rna-based algorithm that distinguished asd and non-asd children the validation set was not used in model development feature selection or training but served only to validate empirical accuracy <b>results:</b> in the training set <i>n</i> = 372; mean age 51 months; 75% male; 51% asd, a set of 32 rna features controlled for demographic and medical characteristics, identified asd status with a cross-validated area under the curve auc of 087 95% ci: 086-088 in the completely separate validation test set <i>n</i> = 84; mean age 50 months; 85% male; 60% asd, the algorithm maintained an auc of 088 82% sensitivity and 88% specificity notably, the rna features were implicated in physiologic processes related to asd axon guidance, neurotrophic signaling <b>conclusion:</b> salivary poly-omic rna measurement represents a novel, non-invasive approach that can accurately identify children with asd this technology could improve the specificity of referrals for asd evaluation or provide objective support for asd diagnoses   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "70919         0       0       1           0          0        0       0   \n",
       "57127         1       0       0           0          0        0       0   \n",
       "80810         0       0       0           0          0        0       0   \n",
       "132500        0       0       0           0          0        0       0   \n",
       "77612         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "70919            0            0         0           0        0       0   \n",
       "57127            0            0         0           0        0       0   \n",
       "80810            0            0         0           0        0       0   \n",
       "132500           0            0         0           0        0       0   \n",
       "77612            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "70919            0             0           0            0         0   \n",
       "57127            0             0           0            0         0   \n",
       "80810            0             0           0            0         0   \n",
       "132500           0             0           0            0         0   \n",
       "77612            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "70919           0            0           0            0           0   \n",
       "57127           0            0           0            0           0   \n",
       "80810           0            0           0            0           0   \n",
       "132500          0            0           0            0           0   \n",
       "77612           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "70919             0          0            0        0         0          0   \n",
       "57127             0          0            0        0         0          0   \n",
       "80810             0          0            0        0         0          0   \n",
       "132500            0          1            0        0         0          0   \n",
       "77612             0          1            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "70919        1        0         0          0        0       0         0   \n",
       "57127        0        0         0          0        0       0         0   \n",
       "80810        0        0         0          0        0       0         0   \n",
       "132500       0        0         0          0        0       0         0   \n",
       "77612        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "70919           0        0           0         0        0        0       0   \n",
       "57127           0        0           0         0        0        0       0   \n",
       "80810           0        0           0         0        0        0       0   \n",
       "132500          1        0           0         0        0        0       0   \n",
       "77612           1        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \\\n",
       "70919            0         0       0            0           0        0   \n",
       "57127            0         0       0            0           0        0   \n",
       "80810            0         0       0            0           0        0   \n",
       "132500           0         0       0            0           0        0   \n",
       "77612            0         0       0            0           0        0   \n",
       "\n",
       "       haem_text obs_text renal_text ackd_text paeds_text  \n",
       "70919          0        0          0         0          1  \n",
       "57127          0        1          0         0          1  \n",
       "80810          0        1          0         0          1  \n",
       "132500         0        0          0         0          1  \n",
       "77612          0        0          0         0          1  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['paeds_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c6cce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33852, '1': 327})\n"
     ]
    }
   ],
   "source": [
    "## STOMATOGNATHIC, DENTAL [C07]  / dent\n",
    "\n",
    "## text\n",
    "text = [' dental', 'dentist', 'dentition', 'teeth', 'tooth', 'canine', 'incisor', 'molars', 'maxilla', 'mandibul', 'mandible',\n",
    "       'stomatognathic', 'gingiva', 'buccal', 'peridont']\n",
    "\n",
    "spec['dent_text'] = np.where(groups['text'].str.contains('maxillofacial'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['dent_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['dent_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['dent_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "68ed791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34462</th>\n",
       "      <td>prediction of 30-day hospital readmissions for all-cause dental conditions using machine learning it is unknown whether patients admitted for all-cause dental conditions acdc are at high risk for hospital readmission, or what are the risk factors for dental hospital readmission</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76894</th>\n",
       "      <td>deep-learning classification using convolutional neural network for evaluation of maxillary sinusitis on panoramic radiography to apply a deep-learning system for diagnosis of maxillary sinusitis on panoramic radiography, and to clarify its diagnostic performance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5792</th>\n",
       "      <td>automated adenoid hypertrophy assessment with lateral cephalometry in children based on artificial intelligence adenoid hypertrophy may lead to pediatric obstructive sleep apnea and mouth breathing the routine screening of adenoid hypertrophy in dental practice is helpful for preventing relevant craniofacial and systemic consequences the purpose of this study was to develop an automated assessment tool for adenoid hypertrophy based on artificial intelligence a clinical dataset containing 581 lateral cephalograms was used to train the convolutional neural network cnn according to fujiokas method for adenoid hypertrophy assessment, the regions of interest were defined with four keypoint landmarks the adenoid ratio based on the four landmarks was used for adenoid hypertrophy assessment another dataset consisting of 160 patientslateral cephalograms were used for evaluating the performance of the network diagnostic performance was evaluated with statistical analysis the developed system exhibited high sensitivity 0906, 95% confidence interval ci: 0750-0980, specificity 0938, 95% ci: 0881-0973 and accuracy 0919, 95% ci: 0877-0961 for adenoid hypertrophy assessment the area under the receiver operating characteristic curve was 0987 95% ci: 0974-1000 these results indicated the proposed assessment system is able to assess ah accurately the cnn-incorporated system showed high accuracy and stability in the detection of adenoid hypertrophy from childrenlateral cephalograms, implying the feasibility of automated adenoid hypertrophy screening utilizing a deep neural network model</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44421</th>\n",
       "      <td>performance of deep learning object detection technology in the detection and diagnosis of maxillary sinus lesions on panoramic radiographs the first aim of this study was to determine the performance of a deep learning object detection technique in the detection of maxillary sinuses on panoramic radiographs the second aim was to clarify the performance in the classification of maxillary sinus lesions compared with healthy maxillary sinuses</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76289</th>\n",
       "      <td>a pilot study using machine learning methods about factors influencing prognosis of dental implants this study tried to find the most significant factors predicting implant prognosis using machine learning methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \\\n",
       "34462                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     prediction of 30-day hospital readmissions for all-cause dental conditions using machine learning it is unknown whether patients admitted for all-cause dental conditions acdc are at high risk for hospital readmission, or what are the risk factors for dental hospital readmission   \n",
       "76894                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    deep-learning classification using convolutional neural network for evaluation of maxillary sinusitis on panoramic radiography to apply a deep-learning system for diagnosis of maxillary sinusitis on panoramic radiography, and to clarify its diagnostic performance   \n",
       "5792   automated adenoid hypertrophy assessment with lateral cephalometry in children based on artificial intelligence adenoid hypertrophy may lead to pediatric obstructive sleep apnea and mouth breathing the routine screening of adenoid hypertrophy in dental practice is helpful for preventing relevant craniofacial and systemic consequences the purpose of this study was to develop an automated assessment tool for adenoid hypertrophy based on artificial intelligence a clinical dataset containing 581 lateral cephalograms was used to train the convolutional neural network cnn according to fujiokas method for adenoid hypertrophy assessment, the regions of interest were defined with four keypoint landmarks the adenoid ratio based on the four landmarks was used for adenoid hypertrophy assessment another dataset consisting of 160 patientslateral cephalograms were used for evaluating the performance of the network diagnostic performance was evaluated with statistical analysis the developed system exhibited high sensitivity 0906, 95% confidence interval ci: 0750-0980, specificity 0938, 95% ci: 0881-0973 and accuracy 0919, 95% ci: 0877-0961 for adenoid hypertrophy assessment the area under the receiver operating characteristic curve was 0987 95% ci: 0974-1000 these results indicated the proposed assessment system is able to assess ah accurately the cnn-incorporated system showed high accuracy and stability in the detection of adenoid hypertrophy from childrenlateral cephalograms, implying the feasibility of automated adenoid hypertrophy screening utilizing a deep neural network model   \n",
       "44421                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               performance of deep learning object detection technology in the detection and diagnosis of maxillary sinus lesions on panoramic radiographs the first aim of this study was to determine the performance of a deep learning object detection technique in the detection of maxillary sinuses on panoramic radiographs the second aim was to clarify the performance in the classification of maxillary sinus lesions compared with healthy maxillary sinuses   \n",
       "76289                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      a pilot study using machine learning methods about factors influencing prognosis of dental implants this study tried to find the most significant factors predicting implant prognosis using machine learning methods   \n",
       "\n",
       "      icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "34462        0       0       0           0          0        0       0   \n",
       "76894        0       0       0           0          0        0       0   \n",
       "5792         0       0       0           0          0        0       0   \n",
       "44421        0       0       0           0          0        0       0   \n",
       "76289        0       0       0           0          0        0       0   \n",
       "\n",
       "      tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "34462           0            0         0           0        0       0   \n",
       "76894           0            0         0           0        0       0   \n",
       "5792            0            0         0           0        0       0   \n",
       "44421           0            0         0           0        0       0   \n",
       "76289           0            0         0           0        0       0   \n",
       "\n",
       "      breast_text breastca_text lungca_text brainca_text gica_text hepca_text  \\\n",
       "34462           0             0           0            0         0          0   \n",
       "76894           0             0           0            0         0          0   \n",
       "5792            0             0           0            0         0          0   \n",
       "44421           0             0           0            0         0          0   \n",
       "76289           0             0           0            0         0          0   \n",
       "\n",
       "      urology_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "34462            0           0            0           0            0   \n",
       "76894            0           0            0           0            0   \n",
       "5792             0           0            0           0            0   \n",
       "44421            0           0            0           0            0   \n",
       "76289            0           0            0           0            0   \n",
       "\n",
       "      psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "34462          0            0        0         0          0       0        0   \n",
       "76894          0            0        0         0          0       0        0   \n",
       "5792           0            0        0         0          0       0        0   \n",
       "44421          0            0        0         0          0       0        0   \n",
       "76289          0            0        0         0          0       0        0   \n",
       "\n",
       "      resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "34462         0          0        0       0         0          0        0   \n",
       "76894         0          0        0       0         0          0        0   \n",
       "5792          1          0        1       0         0          0        0   \n",
       "44421         0          0        0       0         0          0        0   \n",
       "76289         0          0        0       0         0          0        0   \n",
       "\n",
       "      epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "34462           0         0        0        0       0           0         0   \n",
       "76894           0         0        0        0       0           0         0   \n",
       "5792            0         0        0        0       0           0         0   \n",
       "44421           0         0        0        0       0           0         0   \n",
       "76289           0         0        0        0       0           0         0   \n",
       "\n",
       "      dm_text insulin_text retina_text eye_text haem_text obs_text renal_text  \\\n",
       "34462       0            0           0        0         0        0          0   \n",
       "76894       0            0           0        0         0        0          0   \n",
       "5792        0            0           0        0         0        0          0   \n",
       "44421       0            0           0        0         0        0          0   \n",
       "76289       0            0           0        0         0        0          0   \n",
       "\n",
       "      ackd_text paeds_text dent_text  \n",
       "34462         0          0         1  \n",
       "76894         0          0         1  \n",
       "5792          0          1         1  \n",
       "44421         0          0         1  \n",
       "76289         0          0         1  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['dent_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b039222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34020, '1': 159})\n"
     ]
    }
   ],
   "source": [
    "## AUDIOLOGY [C09] / audio\n",
    "\n",
    "## text\n",
    "text = ['audiology', ' ear disease', 'earache', 'labyrinth', 'otitis', 'otosclerosis', 'cochlear', 'tympanic memb',\n",
    "       'otoscop', 'acoustic neuroma', 'meniere', 'hearing loss', 'hearing impairment', 'cholesteatoma', 'otoacoustic', 'deafness', ' deaf ',\n",
    "       'middle ear', 'outer ear', 'inner ear', 'otolog', 'paroxysmal positional vertigo']\n",
    "\n",
    "spec['audio_text'] = np.where(groups['text'].str.contains('hearing aid'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['audio_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['audio_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['audio_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e4229ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113605</th>\n",
       "      <td>prediction of hearing loss among the noise-exposed workers in a steel factory using artificial intelligence approach prediction of hearing loss in noisy workplaces is considered to be an important aspect of hearing conservation program artificial intelligence, as a new approach, can be used to predict the complex phenomenon such as hearing loss using artificial neural networks, this study aims to present an empirical model for the prediction of the hearing loss threshold among noise-exposed workers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59051</th>\n",
       "      <td>building an otoscopic screening prototype tool using deep learning otologic diseases are often difficult to diagnose accurately for primary care providers deep learning methods have been applied with great success in many areas of medicine, often outperforming well trained human observers the aim of this work was to develop and evaluate an automatic software prototype to identify otologic abnormalities using a deep convolutional neural network</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12146</th>\n",
       "      <td>machine learning-based genetic diagnosis models for hereditary hearing loss by the gjb2, slc26a4 and mt-rnr1 variants hereditary hearing loss hhl is the most common sensory deficit, which highly afflicts humans with gene sequencing technology development, more variants will be identified and support genetic diagnoses, which is difficult for human experts to diagnose this study aims to develop a machine learning-based genetic diagnosis model of hhl-related variants of gjb2, slc26a4 and mt-rnr1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168917</th>\n",
       "      <td>a novel machine learning program applied to discover otological diagnoses a novel machine learning system, galactica, has been developed for knowledge discovery from databases this system was applied to discover diagnostic rules from a patient database containing 564 cases with vestibular schwannoma, bening paroxysmal positional vertigo, ménières disease, sudden deafness, traumatic vertigo and vestibular neuritis diagnoses the rules were evaluated using an independent testing set the accuracy of rules for these diagnoses were 91%, 96%, 81%, 95%, 92% and 98%, respectively besides being accurate, the rules contained the five most important diagnostic questions identified in the earlier research the knowledge presented with rules can be easily comprehended and verified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70347</th>\n",
       "      <td>development of an automatic classifier for the prediction of hearing impairment from industrial noise exposure the iso-1999 2013 international organization for standardization, geneva, switzerland standard is the most commonly used approach for estimating noise-induced hearing trauma however, its insensitivity to noise characteristics limits its practical application in this study, an automatic classification method using the support vector machine svm was developed to predict hearing impairment in workers exposed to both gaussian g and non-gaussian non-g industrial noises a recently collected human database n = 2,110 from industrial workers in china was used in the present study a statistical metric, kurtosis, was used to characterize the industrial noise in addition to using all the data as one group, the data were also broken down into the following four subgroups based on the level of kurtosis: g/quasi-g, low-kurtosis, middle-kurtosis, and high-kurtosis groups the performance of the iso-1999 and the svm models was compared over these five groups the results showed that: 1 the performance of the svm model significantly outperformed the iso-1999 model in all five groups 2 the iso-1999 model could not properly predict hearing impairment for the high-kurtosis group moreover, the iso-1999 model is likely to underestimate hearing impairment caused by both g and non-g noise exposures 3 the svm model is a potential tool to predict hearing impairment caused by diverse noise exposures</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "113605                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          prediction of hearing loss among the noise-exposed workers in a steel factory using artificial intelligence approach prediction of hearing loss in noisy workplaces is considered to be an important aspect of hearing conservation program artificial intelligence, as a new approach, can be used to predict the complex phenomenon such as hearing loss using artificial neural networks, this study aims to present an empirical model for the prediction of the hearing loss threshold among noise-exposed workers   \n",
       "59051                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   building an otoscopic screening prototype tool using deep learning otologic diseases are often difficult to diagnose accurately for primary care providers deep learning methods have been applied with great success in many areas of medicine, often outperforming well trained human observers the aim of this work was to develop and evaluate an automatic software prototype to identify otologic abnormalities using a deep convolutional neural network   \n",
       "12146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 machine learning-based genetic diagnosis models for hereditary hearing loss by the gjb2, slc26a4 and mt-rnr1 variants hereditary hearing loss hhl is the most common sensory deficit, which highly afflicts humans with gene sequencing technology development, more variants will be identified and support genetic diagnoses, which is difficult for human experts to diagnose this study aims to develop a machine learning-based genetic diagnosis model of hhl-related variants of gjb2, slc26a4 and mt-rnr1   \n",
       "168917                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a novel machine learning program applied to discover otological diagnoses a novel machine learning system, galactica, has been developed for knowledge discovery from databases this system was applied to discover diagnostic rules from a patient database containing 564 cases with vestibular schwannoma, bening paroxysmal positional vertigo, ménières disease, sudden deafness, traumatic vertigo and vestibular neuritis diagnoses the rules were evaluated using an independent testing set the accuracy of rules for these diagnoses were 91%, 96%, 81%, 95%, 92% and 98%, respectively besides being accurate, the rules contained the five most important diagnostic questions identified in the earlier research the knowledge presented with rules can be easily comprehended and verified   \n",
       "70347   development of an automatic classifier for the prediction of hearing impairment from industrial noise exposure the iso-1999 2013 international organization for standardization, geneva, switzerland standard is the most commonly used approach for estimating noise-induced hearing trauma however, its insensitivity to noise characteristics limits its practical application in this study, an automatic classification method using the support vector machine svm was developed to predict hearing impairment in workers exposed to both gaussian g and non-gaussian non-g industrial noises a recently collected human database n = 2,110 from industrial workers in china was used in the present study a statistical metric, kurtosis, was used to characterize the industrial noise in addition to using all the data as one group, the data were also broken down into the following four subgroups based on the level of kurtosis: g/quasi-g, low-kurtosis, middle-kurtosis, and high-kurtosis groups the performance of the iso-1999 and the svm models was compared over these five groups the results showed that: 1 the performance of the svm model significantly outperformed the iso-1999 model in all five groups 2 the iso-1999 model could not properly predict hearing impairment for the high-kurtosis group moreover, the iso-1999 model is likely to underestimate hearing impairment caused by both g and non-g noise exposures 3 the svm model is a potential tool to predict hearing impairment caused by diverse noise exposures   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "113605        0       0       0           0          0        0       0   \n",
       "59051         0       0       0           0          0        0       0   \n",
       "12146         0       0       0           0          0        0       0   \n",
       "168917        0       0       0           0          0        0       0   \n",
       "70347         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "113605           0            0         0           0        0       0   \n",
       "59051            0            0         0           0        0       0   \n",
       "12146            0            0         0           0        0       0   \n",
       "168917           0            0         0           0        0       0   \n",
       "70347            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "113605           0             0           0            0         0   \n",
       "59051            0             0           0            0         0   \n",
       "12146            0             0           0            0         0   \n",
       "168917           0             0           0            0         0   \n",
       "70347            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "113605          0            0           0            0           0   \n",
       "59051           0            0           0            0           0   \n",
       "12146           0            0           0            0           0   \n",
       "168917          0            0           0            0           0   \n",
       "70347           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "113605            0          0            0        0         0          0   \n",
       "59051             0          0            0        0         0          0   \n",
       "12146             0          0            0        0         0          0   \n",
       "168917            0          0            0        0         0          0   \n",
       "70347             0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "113605       0        0         0          0        0       0         0   \n",
       "59051        0        0         0          0        0       0         0   \n",
       "12146        0        0         0          0        0       0         0   \n",
       "168917       0        0         0          0        0       0         0   \n",
       "70347        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "113605          0        0           0         0        0        0       0   \n",
       "59051           0        0           0         0        0        0       0   \n",
       "12146           0        0           0         0        0        0       0   \n",
       "168917          0        0           0         0        0        0       0   \n",
       "70347           0        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \\\n",
       "113605           0         0       0            0           0        0   \n",
       "59051            0         0       0            0           0        0   \n",
       "12146            0         0       0            0           0        0   \n",
       "168917           0         0       0            0           0        0   \n",
       "70347            0         0       0            0           0        0   \n",
       "\n",
       "       haem_text obs_text renal_text ackd_text paeds_text dent_text audio_text  \n",
       "113605         0        0          0         0          0         0          1  \n",
       "59051          0        0          0         0          0         0          1  \n",
       "12146          0        0          0         0          0         0          1  \n",
       "168917         0        0          0         0          0         0          1  \n",
       "70347          0        0          0         0          0         0          1  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['audio_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "527f953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34079, '1': 100})\n"
     ]
    }
   ],
   "source": [
    "## BRAIN COMPUTER / bci\n",
    "\n",
    "## text\n",
    "spec['bci_text'] = np.where(groups['text'].str.contains(\"brain control\"), \"1\", \"0\")\n",
    "spec['bci_text'] = np.where(groups['text'].str.contains(\"brain computer\"), \"1\", \"0\")\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['bci_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "93e6f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33915, '1': 264})\n"
     ]
    }
   ],
   "source": [
    "## PROSTHESIS CONTROL / prosth\n",
    "\n",
    "## text\n",
    "spec['prosth_text'] = np.where(groups['text'].str.contains(\"prosthetic\"), \"1\", \"0\")\n",
    "spec['prosth_text'] = np.where(groups['text'].str.contains(\"prosthesis\"), \"1\", spec['prosth_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['prosth_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3dce9999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>bci_text</th>\n",
       "      <th>prosth_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9307</th>\n",
       "      <td>a machine learning framework to optimize optic nerve electrical stimulation for vision restoration optic nerve electrical stimulation is a promising technique to restore vision in blind subjects machine learning methods can be used to select effective stimulation protocols, but they require a model of the stimulated system to generate enough training data here, we use a convolutional neural network cnn as a model of the ventral visual stream a genetic algorithm drives the activation of the units in a layer of the cnn representing a cortical region toward a desired pattern, by refining the activation imposed at a layer representing the optic nerve to simulate the pattern of activation elicited by the sites of an electrode array, a simple point-source model was introduced and its optimization process was investigated for static and dynamic scenes psychophysical data confirm that our stimulation evolution framework produces results compatible with natural vision machine learning approaches could become a very powerful tool to optimize and personalize neuroprosthetic systems</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46246</th>\n",
       "      <td>elbow angle generation during activities of daily living using a submovement prediction model the present study aimed to develop a realistic model for the generation of human activities of daily living adl movements the angular profiles of the elbow joint during functional adl tasks such as eating and drinking were generated by a submovement-based closed-loop model first, the adl movements recorded from three human participants were broken down into logical phases, and each phase was decomposed into submovement components three separate artificial neural networks were trained to learn the submovement parameters and were then incorporated into a closed-loop model with error correction ability the model was able to predict angular trajectories of human adl movements with target access rate = 100%, vaf = 989%, and nrmse = 47% relative to the actual trajectories in addition, the model can be used to provide the desired target for practical trajectory planning in rehabilitation systems such as functional electrical stimulation, robot therapy, brain-computer interface, and prosthetic devices</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105929</th>\n",
       "      <td>influence of multiple dynamic factors on the performance of myoelectric pattern recognition hand motion classification using surface electromyogram emg signals has been widely studied for the control of powered prosthetics in laboratory conditions however, clinical applicability has been limited, as imposed by factors like electrodes shift, variations in the contraction force levels, forearm rotation angles, change of limb position and many other factors that all affect the emg pattern recognition performance while the impact of several of these factors on emg parameter estimation and pattern recognition has been considered individually in previous studies, a minimum number of experiments were reported to study the influence of multiple dynamic factors in this paper, we investigate the combined effect of varying forearm rotation angles and contraction force levels on the robustness of emg pattern recognition, while utilizing different time-and-frequency based feature extraction methods the emg pattern recognition system has been validated on a set of 11 subjects ten intact-limbed and one bilateral transradial amputee performing six classes of hand motions, each with three different force levels, each at three different forearm rotation angles, with six emg electrodes plus an accelerometer on the subjectsforearm our results suggest that the performance of the learning algorithms can be improved with the time-dependent power spectrum descriptors td-psd utilized in our experiments, with average classification accuracies of up to 90% across all subjects, force levels, and forearm rotation angles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166141</th>\n",
       "      <td>classification of finger activation for use in a robotic prosthesis arm hand amputees would highly benefit from a robotic prosthesis, which would allow the movement of a number of fingers in this paper we propose using the electromyographic signals recorded by two pairs of electrodes placed over the arm for operating such prosthesis multiple features from these signals are extracted whence the most relevant features are selected by a genetic algorithm as inputs for a simple classifier this method results in a probability of error of less than 2%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54633</th>\n",
       "      <td>a piezoresistive array armband with reduced number of sensors for hand gesture recognition human machine interfaces hmis are employed in a broad range of applications, spanning from assistive devices for disability to remote manipulation and gaming controllers in this study, a new piezoresistive sensors array armband is proposed for hand gesture recognition the armband encloses only three sensors targeting specific forearm muscles, with the aim to discriminate eight hand movements each sensor is made by a force-sensitive resistor fsr with a dedicated mechanical coupler and is designed to sense muscle swelling during contraction the armband is designed to be easily wearable and adjustable for any user and was tested on 10 volunteers hand gestures are classified by means of different machine learning algorithms, and classification performances are assessed applying both, the 10-fold and leave-one-out cross-validations a linear support vector machine provided 96% mean accuracy across all participants ultimately, this classifier was implemented on an arduino platform and allowed successful control for videogames in real-time the low power consumption together with the high level of accuracy suggests the potential of this device for exergames commonly employed for neuromotor rehabilitation the reduced number of sensors makes this hmi also suitable for hand-prosthesis control</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \\\n",
       "9307                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        a machine learning framework to optimize optic nerve electrical stimulation for vision restoration optic nerve electrical stimulation is a promising technique to restore vision in blind subjects machine learning methods can be used to select effective stimulation protocols, but they require a model of the stimulated system to generate enough training data here, we use a convolutional neural network cnn as a model of the ventral visual stream a genetic algorithm drives the activation of the units in a layer of the cnn representing a cortical region toward a desired pattern, by refining the activation imposed at a layer representing the optic nerve to simulate the pattern of activation elicited by the sites of an electrode array, a simple point-source model was introduced and its optimization process was investigated for static and dynamic scenes psychophysical data confirm that our stimulation evolution framework produces results compatible with natural vision machine learning approaches could become a very powerful tool to optimize and personalize neuroprosthetic systems   \n",
       "46246                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        elbow angle generation during activities of daily living using a submovement prediction model the present study aimed to develop a realistic model for the generation of human activities of daily living adl movements the angular profiles of the elbow joint during functional adl tasks such as eating and drinking were generated by a submovement-based closed-loop model first, the adl movements recorded from three human participants were broken down into logical phases, and each phase was decomposed into submovement components three separate artificial neural networks were trained to learn the submovement parameters and were then incorporated into a closed-loop model with error correction ability the model was able to predict angular trajectories of human adl movements with target access rate = 100%, vaf = 989%, and nrmse = 47% relative to the actual trajectories in addition, the model can be used to provide the desired target for practical trajectory planning in rehabilitation systems such as functional electrical stimulation, robot therapy, brain-computer interface, and prosthetic devices   \n",
       "105929  influence of multiple dynamic factors on the performance of myoelectric pattern recognition hand motion classification using surface electromyogram emg signals has been widely studied for the control of powered prosthetics in laboratory conditions however, clinical applicability has been limited, as imposed by factors like electrodes shift, variations in the contraction force levels, forearm rotation angles, change of limb position and many other factors that all affect the emg pattern recognition performance while the impact of several of these factors on emg parameter estimation and pattern recognition has been considered individually in previous studies, a minimum number of experiments were reported to study the influence of multiple dynamic factors in this paper, we investigate the combined effect of varying forearm rotation angles and contraction force levels on the robustness of emg pattern recognition, while utilizing different time-and-frequency based feature extraction methods the emg pattern recognition system has been validated on a set of 11 subjects ten intact-limbed and one bilateral transradial amputee performing six classes of hand motions, each with three different force levels, each at three different forearm rotation angles, with six emg electrodes plus an accelerometer on the subjectsforearm our results suggest that the performance of the learning algorithms can be improved with the time-dependent power spectrum descriptors td-psd utilized in our experiments, with average classification accuracies of up to 90% across all subjects, force levels, and forearm rotation angles    \n",
       "166141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              classification of finger activation for use in a robotic prosthesis arm hand amputees would highly benefit from a robotic prosthesis, which would allow the movement of a number of fingers in this paper we propose using the electromyographic signals recorded by two pairs of electrodes placed over the arm for operating such prosthesis multiple features from these signals are extracted whence the most relevant features are selected by a genetic algorithm as inputs for a simple classifier this method results in a probability of error of less than 2%   \n",
       "54633                                                                                                                                                                                                                                      a piezoresistive array armband with reduced number of sensors for hand gesture recognition human machine interfaces hmis are employed in a broad range of applications, spanning from assistive devices for disability to remote manipulation and gaming controllers in this study, a new piezoresistive sensors array armband is proposed for hand gesture recognition the armband encloses only three sensors targeting specific forearm muscles, with the aim to discriminate eight hand movements each sensor is made by a force-sensitive resistor fsr with a dedicated mechanical coupler and is designed to sense muscle swelling during contraction the armband is designed to be easily wearable and adjustable for any user and was tested on 10 volunteers hand gestures are classified by means of different machine learning algorithms, and classification performances are assessed applying both, the 10-fold and leave-one-out cross-validations a linear support vector machine provided 96% mean accuracy across all participants ultimately, this classifier was implemented on an arduino platform and allowed successful control for videogames in real-time the low power consumption together with the high level of accuracy suggests the potential of this device for exergames commonly employed for neuromotor rehabilitation the reduced number of sensors makes this hmi also suitable for hand-prosthesis control   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "9307          0       0       0           0          0        0       0   \n",
       "46246         0       0       0           0          0        0       0   \n",
       "105929        0       0       0           0          0        0       0   \n",
       "166141        0       0       0           0          0        0       0   \n",
       "54633         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "9307             0            0         0           0        0       0   \n",
       "46246            0            0         0           0        0       0   \n",
       "105929           0            0         0           0        0       0   \n",
       "166141           0            0         0           0        0       0   \n",
       "54633            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "9307             0             0           0            0         0   \n",
       "46246            0             0           0            0         0   \n",
       "105929           0             0           0            0         0   \n",
       "166141           0             0           0            0         0   \n",
       "54633            0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "9307            0            0           0            0           0   \n",
       "46246           0            0           0            0           0   \n",
       "105929          0            0           0            0           0   \n",
       "166141          0            0           0            0           0   \n",
       "54633           0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "9307              0          1            0        0         0          0   \n",
       "46246             0          0            0        0         0          0   \n",
       "105929            0          0            0        0         0          0   \n",
       "166141            0          0            0        0         0          0   \n",
       "54633             0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "9307         0        0         0          0        0       0         0   \n",
       "46246        0        0         0          0        0       0         0   \n",
       "105929       0        0         0          0        0       0         0   \n",
       "166141       0        0         0          0        0       0         0   \n",
       "54633        0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "9307            1        0           0         0        0        0       0   \n",
       "46246           1        0           0         0        0        0       0   \n",
       "105929          0        0           0         0        0        0       0   \n",
       "166141          0        0           0         0        0        0       0   \n",
       "54633           1        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \\\n",
       "9307             0         0       0            0           0        1   \n",
       "46246            0         0       0            0           0        0   \n",
       "105929           0         0       0            0           0        0   \n",
       "166141           0         0       0            0           0        0   \n",
       "54633            0         0       0            0           0        0   \n",
       "\n",
       "       haem_text obs_text renal_text ackd_text paeds_text dent_text  \\\n",
       "9307           0        0          0         0          0         0   \n",
       "46246          0        0          0         0          0         0   \n",
       "105929         0        0          0         0          0         0   \n",
       "166141         0        0          0         0          0         0   \n",
       "54633          0        0          0         0          0         0   \n",
       "\n",
       "       audio_text bci_text prosth_text  \n",
       "9307            0        0           1  \n",
       "46246           0        0           1  \n",
       "105929          0        0           1  \n",
       "166141          0        0           1  \n",
       "54633           0        0           1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['prosth_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5ecf990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34109, '1': 70})\n"
     ]
    }
   ],
   "source": [
    "## ASSISTIVE DEVICE CONTROL / assist\n",
    "\n",
    "## text\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"wheelchair\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"scooter\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"mobility device\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"assistive device\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"exoskeleton\"), \"1\", \"0\")\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['assist_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bca0a02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>bci_text</th>\n",
       "      <th>prosth_text</th>\n",
       "      <th>assist_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11103</th>\n",
       "      <td>assist-as-needed exoskeleton for hand joint rehabilitation based on muscle effort detection robotic-assisted systems have gained significant traction in post-stroke therapies to support rehabilitation, since these systems can provide high-intensity and high-frequency treatment while allowing accurate motion-control over the patients progress in this paper, we tackle how to provide active support through a robotic-assisted exoskeleton by developing a novel closed-loop architecture that continually measures electromyographic signals emg, in order to adjust the assistance given by the exoskeleton we used emg signals acquired from four patients with post-stroke hand impairments for training machine learning models used to characterize muscle effort by classifying three muscular condition levels based on contraction strength, co-activation, and muscular activation measurements the proposed closed-loop system takes into account the emg muscle effort to modulate the exoskeleton velocity during the rehabilitation therapy experimental results indicate the maximum variation on velocity was 07 mm/s, while the proposed control system effectively modulated the movements of the exoskeleton based on the emg readings, keeping a reference tracking error &lt;5%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101488</th>\n",
       "      <td>role of muscle synergies in real-time classification of upper limb motions using extreme learning machines myoelectric signals offer significant insights in interpreting the motion intention and extent of effort involved in performing a movement, with application in prostheses, orthosis and exoskeletons feature extraction plays a vital role, and follows two approaches: emg and synergy features more recently, muscle synergy based features are being increasingly explored, since it simplifies dimensionality of control, and are considered to be more robust to signal variations another important aspect in a myoelectrically controlled devices is the learning capability and speed of performance for online decoding extreme learning machine elm is a relatively new neural-network based learning algorithm: its performance hasnt been explored in the context of online control, which is a more reliable measure compared to offline analysis to this purpose we aim at focusing our investigation on a myoelectric-based interface which is able to identify and online classify, upper limb motions involving shoulder and elbow the main objective is to compare the performance of the decoder trained using elm, for two different features: emg and synergy features</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57917</th>\n",
       "      <td>gait phase classification and assist torque prediction for a lower limb exoskeleton system using kernel recursive least-squares method the gait phase classification method is a key technique to control an exoskeleton robot different people have different gait features while wearing an exoskeleton robot due to the gap between the exoskeleton and the wearer and their operation habits, such as the correspondence between the joint angle and the moment at which the foot contacts the ground, the amplitude of the joint angle and others in order to enhance the performance of the gait phase classification in an exoskeleton robot using only the angle of hip and knee joints, a kernel recursive least-squares krls algorithm is introduced to build a gait phase classification model we also build an assist torque predictor based on the krls algorithm in this work considering the adaptation of unique gait features in this paper, we evaluate the classification performance of the krls model by comparing with two other commonly used gait recognition methods-the multi-layer perceptron neural network mlpnn method and the support vector machine svm algorithm in this experiment, the training and testing datasets for the models built by krls, mlpnn and svm were collected from 10 healthy volunteers the gait data are collected from the exoskeleton robot that we designed rather than collected from the human body these data depict the human-robot coupling gait that includes unique gait features the krls classification results are in average 3% higher than mlpnn and svm the testing average accuracy of krls is about 86% the prediction results of krls are twice as good as mlpnn in assist torque prediction experiments the krls performs in a good, stable, and robust way and shows model generalization abilities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39334</th>\n",
       "      <td>continuous estimation of knee joint angle based on surface electromyography using a long short-term memory neural network and time-advanced feature continuous joint angle estimation based on a surface electromyography semg signal can be used to improve the man-machine coordination performance of the exoskeleton in this study, we proposed a time-advanced feature and utilized long short-term memory lstm with a root mean square rms feature and its time-advanced feature rmstaf; collectively referred to as rrtaf of semg to estimate the knee joint angle to evaluate the effect of joint angle estimation, we used root mean square error rmse and cross-correlation coefficient &lt;i&gt;ρ&lt;/i&gt; between the estimated angle and actual angle we also compared three methods ie, lstm using rms, bpnn back propagation neural network using rrtaf, and bpnn using rms with lstm using rrtaf to highlight its good performance five healthy subjects participated in the experiment and their eight muscle ie, rectus femoris rf, biceps femoris bf, semitendinosus st, gracilis gc, semimembranosus sm, sartorius sr, medial gastrocnemius mg, and tibialis anterior ta semg signals were taken as algorithm inputs moreover, the knee joint angles were used as target values the experimental results showed that, compared with lstm using rms, bpnn using rrtaf, and bpnn using rms, the average rmse values of lstm using rrtaf were respectively reduced by 857%, 4662%, and 6869%, whereas the average &lt;i&gt;ρ&lt;/i&gt; values were respectively increased by 031%, 415%, and 1835% the results demonstrated that lstm using rrtaf, which contained the time-advanced feature, had better performance for estimating the knee joint motion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59380</th>\n",
       "      <td>determining user intent of partly dynamic shoulder tasks in individuals with chronic stroke using pattern recognition stroke remains the leading cause of long-term disability in the us although therapy can achieve limited improvement of paretic arm use and performance, weakness and abnormal muscle synergies-which cause unintentional elbow, wrist, and finger flexion during shoulder abduction-contribute significantly to limb disuse and compound rehabilitation efforts emerging wearable exoskeleton technology could provide powered abduction support for the paretic arm, but requires a clinically feasible, robust control scheme capable of differentiating multiple shoulder degrees-of-freedom this study examines whether pattern recognition of sensor data can accurately identify user intent for 9 combinations of 1- and 2- degree-of-freedom shoulder tasks participants with stroke n = 12 used their paretic and non-paretic arms, and healthy controls n = 12 used their dominant arm to complete tasks on a lab-based robot involving combinations of abduction, adduction, and internal and external rotation of the shoulder we examined the effect of arm paretic, non-paretic, load level 25% vs 50% maximal voluntary torque, and dataset electromyography, load cell, or combined on classifier performance results suggest that paretic arm, lower load levels, and using load cell or emg data alone reduced classifier accuracy however, this method still shows promise further work will examine classifier-user interaction during active control of a robotic device and optimization/minimization of sensors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55778</th>\n",
       "      <td>prediction of plantar forces during gait using wearable sensors and deep neural networks&lt;sup&gt;&lt;/sup&gt; to enable on-time and high-fidelity lower-limb exoskeleton control, it is effective to predict the future human motion from the observed status in this research, we propose a novel method to predict future plantar force during the gait using imu and plantar sensors deep neural networks dnn are used to learn the non-linear relationship between the measured sensor data and the future plantar force data using the trained network, we can predict the plantar force not only during walking but also at the start and end of walking in the experiments, the performance of the proposed method is confirmed for different prediction time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63793</th>\n",
       "      <td>sub-optimally solving actuator redundancy in a hybrid neuroprosthetic system with a multi-layer neural network structure functional electrical stimulation fes has recently been proposed as a supplementary torque assist in lower-limb powered exoskeletons for persons with paraplegia in the combined system, also known as a hybrid neuroprosthesis, both fes-assist and the exoskeleton act to generate lower-limb torques to achieve standing and walking functions due to this actuator redundancy, we are motivated to optimally allocate fes-assist and exoskeleton torque based on a performance index that penalizes fes overuse to minimize muscle fatigue while also minimizing regulation or tracking errors traditional optimal control approaches need a system model to optimize; however, it is often difficult to formulate a musculoskeletal model that accurately predicts muscle responses due to fes in this paper, we use a novel identification and control structure that contains a recurrent neural network rnn and several feedforward neural networks fnns the rnn is trained by supervised learning to identify the system dynamics, while the fnns are trained by a reinforcement learning method to provide sub-optimal control actions the output layer of each fnn has its unique activation functions, so that the asymmetric constraint of fes and the symmetric constraint of exoskeleton motor control input can be realized this new structure is experimentally validated on a seated human participant using a single joint hybrid neuroprosthesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36564</th>\n",
       "      <td>elbow movement estimation based on emg with narx neural networks the use of the electrical activity from the muscles may provide a natural way to control exoskeletons or other robotic devices seamlessly the major challenges to achieve this goal are human motor redundancy and surface electromyography semg variability the goal of this work is to find a feature extraction and classification procedures to estimate accurately elbow angular trajectory by means of a narx neural network the processing time-step should be small enough to make it feasible its further use for online control of an exoskeleton in order to do so we analysed the biceps and triceps brachii data from an elbow flexo-extension coincident timing task performed in the horizontal plane the semg data was pre-processed and its energy was divided in five frequency intervals that were fed to a nonlinear auto regressive with exogenous inputs narx neural network the estimated angular trajectory was compared with the measured one showing a high correlation between them and a rmse error maximum of 7 degrees the procedure presented here shows a reasonably good estimation that, after training, allows real-time implementation in addition, the results are encouraging to include more complex tasks including the shoulder joint</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101114</th>\n",
       "      <td>pso-svm-based online locomotion mode identification for rehabilitation robotic exoskeletons locomotion mode identification is essential for the control of a robotic rehabilitation exoskeletons this paper proposes an online support vector machine svm optimized by particle swarm optimization pso to identify different locomotion modes to realize a smooth and automatic locomotion transition a pso algorithm is used to obtain the optimal parameters of svm for a better overall performance signals measured by the foot pressure sensors integrated in the insoles of wearable shoes and the mems-based attitude and heading reference systems ahrs attached on the shoes and shanks of leg segments are fused together as the input information of svm based on the chosen window whose size is 200 ms with sampling frequency of 40 hz, a three-layer wavelet packet analysis wpa is used for feature extraction, after which, the kernel principal component analysis kpca is utilized to reduce the dimension of the feature set to reduce computation cost of the svm since the signals are from two types of different sensors, the normalization is conducted to scale the input into the interval of 0, 1 five-fold cross validation is adapted to train the classifier, which prevents the classifier over-fitting based on the svm model obtained offline in matlab, an online svm algorithm is constructed for locomotion mode identification experiments are performed for different locomotion modes and experimental results show the effectiveness of the proposed algorithm with an accuracy of 9600% ± 245% to improve its accuracy, majority vote algorithm mva is used for post-processing, with which the identification accuracy is better than 9835% ± 165% the proposed algorithm can be extended and employed in the field of robotic rehabilitation and assistance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65406</th>\n",
       "      <td>assessment of an on-board classifier for activity recognition on an active back-support exoskeleton despite the growing interest, the adoption of industrial exoskeletons may still be held back by technical limitations to enhance versatility and promote adoption, one aspect of interest could be represented by the potential of active and quasi-passive devices to automatically distinguish different activities and adjust their assistive profiles accordingly this contribution focuses on an active back-support exoskeleton and extends previous work proposing the use of a support vector machine to classify walking, bending and standing thanks to the introduction of a new feature-forearm muscle activity-this study shows that it is possible to perform reliable online classification as a consequence, the authors introduce a new hierarchically-structured controller for the exoskeleton under analysis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97097</th>\n",
       "      <td>hand motion recognition based on forearm deformation measured with a distance sensor array studies of upper limb motion analysis using surface electromyogram semg signals measured from the forearm plays an important role in various applications, such as human interfaces for controlling robotic exoskeletons, prosthetic hands, and evaluation of body functions though the semg signals have a lot of information about the activities of the muscles, the signals do not have the activities of the deep layer muscles we focused on forearm deformation, since hand motion brings the muscles, tendons, and skeletons under the skin the reason why we focus is that we believe the forearm deformation delivers information about the activities of deep layer muscles in this paper, we propose a hand motion recognition method based on the forearm deformation measured with a distance sensor array the method uses the support vector machine our method achieved a mean accuracy of 926% for seven hand motions because the accuracy of the pronation and the supination are high, the distance sensor array has the potential to estimate the activities of deep layer muscles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53186</th>\n",
       "      <td>determining the online measurable input variables in human joint moment intelligent prediction based on the hill muscle model &lt;i&gt;introduction&lt;/i&gt;: human joint moment is a critical parameter to rehabilitation assessment and human-robot interaction, which can be predicted using an artificial neural network ann model however, challenge remains as lack of an effective approach to determining the input variables for the ann model in joint moment prediction, which determines the number of input sensors and the complexity of prediction &lt;i&gt;methods&lt;/i&gt;: to address this research gap, this study develops a mathematical model based on the hill muscle model to determining the online input variables of the ann for the prediction of joint moments in this method, the muscle activation, muscle-tendon moment velocity and length in the hill muscle model and muscle-tendon moment arm are translated to the online measurable variables, ie muscle electromyography emg, joint angles and angular velocities of the muscle span to test the predictive ability of these input variables, an ann model is designed and trained to predict joint moments the ann model with the online measurable input variables is tested on the experimental data collected from ten healthy subjects running with the speeds of 2, 3, 4 and 5 m/s on a treadmill the variance accounted for vaf between the predicted and inverse dynamics moment is used to evaluate the prediction accuracy &lt;i&gt;results&lt;/i&gt;: the results suggested that the method can predict joint moments with a higher accuracy mean vaf = 8967±556 % than those obtained by using other joint angles and angular velocities as inputs mean vaf = 8627±66% evaluated by jack-knife cross-validation &lt;i&gt;conclusion&lt;/i&gt;&lt;i&gt;s&lt;/i&gt;: the proposed method provides us with a powerful tool to predict joint moment based on online measurable variables, which establishes the theoretical basis for optimizing the input sensors and detection complexity of the prediction system it may facilitate the research on exoskeleton robot control and real-time gait analysis in motor rehabilitation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9358</th>\n",
       "      <td>continuous gait phase estimation using lstm for robotic transfemoral prosthesis across walking speeds user gait phase estimation plays a key role for the seamless control of the lower-limb robotic assistive devices eg, exoskeletons or prostheses during ambulation to achieve this, several studies have attempted to estimate the gait phase using a thigh or shank angle however, their estimation resulted in some deviation from the actual walking and varied across the walking speeds in this study, we investigated the different setups using for the machine learning approach to obtain more accurate and consistent gait phase estimation for the robotic transfemoral prosthesis over different walking speeds considering the transfemoral prosthetic application, we proposed two different sensor setups: i the angular positions and velocities of both thigh and torso s1 and ii the angular positions and velocities of both thigh and torso, and heel force data s2 the proposed setups and method are experimentally evaluated with three healthy young subjects at four different walking speeds: 05, 10, 15, and 20 m/s both results showed robust and accurate gait phase estimation with respect to the ground truth loss value of s1: 454e-03 vs s2: 470e-03 s1 had the advantage of a simple equipment setup using only two imus, while s2 had the advantage of estimating more accurate heel-strikes than s1 by using additional heel force data the choice between the two sensor setups can depend on the researcherspreference in consideration of the device setup or the focus of the interest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>musclenet: mapping electromyography to kinematic and dynamic biomechanical variables by machine learning &lt;i&gt;objective&lt;/i&gt;this paper proposes machine learning models for mapping surface electromyography semg signals to regression of joint angle, joint velocity, joint acceleration, joint torque, and activation torque&lt;i&gt;approach&lt;/i&gt;the regression models, collectively known as musclenet, take one of four forms: ann forward artificial neural network, rnn recurrent neural network, cnn convolutional neural network, and rcnn recurrent convolutional neural network inspired by conventional biomechanical muscle models, delayed kinematic signals were used along with semg signals as the machine learning models input; specifically, the cnn and rcnn were modeled with novel configurations for these input conditions the modelsinputs contain either raw or filtered semg signals, which allowed evaluation of the filtering capabilities of the models the models were trained using human experimental data and evaluated with different individual data&lt;i&gt;main results&lt;/i&gt;results were compared in terms of regression error using the root-mean-square and model computation delay the results indicate that the rnn with filtered semg signals and rcnn with raw semg signals models, both with delayed kinematic data, can extract underlying motor control information such as joint activation torque or joint angle from semg signals in pick-and-place tasks the cnns and rcnns were able to filter raw semg signals&lt;i&gt;significance&lt;/i&gt;all forms of musclenet were found to map semg signals within 2 ms, fast enough for real-time applications such as the control of exoskeletons or active prostheses the rnn model with filtered semg and delayed kinematic signals is particularly appropriate for applications in musculoskeletal simulation and biomechatronic device control</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130901</th>\n",
       "      <td>filtering essential tremor noise on surface emg based on squared sine wave approximation essential tremor et refers to involuntary movements of a part of the body et patients have serious difficulties in performing daily living activities our ultimate goal is to develop a system that can enable et patients to perform daily living activities we have been developing an exoskeleton robot for et patients we make use of the electromyogram emg signal to control this robot however, the emg signal of et patients contains not only signals from voluntary movements but also noise from involuntary tremors in this paper, we focus on developing a signal processing method to suppress tremor noise present in the surface emg signal the proposed filter detected attenuation ratio by the correlation between the last emg data and one period squared sine wave the filtered emg signals indicated that essential tremor noise of the elbow flexed posture while holding a water-filled bottle was suppressed in addition, voluntary information was less affected by the filter welchs t-value test confirmed that ease of extraction of voluntary movement was increased by the proposed filter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "11103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                assist-as-needed exoskeleton for hand joint rehabilitation based on muscle effort detection robotic-assisted systems have gained significant traction in post-stroke therapies to support rehabilitation, since these systems can provide high-intensity and high-frequency treatment while allowing accurate motion-control over the patients progress in this paper, we tackle how to provide active support through a robotic-assisted exoskeleton by developing a novel closed-loop architecture that continually measures electromyographic signals emg, in order to adjust the assistance given by the exoskeleton we used emg signals acquired from four patients with post-stroke hand impairments for training machine learning models used to characterize muscle effort by classifying three muscular condition levels based on contraction strength, co-activation, and muscular activation measurements the proposed closed-loop system takes into account the emg muscle effort to modulate the exoskeleton velocity during the rehabilitation therapy experimental results indicate the maximum variation on velocity was 07 mm/s, while the proposed control system effectively modulated the movements of the exoskeleton based on the emg readings, keeping a reference tracking error <5%   \n",
       "101488                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    role of muscle synergies in real-time classification of upper limb motions using extreme learning machines myoelectric signals offer significant insights in interpreting the motion intention and extent of effort involved in performing a movement, with application in prostheses, orthosis and exoskeletons feature extraction plays a vital role, and follows two approaches: emg and synergy features more recently, muscle synergy based features are being increasingly explored, since it simplifies dimensionality of control, and are considered to be more robust to signal variations another important aspect in a myoelectrically controlled devices is the learning capability and speed of performance for online decoding extreme learning machine elm is a relatively new neural-network based learning algorithm: its performance hasnt been explored in the context of online control, which is a more reliable measure compared to offline analysis to this purpose we aim at focusing our investigation on a myoelectric-based interface which is able to identify and online classify, upper limb motions involving shoulder and elbow the main objective is to compare the performance of the decoder trained using elm, for two different features: emg and synergy features   \n",
       "57917                                                                                                                                                                                                                                                                                             gait phase classification and assist torque prediction for a lower limb exoskeleton system using kernel recursive least-squares method the gait phase classification method is a key technique to control an exoskeleton robot different people have different gait features while wearing an exoskeleton robot due to the gap between the exoskeleton and the wearer and their operation habits, such as the correspondence between the joint angle and the moment at which the foot contacts the ground, the amplitude of the joint angle and others in order to enhance the performance of the gait phase classification in an exoskeleton robot using only the angle of hip and knee joints, a kernel recursive least-squares krls algorithm is introduced to build a gait phase classification model we also build an assist torque predictor based on the krls algorithm in this work considering the adaptation of unique gait features in this paper, we evaluate the classification performance of the krls model by comparing with two other commonly used gait recognition methods-the multi-layer perceptron neural network mlpnn method and the support vector machine svm algorithm in this experiment, the training and testing datasets for the models built by krls, mlpnn and svm were collected from 10 healthy volunteers the gait data are collected from the exoskeleton robot that we designed rather than collected from the human body these data depict the human-robot coupling gait that includes unique gait features the krls classification results are in average 3% higher than mlpnn and svm the testing average accuracy of krls is about 86% the prediction results of krls are twice as good as mlpnn in assist torque prediction experiments the krls performs in a good, stable, and robust way and shows model generalization abilities   \n",
       "39334                                                                                                                                                                                                                                                                                                                                                                                                                         continuous estimation of knee joint angle based on surface electromyography using a long short-term memory neural network and time-advanced feature continuous joint angle estimation based on a surface electromyography semg signal can be used to improve the man-machine coordination performance of the exoskeleton in this study, we proposed a time-advanced feature and utilized long short-term memory lstm with a root mean square rms feature and its time-advanced feature rmstaf; collectively referred to as rrtaf of semg to estimate the knee joint angle to evaluate the effect of joint angle estimation, we used root mean square error rmse and cross-correlation coefficient <i>ρ</i> between the estimated angle and actual angle we also compared three methods ie, lstm using rms, bpnn back propagation neural network using rrtaf, and bpnn using rms with lstm using rrtaf to highlight its good performance five healthy subjects participated in the experiment and their eight muscle ie, rectus femoris rf, biceps femoris bf, semitendinosus st, gracilis gc, semimembranosus sm, sartorius sr, medial gastrocnemius mg, and tibialis anterior ta semg signals were taken as algorithm inputs moreover, the knee joint angles were used as target values the experimental results showed that, compared with lstm using rms, bpnn using rrtaf, and bpnn using rms, the average rmse values of lstm using rrtaf were respectively reduced by 857%, 4662%, and 6869%, whereas the average <i>ρ</i> values were respectively increased by 031%, 415%, and 1835% the results demonstrated that lstm using rrtaf, which contained the time-advanced feature, had better performance for estimating the knee joint motion   \n",
       "59380                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                determining user intent of partly dynamic shoulder tasks in individuals with chronic stroke using pattern recognition stroke remains the leading cause of long-term disability in the us although therapy can achieve limited improvement of paretic arm use and performance, weakness and abnormal muscle synergies-which cause unintentional elbow, wrist, and finger flexion during shoulder abduction-contribute significantly to limb disuse and compound rehabilitation efforts emerging wearable exoskeleton technology could provide powered abduction support for the paretic arm, but requires a clinically feasible, robust control scheme capable of differentiating multiple shoulder degrees-of-freedom this study examines whether pattern recognition of sensor data can accurately identify user intent for 9 combinations of 1- and 2- degree-of-freedom shoulder tasks participants with stroke n = 12 used their paretic and non-paretic arms, and healthy controls n = 12 used their dominant arm to complete tasks on a lab-based robot involving combinations of abduction, adduction, and internal and external rotation of the shoulder we examined the effect of arm paretic, non-paretic, load level 25% vs 50% maximal voluntary torque, and dataset electromyography, load cell, or combined on classifier performance results suggest that paretic arm, lower load levels, and using load cell or emg data alone reduced classifier accuracy however, this method still shows promise further work will examine classifier-user interaction during active control of a robotic device and optimization/minimization of sensors   \n",
       "55778                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  prediction of plantar forces during gait using wearable sensors and deep neural networks<sup></sup> to enable on-time and high-fidelity lower-limb exoskeleton control, it is effective to predict the future human motion from the observed status in this research, we propose a novel method to predict future plantar force during the gait using imu and plantar sensors deep neural networks dnn are used to learn the non-linear relationship between the measured sensor data and the future plantar force data using the trained network, we can predict the plantar force not only during walking but also at the start and end of walking in the experiments, the performance of the proposed method is confirmed for different prediction time   \n",
       "63793                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               sub-optimally solving actuator redundancy in a hybrid neuroprosthetic system with a multi-layer neural network structure functional electrical stimulation fes has recently been proposed as a supplementary torque assist in lower-limb powered exoskeletons for persons with paraplegia in the combined system, also known as a hybrid neuroprosthesis, both fes-assist and the exoskeleton act to generate lower-limb torques to achieve standing and walking functions due to this actuator redundancy, we are motivated to optimally allocate fes-assist and exoskeleton torque based on a performance index that penalizes fes overuse to minimize muscle fatigue while also minimizing regulation or tracking errors traditional optimal control approaches need a system model to optimize; however, it is often difficult to formulate a musculoskeletal model that accurately predicts muscle responses due to fes in this paper, we use a novel identification and control structure that contains a recurrent neural network rnn and several feedforward neural networks fnns the rnn is trained by supervised learning to identify the system dynamics, while the fnns are trained by a reinforcement learning method to provide sub-optimal control actions the output layer of each fnn has its unique activation functions, so that the asymmetric constraint of fes and the symmetric constraint of exoskeleton motor control input can be realized this new structure is experimentally validated on a seated human participant using a single joint hybrid neuroprosthesis   \n",
       "36564                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             elbow movement estimation based on emg with narx neural networks the use of the electrical activity from the muscles may provide a natural way to control exoskeletons or other robotic devices seamlessly the major challenges to achieve this goal are human motor redundancy and surface electromyography semg variability the goal of this work is to find a feature extraction and classification procedures to estimate accurately elbow angular trajectory by means of a narx neural network the processing time-step should be small enough to make it feasible its further use for online control of an exoskeleton in order to do so we analysed the biceps and triceps brachii data from an elbow flexo-extension coincident timing task performed in the horizontal plane the semg data was pre-processed and its energy was divided in five frequency intervals that were fed to a nonlinear auto regressive with exogenous inputs narx neural network the estimated angular trajectory was compared with the measured one showing a high correlation between them and a rmse error maximum of 7 degrees the procedure presented here shows a reasonably good estimation that, after training, allows real-time implementation in addition, the results are encouraging to include more complex tasks including the shoulder joint   \n",
       "101114                                                                                                                                                                                                                                                                   pso-svm-based online locomotion mode identification for rehabilitation robotic exoskeletons locomotion mode identification is essential for the control of a robotic rehabilitation exoskeletons this paper proposes an online support vector machine svm optimized by particle swarm optimization pso to identify different locomotion modes to realize a smooth and automatic locomotion transition a pso algorithm is used to obtain the optimal parameters of svm for a better overall performance signals measured by the foot pressure sensors integrated in the insoles of wearable shoes and the mems-based attitude and heading reference systems ahrs attached on the shoes and shanks of leg segments are fused together as the input information of svm based on the chosen window whose size is 200 ms with sampling frequency of 40 hz, a three-layer wavelet packet analysis wpa is used for feature extraction, after which, the kernel principal component analysis kpca is utilized to reduce the dimension of the feature set to reduce computation cost of the svm since the signals are from two types of different sensors, the normalization is conducted to scale the input into the interval of 0, 1 five-fold cross validation is adapted to train the classifier, which prevents the classifier over-fitting based on the svm model obtained offline in matlab, an online svm algorithm is constructed for locomotion mode identification experiments are performed for different locomotion modes and experimental results show the effectiveness of the proposed algorithm with an accuracy of 9600% ± 245% to improve its accuracy, majority vote algorithm mva is used for post-processing, with which the identification accuracy is better than 9835% ± 165% the proposed algorithm can be extended and employed in the field of robotic rehabilitation and assistance    \n",
       "65406                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        assessment of an on-board classifier for activity recognition on an active back-support exoskeleton despite the growing interest, the adoption of industrial exoskeletons may still be held back by technical limitations to enhance versatility and promote adoption, one aspect of interest could be represented by the potential of active and quasi-passive devices to automatically distinguish different activities and adjust their assistive profiles accordingly this contribution focuses on an active back-support exoskeleton and extends previous work proposing the use of a support vector machine to classify walking, bending and standing thanks to the introduction of a new feature-forearm muscle activity-this study shows that it is possible to perform reliable online classification as a consequence, the authors introduce a new hierarchically-structured controller for the exoskeleton under analysis   \n",
       "97097                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           hand motion recognition based on forearm deformation measured with a distance sensor array studies of upper limb motion analysis using surface electromyogram semg signals measured from the forearm plays an important role in various applications, such as human interfaces for controlling robotic exoskeletons, prosthetic hands, and evaluation of body functions though the semg signals have a lot of information about the activities of the muscles, the signals do not have the activities of the deep layer muscles we focused on forearm deformation, since hand motion brings the muscles, tendons, and skeletons under the skin the reason why we focus is that we believe the forearm deformation delivers information about the activities of deep layer muscles in this paper, we propose a hand motion recognition method based on the forearm deformation measured with a distance sensor array the method uses the support vector machine our method achieved a mean accuracy of 926% for seven hand motions because the accuracy of the pronation and the supination are high, the distance sensor array has the potential to estimate the activities of deep layer muscles   \n",
       "53186   determining the online measurable input variables in human joint moment intelligent prediction based on the hill muscle model <i>introduction</i>: human joint moment is a critical parameter to rehabilitation assessment and human-robot interaction, which can be predicted using an artificial neural network ann model however, challenge remains as lack of an effective approach to determining the input variables for the ann model in joint moment prediction, which determines the number of input sensors and the complexity of prediction <i>methods</i>: to address this research gap, this study develops a mathematical model based on the hill muscle model to determining the online input variables of the ann for the prediction of joint moments in this method, the muscle activation, muscle-tendon moment velocity and length in the hill muscle model and muscle-tendon moment arm are translated to the online measurable variables, ie muscle electromyography emg, joint angles and angular velocities of the muscle span to test the predictive ability of these input variables, an ann model is designed and trained to predict joint moments the ann model with the online measurable input variables is tested on the experimental data collected from ten healthy subjects running with the speeds of 2, 3, 4 and 5 m/s on a treadmill the variance accounted for vaf between the predicted and inverse dynamics moment is used to evaluate the prediction accuracy <i>results</i>: the results suggested that the method can predict joint moments with a higher accuracy mean vaf = 8967±556 % than those obtained by using other joint angles and angular velocities as inputs mean vaf = 8627±66% evaluated by jack-knife cross-validation <i>conclusion</i><i>s</i>: the proposed method provides us with a powerful tool to predict joint moment based on online measurable variables, which establishes the theoretical basis for optimizing the input sensors and detection complexity of the prediction system it may facilitate the research on exoskeleton robot control and real-time gait analysis in motor rehabilitation   \n",
       "9358                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         continuous gait phase estimation using lstm for robotic transfemoral prosthesis across walking speeds user gait phase estimation plays a key role for the seamless control of the lower-limb robotic assistive devices eg, exoskeletons or prostheses during ambulation to achieve this, several studies have attempted to estimate the gait phase using a thigh or shank angle however, their estimation resulted in some deviation from the actual walking and varied across the walking speeds in this study, we investigated the different setups using for the machine learning approach to obtain more accurate and consistent gait phase estimation for the robotic transfemoral prosthesis over different walking speeds considering the transfemoral prosthetic application, we proposed two different sensor setups: i the angular positions and velocities of both thigh and torso s1 and ii the angular positions and velocities of both thigh and torso, and heel force data s2 the proposed setups and method are experimentally evaluated with three healthy young subjects at four different walking speeds: 05, 10, 15, and 20 m/s both results showed robust and accurate gait phase estimation with respect to the ground truth loss value of s1: 454e-03 vs s2: 470e-03 s1 had the advantage of a simple equipment setup using only two imus, while s2 had the advantage of estimating more accurate heel-strikes than s1 by using additional heel force data the choice between the two sensor setups can depend on the researcherspreference in consideration of the device setup or the focus of the interest   \n",
       "7744                                                                                                                                                                                                                                                        musclenet: mapping electromyography to kinematic and dynamic biomechanical variables by machine learning <i>objective</i>this paper proposes machine learning models for mapping surface electromyography semg signals to regression of joint angle, joint velocity, joint acceleration, joint torque, and activation torque<i>approach</i>the regression models, collectively known as musclenet, take one of four forms: ann forward artificial neural network, rnn recurrent neural network, cnn convolutional neural network, and rcnn recurrent convolutional neural network inspired by conventional biomechanical muscle models, delayed kinematic signals were used along with semg signals as the machine learning models input; specifically, the cnn and rcnn were modeled with novel configurations for these input conditions the modelsinputs contain either raw or filtered semg signals, which allowed evaluation of the filtering capabilities of the models the models were trained using human experimental data and evaluated with different individual data<i>main results</i>results were compared in terms of regression error using the root-mean-square and model computation delay the results indicate that the rnn with filtered semg signals and rcnn with raw semg signals models, both with delayed kinematic data, can extract underlying motor control information such as joint activation torque or joint angle from semg signals in pick-and-place tasks the cnns and rcnns were able to filter raw semg signals<i>significance</i>all forms of musclenet were found to map semg signals within 2 ms, fast enough for real-time applications such as the control of exoskeletons or active prostheses the rnn model with filtered semg and delayed kinematic signals is particularly appropriate for applications in musculoskeletal simulation and biomechatronic device control   \n",
       "130901                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        filtering essential tremor noise on surface emg based on squared sine wave approximation essential tremor et refers to involuntary movements of a part of the body et patients have serious difficulties in performing daily living activities our ultimate goal is to develop a system that can enable et patients to perform daily living activities we have been developing an exoskeleton robot for et patients we make use of the electromyogram emg signal to control this robot however, the emg signal of et patients contains not only signals from voluntary movements but also noise from involuntary tremors in this paper, we focus on developing a signal processing method to suppress tremor noise present in the surface emg signal the proposed filter detected attenuation ratio by the correlation between the last emg data and one period squared sine wave the filtered emg signals indicated that essential tremor noise of the elbow flexed posture while holding a water-filled bottle was suppressed in addition, voluntary information was less affected by the filter welchs t-value test confirmed that ease of extraction of voluntary movement was increased by the proposed filter   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "11103         0       0       0           0          0        0       0   \n",
       "101488        0       0       0           0          0        0       0   \n",
       "57917         0       0       0           0          0        0       0   \n",
       "39334         0       0       0           0          0        0       0   \n",
       "59380         0       0       0           0          0        0       0   \n",
       "55778         0       0       0           0          0        0       0   \n",
       "63793         0       0       0           0          0        0       0   \n",
       "36564         0       0       0           0          0        0       0   \n",
       "101114        0       0       0           0          0        0       0   \n",
       "65406         0       0       0           0          0        0       0   \n",
       "97097         0       0       0           0          0        0       0   \n",
       "53186         0       0       0           0          0        0       0   \n",
       "9358          0       0       0           0          0        0       0   \n",
       "7744          0       0       0           0          0        0       0   \n",
       "130901        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "11103            0            0         0           0        0       0   \n",
       "101488           0            0         0           0        0       0   \n",
       "57917            0            0         0           0        0       0   \n",
       "39334            0            0         0           0        0       0   \n",
       "59380            0            0         0           0        0       0   \n",
       "55778            0            0         0           0        0       0   \n",
       "63793            0            0         0           0        0       0   \n",
       "36564            0            0         0           0        0       0   \n",
       "101114           0            0         0           0        0       0   \n",
       "65406            0            0         0           0        0       0   \n",
       "97097            0            0         0           0        0       0   \n",
       "53186            0            0         0           0        0       0   \n",
       "9358             0            0         0           0        0       0   \n",
       "7744             0            0         0           0        0       0   \n",
       "130901           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "11103            0             0           0            0         0   \n",
       "101488           0             0           0            0         0   \n",
       "57917            0             0           0            0         0   \n",
       "39334            0             0           0            0         0   \n",
       "59380            0             0           0            0         0   \n",
       "55778            0             0           0            0         0   \n",
       "63793            0             0           0            0         0   \n",
       "36564            0             0           0            0         0   \n",
       "101114           0             0           0            0         0   \n",
       "65406            0             0           0            0         0   \n",
       "97097            0             0           0            0         0   \n",
       "53186            0             0           0            0         0   \n",
       "9358             0             0           0            0         0   \n",
       "7744             0             0           0            0         0   \n",
       "130901           0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "11103           0            0           0            0           0   \n",
       "101488          0            0           0            0           0   \n",
       "57917           0            0           0            0           0   \n",
       "39334           0            0           0            0           0   \n",
       "59380           0            0           0            0           0   \n",
       "55778           0            0           0            0           0   \n",
       "63793           0            0           0            0           0   \n",
       "36564           0            0           0            0           0   \n",
       "101114          0            0           0            0           0   \n",
       "65406           0            0           0            0           0   \n",
       "97097           0            0           0            0           0   \n",
       "53186           0            0           0            0           0   \n",
       "9358            0            0           0            0           0   \n",
       "7744            0            0           0            0           0   \n",
       "130901          0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "11103             0          0            0        0         0          0   \n",
       "101488            0          0            0        0         0          0   \n",
       "57917             0          0            0        0         0          0   \n",
       "39334             0          0            0        0         0          0   \n",
       "59380             0          0            0        0         0          0   \n",
       "55778             0          0            0        0         0          0   \n",
       "63793             0          0            0        1         0          0   \n",
       "36564             0          0            0        0         0          0   \n",
       "101114            0          0            0        0         0          0   \n",
       "65406             0          0            0        0         0          0   \n",
       "97097             0          0            0        0         0          0   \n",
       "53186             0          0            0        0         0          0   \n",
       "9358              0          0            0        0         0          0   \n",
       "7744              0          0            0        1         0          0   \n",
       "130901            0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "11103        0        0         0          0        0       0         0   \n",
       "101488       0        0         0          0        0       0         0   \n",
       "57917        0        0         0          0        0       0         0   \n",
       "39334        1        0         0          0        0       0         0   \n",
       "59380        0        0         0          0        0       0         0   \n",
       "55778        0        0         0          0        0       0         0   \n",
       "63793        0        0         0          0        0       0         0   \n",
       "36564        0        0         0          0        0       0         0   \n",
       "101114       0        0         0          0        0       0         0   \n",
       "65406        0        0         0          0        0       0         0   \n",
       "97097        0        0         0          0        0       0         0   \n",
       "53186        0        0         0          0        0       0         0   \n",
       "9358         0        0         0          0        0       0         0   \n",
       "7744         0        0         0          0        0       0         0   \n",
       "130901       0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "11103           0        0           0         0        0        0       0   \n",
       "101488          0        0           0         0        0        0       0   \n",
       "57917           0        0           0         0        0        0       0   \n",
       "39334           0        0           0         0        0        0       0   \n",
       "59380           0        0           0         0        0        0       0   \n",
       "55778           0        0           0         0        0        0       0   \n",
       "63793           1        0           0         0        0        0       0   \n",
       "36564           0        0           0         0        0        0       0   \n",
       "101114          0        0           0         0        0        0       0   \n",
       "65406           0        0           0         0        0        0       0   \n",
       "97097           0        0           0         0        0        0       0   \n",
       "53186           0        0           0         0        0        0       0   \n",
       "9358            0        0           0         0        0        0       0   \n",
       "7744            0        0           0         0        0        0       0   \n",
       "130901          0        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \\\n",
       "11103            0         0       0            0           0        0   \n",
       "101488           0         0       0            0           0        0   \n",
       "57917            0         0       0            0           0        0   \n",
       "39334            0         0       0            0           0        0   \n",
       "59380            0         0       0            0           0        0   \n",
       "55778            0         0       0            0           0        0   \n",
       "63793            0         0       0            0           0        0   \n",
       "36564            0         0       0            0           0        0   \n",
       "101114           0         0       0            0           0        0   \n",
       "65406            0         0       0            0           0        0   \n",
       "97097            0         0       0            0           0        0   \n",
       "53186            0         0       0            0           0        0   \n",
       "9358             0         0       0            0           0        0   \n",
       "7744             0         0       0            0           0        0   \n",
       "130901           0         0       0            0           0        0   \n",
       "\n",
       "       haem_text obs_text renal_text ackd_text paeds_text dent_text  \\\n",
       "11103          0        0          0         0          0         0   \n",
       "101488         0        0          0         0          0         0   \n",
       "57917          0        0          0         0          0         0   \n",
       "39334          0        0          0         0          0         0   \n",
       "59380          0        0          0         0          0         0   \n",
       "55778          0        0          0         0          0         0   \n",
       "63793          0        0          0         0          0         0   \n",
       "36564          0        0          0         0          0         0   \n",
       "101114         0        0          0         0          0         0   \n",
       "65406          0        0          0         0          0         0   \n",
       "97097          0        0          0         0          0         0   \n",
       "53186          0        0          0         0          0         0   \n",
       "9358           0        0          0         0          0         0   \n",
       "7744           0        0          0         0          0         0   \n",
       "130901         0        0          0         0          0         0   \n",
       "\n",
       "       audio_text bci_text prosth_text assist_text  \n",
       "11103           0        0           0           1  \n",
       "101488          0        0           0           1  \n",
       "57917           0        0           0           1  \n",
       "39334           0        0           0           1  \n",
       "59380           0        0           0           1  \n",
       "55778           0        0           0           1  \n",
       "63793           0        0           1           1  \n",
       "36564           0        0           0           1  \n",
       "101114          0        0           0           1  \n",
       "65406           0        0           0           1  \n",
       "97097           0        0           1           1  \n",
       "53186           0        0           0           1  \n",
       "9358            0        0           1           1  \n",
       "7744            0        0           0           1  \n",
       "130901          0        0           0           1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['assist_text']=='1'].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2496863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33591, '1': 588})\n"
     ]
    }
   ],
   "source": [
    "## HOME ACTIVITY, REHABIILTATION / active\n",
    "\n",
    "## text\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activity monitor\"), \"1\", \"0\")\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activity detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activities monitor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activities detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"home environ\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"fall detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"fall monitor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls monitor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"daily activit\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activity classif\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"daily living\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"fall prevent\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls in home\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls at home\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"home sensor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"gait analysis\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"gait detection\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"gait classification\"), \"1\", spec['activity_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['activity_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5c0adcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>urology_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>bci_text</th>\n",
       "      <th>prosth_text</th>\n",
       "      <th>assist_text</th>\n",
       "      <th>activity_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52887</th>\n",
       "      <td>a novel hybrid deep neural network to predict pre-impact fall for older people based on wearable inertial sensors falls in the elderly is a major public health concern due to its high prevalence, serious consequences and heavy burden on the society many falls in older people happen within a very short time, which makes it difficult to predict a fall before it occurs and then to provide protection for the person who is falling the primary objective of this study was to develop deep neural networks for predicting a fall during its initiation and descending but before the body impacts to the ground so that a safety mechanism can be enabled to prevent fall-related injuries we divided the falling process into three stages non-fall, pre-impact fall and fall and developed deep neutral networks to perform three-class classification three deep learning models, convolutional neural network cnn, long short term memory lstm, and a novel hybrid model integrating both convolution and long short term memory convlstm were proposed and evaluated on a large public dataset of various falls and activities of daily living adl acquired with wearable inertial sensors accelerometer and gyroscope fivefold cross validation results showed that the hybrid convlstm model had mean sensitivities of 9315, 9378, and 9600% for non-fall, pre-impact fall and fall, respectively, which were higher than both lstm except the fall class and cnn models convlstm model also showed higher specificities for all three classes 9659, 9449, and 9869% than lstm and cnn models in addition, latency test on a microcontroller unit showed that convlstm model had a short latency of 106 ms, which was much lower than lstm model 315 ms and comparable with cnn model 077 ms high prediction accuracy especially for pre-impact fall and low latency on the microboard indicated that the proposed hybrid convlstm model outperformed both lstm and cnn models these findings suggest that our proposed novel hybrid convlstm model has great potential to be embedded into wearable inertial sensor-based systems to predict pre-impact fall in real-time so that protective devices could be triggered in time to prevent fall-related injuries for older people</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94882</th>\n",
       "      <td>extracting aerobic system dynamics during unsupervised activities of daily living using wearable sensor machine learning models physical activity levels are related through algorithms to the energetic demand, with no information regarding the integrity of the multiple physiological systems involved in the energetic supply longitudinal analysis of the oxygen uptake v̇o&lt;sub&gt;2&lt;/sub&gt; by wearable sensors in realistic settings might permit development of a practical tool for the study of the longitudinal aerobic system dynamics ie, v̇o&lt;sub&gt;2&lt;/sub&gt; kinetics this study evaluated aerobic system dynamics based on predicted v̇o&lt;sub&gt;2&lt;/sub&gt; data obtained from wearable sensors during unsupervised activities of daily living μadl thirteen healthy men performed a laboratory-controlled moderate exercise protocol and were monitored for ≈6 h/day for 4 days μadl data variables derived from hip accelerometer acc&lt;sub&gt;hip&lt;/sub&gt;, heart rate monitor, and respiratory bands during μadl were extracted and processed by a validated random forest regression model to predict v̇o&lt;sub&gt;2&lt;/sub&gt; the aerobic system analysis was based on the frequency-domain analysis of acc&lt;sub&gt;hip&lt;/sub&gt; and predicted v̇o&lt;sub&gt;2&lt;/sub&gt; data obtained during μadl optimal samples for frequency domain analysis constrained to ≤001 hz were selected when acc&lt;sub&gt;hip&lt;/sub&gt; was higher than 005 g at a given frequency ie, participants were active the temporal characteristics of predicted v̇o&lt;sub&gt;2&lt;/sub&gt; data during μadl correlated with the temporal characteristics of measured v̇o&lt;sub&gt;2&lt;/sub&gt; data during laboratory-controlled protocol formula: see text = 082, p &lt; 0001, n = 13 in conclusion, aerobic system dynamics can be investigated during unsupervised activities of daily living by wearable sensors although speculative, these algorithms have the potential to be incorporated into wearable systems for early detection of changes in health status in realistic environments by detecting changes in aerobic response dynamics new &amp; noteworthy the early detection of subclinical aerobic system impairments might be indicative of impaired physiological reserves that impact the capacity for physical activity this study is the first to use wearable sensors in unsupervised activities of daily living in combination with novel machine learning algorithms to investigate the aerobic system dynamics with the potential to contribute to models of functional health status and guide future individualized health care in the normal population</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17351</th>\n",
       "      <td>deep chores: estimating hallmark measures of physical activity using deep learning wrist accelerometers for assessing hallmark measures of physical activity pa are rapidly growing with the advent of smartwatch technology given the growing popularity of wrist-worn accelerometers, there needs to be a rigorous evaluation for recognizing pa type and estimating energy expenditure ee across the lifespan participants 66% women, aged 20-89 yrs performed a battery of 33 daily activities in a standardized laboratory setting while a tri-axial accelerometer collected data from the right wrist a portable metabolic unit was worn to measure metabolic intensity we built deep learning networks to extract spatial and temporal representations from the time-series data, and used them to recognize pa type and estimate ee the deep learning models resulted in high performance; the f1 score was: 082, 081, and 95 for recognizing sedentary, locomotor, and lifestyle activities, respectively the root mean square error was 11 +/-013 for the estimation of ee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116860</th>\n",
       "      <td>dynamic bayesian networks for context-aware fall risk assessment fall incidents among the elderly often occur in the home and can cause serious injuries affecting their independent living this paper presents an approach where data from wearable sensors integrated in a smart home environment is combined using a dynamic bayesian network the smart home environment provides contextual data, obtained from environmental sensors, and contributes to assessing a fall risk probability the evaluation of the developed system is performed through simulation each time step is represented by a single user activity and interacts with a fall sensors located on a mobile device a posterior probability is calculated for each recognized activity or contextual information the output of the system provides a total risk assessment of falling given a response from the fall sensor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125139</th>\n",
       "      <td>impact of study design on development and evaluation of an activity-type classifier methods to classify activity types are often evaluated with an experimental protocol involving prescribed physical activities under confined laboratory conditions, which may not reflect real-life conditions the present study aims to evaluate how study design may impact on classifier performance in real life twenty-eight healthy participants 21-53 yr were asked to wear nine triaxial accelerometers while performing 58 activity types selected to simulate activities in real life for each sensor location, logistic classifiers were trained in subsets of up to 8 activities to distinguish between walking and nonwalking activities and were then evaluated in all 58 activities different weighting factors were used to convert the resulting confusion matrices into an estimation of the confusion matrix as would apply in the real-life setting by creating four different real-life scenarios, as well as one traditional laboratory scenario the sensitivity of a classifier estimated with a traditional laboratory protocol is within the range of estimates derived from real-life scenarios for any body location the specificity, however, was systematically overestimated by the traditional laboratory scenario walking time was systematically overestimated, except for lower back sensor data range: 7-757% in conclusion, classifier performance under confined conditions may not accurately reflect classifier performance in real life future studies that aim to evaluate activity classification methods are warranted to pay special attention to the representativeness of experimental conditions for real-life conditions</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81852</th>\n",
       "      <td>imu-based classification of parkinsons disease from gait: a sensitivity analysis on sensor location and feature selection inertial measurement units imus have a long-lasting popularity in a variety of industrial applications from navigation systems to guidance and robotics their use in clinical practice is now becoming more common, thanks to miniaturization and the ability to integrate on-board computational and decision-support features imu-based gait analysis is a paradigm of this evolving process, and in this study its use for the assessment of parkinsons disease pd is comprehensively analyzed data coming from 25 individuals with different levels of pd symptoms severity and an equal number of age-matched healthy individuals were included into a set of 6 different machine learning ml techniques, processing 18 different configurations of gait parameters taken from 8 imu sensors classification accuracy was calculated for each configuration and ml technique, adding two meta-classifiers based on the results obtained from all individual techniques through majority of voting, with two different weighting schemes average classification accuracy ranged between 63% and 80% among classifiers and increased up to 96% for one meta-classifier configuration configurations based on a statistical preselection process showed the highest average classification accuracy when reducing the number of sensors, features based on the joint range of motion were more accurate than those based on spatio-temporal parameters in particular, best results were obtained with the knee range of motion, calculated with four imus, placed bilaterally the obtained findings provide data-driven evidence on which combination of sensor configurations and classification methods to be used during imu-based gait analysis to grade the severity level of pd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36550</th>\n",
       "      <td>skeleton data pre-processing for human pose recognition using neural network automatic monitoring of daily living activities can greatly improve the possibility of living autonomously for frail individuals pose recognition based on skeleton tracking data is promising for identifying dangerous situations and trigger external intervention or other alarms, while avoiding privacy issues and the need for patient compliance here we present the benefits of pre-processing kinect-recorded skeleton data to limit the several errors produced by the system when the subject is not in ideal tracking conditions the accuracy of our two hidden layers mlp classifier improved from about 82% to over 92% in recognizing actors in four different poses: standing, sitting, lying and dangerous sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>a comparative study of time frequency representation techniques for freeze of gait detection and prediction freezing of gait fog is an impairment that affects the majority of patients in the advanced stages of parkinsons disease pd fog can lead to sudden falls and injuries, negatively impacting the quality of life for the patients and their families rhythmic auditory stimulation ras can be used to help patients recover from fog and resume normal gait ras might be ineffective due to the latency between the start of a fog event, its detection and initialization of ras we propose a system capable of both fog prediction and detection using signals from tri-axial accelerometer sensors that will be useful in initializing ras with minimal latency we compared the performance of several time frequency analysis techniques, including moving windows extracted from the signals, handcrafted features, recurrence plots rp, short time fourier transform stft, discreet wavelet transform dwt and pseudo wigner ville distribution pwvd with deep learning dl based long short term memory lstm and convolutional neural networks cnn we also propose three ensemble network architectures that combine all the time frequency representations and dl architectures experimental results show that our ensemble architectures significantly improve the performance compared with existing techniques we also present the results of applying our method trained on a publicly available dataset to data collected from patients using wearable sensors in collaboration with at still university</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9522</th>\n",
       "      <td>hand tremor detection in videos with cluttered background using neural network based approaches with the increasing prevalence of neurodegenerative diseases, including parkinsons disease, hand tremor detection has become a popular research topic because it helps with the diagnosis and tracking of disease progression conventional hand tremor detection algorithms involved wearable sensors a non-invasive hand tremor detection algorithm using videos as input is desirable but the existing video-based algorithms are sensitive to environmental conditions an algorithm, with the capability of detecting hand tremor from videos with a cluttered background, would allow the videos recorded in a non-research environment to be used clinicians and researchers could use videos collected from patients and participants in their own home environment or standard clinical settings neural network based machine learning architectures provide high accuracy classification results in related fields including hand gesture recognition and body movement detection systems we thus investigated the accuracy of advanced neural network architectures to automatically detect hand tremor in videos with a cluttered background we examined configurations with different sets of features and neural network based classification models we compared the performance of different combinations of features and classification models and then selected the combination which provided the highest accuracy of hand tremor detection we used cross validation to test the accuracy of the trained model predictions the highest classification accuracy for automatically detecting tremor vs non tremor was 806% and this was obtained using convolutional neural network-long short-term memory and features based on measures of frequency and amplitude change</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96342</th>\n",
       "      <td>prediction of oxygen uptake dynamics by machine learning analysis of wearable sensors during activities of daily living currently, oxygen uptake  is the most precise means of investigating aerobic fitness and level of physical activity; however, can only be directly measured in supervised conditions with the advancement of new wearable sensor technologies and data processing approaches, it is possible to accurately infer work rate and predict during activities of daily living adl the main objective of this study was to develop and verify the methods required to predict and investigate the dynamics during adl the variables derived from the wearable sensors were used to create a predictor based on a random forest method the temporal dynamics were assessed by the mean normalized gain amplitude mng obtained from frequency domain analysis the mng provides a means to assess aerobic fitness the predicted during adl was strongly correlated r = 087, p &lt; 0001 with the measured and the prediction bias was 02 ml·min&lt;sup&gt;-1&lt;/sup&gt;·kg&lt;sup&gt;-1&lt;/sup&gt; the mng calculated based on predicted was strongly correlated r = 071, p &lt; 0001 with mng calculated based on measured data this new technology provides an important advance in ambulatory and continuous assessment of aerobic fitness with potential for future applications such as the early detection of deterioration of physical health</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41884</th>\n",
       "      <td>human activity recognition based on dynamic active learning activity of daily living is an important indicator of the health status and functional capabilities of an individual activity recognition, which aims at understanding the behavioral patterns of people, has increasingly received attention in recent years however, there are still a number of challenges confronting the task first, labelling training data is expensive and time-consuming, leading to limited availability of annotations secondly, activities performed by individuals have considerable variability, which renders the generally used supervised learning with a fixed label set unsuitable to address these issues, we propose a dynamic active learning-based activity recognition method in this work different from traditional active learning methods which select samples based on a fixed label set, the proposed method not only selects informative samples from known classes, but also dynamically identifies new activities which are not included in the predefined label set starting with a classifier that has access to a limited number of labelled samples, we iteratively extend the training set with informative labels by fully considering the uncertainty, diversity and representativeness of samples, based on which better-informed classifiers can be trained, further reducing the annotation cost we evaluate the proposed method on two synthetic datasets and two existing benchmark datasets experimental results demonstrate that our method not only boosts the activity recognition performance with considerably reduced annotation cost, but also enables adaptive daily activity analysis allowing the presence and detection of novel activities and patterns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74910</th>\n",
       "      <td>a hierarchical deep fusion framework for egocentric activity recognition using a wearable hybrid sensor system recently, egocentric activity recognition has attracted considerable attention in the pattern recognition and artificial intelligence communities because of its wide applicability in medical care, smart homes, and security monitoring in this study, we developed and implemented a deep-learning-based hierarchical fusion framework for the recognition of egocentric activities of daily living adls in a wearable hybrid sensor system comprising motion sensors and cameras long short-term memory lstm and a convolutional neural network are used to perform egocentric adl recognition based on motion sensor data and photo streaming in different layers, respectively the motion sensor data are used solely for activity classification according to motion state, while the photo stream is used for further specific activity recognition in the motion state groups thus, both motion sensor data and photo stream work in their most suitable classification mode to significantly reduce the negative influence of sensor differences on the fusion results experimental results show that the proposed method not only is more accurate than the existing direct fusion method by up to 6% but also avoids the time-consuming computation of optical flow in the existing method, which makes the proposed algorithm less complex and more suitable for practical application</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37198</th>\n",
       "      <td>automatic diagnosis of cerebral palsy gait using computational intelligence techniques: a low-cost multi-sensor approach automatic diagnosing of cerebral palsy cp gait is crucial in quantitative evaluation of a therapeutic intervention existing systems for such gait assessment are expensive and require user intervention this study proposes a low-cost gait assessment system equipped with multiple kinect sensors forty subjects 20 cp patients and 20 normal were recruited for the experiment to remove outlier frames from the combined gait signal of multiple sensors a data driven algorithm was proposed different supervised classifiers along with extreme learning machine were investigated to diagnose cp gait in addition, a feature level analysis was also performed several spatio-temporal features ie step length, stride length, stride time, etc were extracted the strength of walking ratio, a speed invariant feature, to detect cp gait was thoroughly analyzed the proposed system outperformed state-of-the-art with ≈98% of accuracy sensitivity: 100%, and specificity: 9687% results indicate a substantial improvement in abnormality detection performance after outlier removal based on relieff feature ranking algorithm, walking ratio ranked the best among other classical gait features performance of all classifiers increased substantially using walking ratio as a feature extreme learning machine demonstrated a competing performance in all cases the higher classification accuracy of this low-cost system using only a single feature makes it attractive for cp gait detection</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110254</th>\n",
       "      <td>in-home behavioral monitoring using simultaneous localization and activity detection shifting demographics in the us has created an urgent need to reform the policies, practices, and technology associated with delivering healthcare to geriatric populations automated monitoring systems can improve the quality of life while reducing healthcare costs for individuals aging in place for these systems to be successful, both activity detection and localization are important, but most existing research focuses on only one of these technologies and systems that do collect both data treat these data sources separately here, we present slad simultaneous localization and activity detection a novel framework for simultaneously processing data collected from localization and activity classification systems using a hidden markov model and machine learning techniques, slad fuses these two sources of data in realtime using a probabilistic likelihood framework, which allows activity data to refine localization, and vice-versa to evaluate the system, a wireless sensor network was deployed to collect rssi data and imu data concurrently from a wrist-worn watch; the rssi data was processed using a radial basis function neural network localization algorithm, and the resulting position likelihoods were combined with the likelihoods from an imu acitivty classification algorithm in an experiment conducted in an indoor office environment, the proposed method produces 97% localization accuracy and 85% activity classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>evaluation of three state-of-the-art classifiers for recognition of activities of daily living from smart home ambient data smart homes for the aging population have recently started attracting the attention of the research community the health state of smart homes is comprised of many different levels; starting with the physical health of citizens, it also includes longer-term health norms and outcomes, as well as the arena of positive behavior changes one of the problems of interest is to monitor the activities of daily living adl of the elderly, aiming at their protection and well-being for this purpose, we installed passive infrared pir sensors to detect motion in a specific area inside a smart apartment and used them to collect a set of adl in a novel approach, we describe a technology that allows the ground truth collected in one smart home to train activity recognition systems for other smart homes we asked the users to label all instances of all adl only once and subsequently applied data mining techniques to cluster in-home sensor firings each cluster would therefore represent the instances of the same activity once the clusters were associated to their corresponding activities, our system was able to recognize future activities to improve the activity recognition accuracy, our system preprocessed raw sensor data by identifying overlapping activities to evaluate the recognition performance from a 200-day dataset, we implemented three different active learning classification algorithms and compared their performance: naive bayesian nb, support vector machine svm and random forest rf based on our results, the rf classifier recognized activities with an average specificity of 9653%, a sensitivity of 6849%, a precision of 7441% and an f-measure of 7133%, outperforming both the nb and svm classifiers further clustering markedly improved the results of the rf classifier an activity recognition system based on pir sensors in conjunction with a clustering classification approach was able to detect adl from datasets collected from different homes thus, our pir-based smart home technology could improve care and provide valuable information to better understand the functioning of our societies, as well as to inform both individual and collective action in a smart city scenario</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "52887                                                                                                                                                                                                                                                                                           a novel hybrid deep neural network to predict pre-impact fall for older people based on wearable inertial sensors falls in the elderly is a major public health concern due to its high prevalence, serious consequences and heavy burden on the society many falls in older people happen within a very short time, which makes it difficult to predict a fall before it occurs and then to provide protection for the person who is falling the primary objective of this study was to develop deep neural networks for predicting a fall during its initiation and descending but before the body impacts to the ground so that a safety mechanism can be enabled to prevent fall-related injuries we divided the falling process into three stages non-fall, pre-impact fall and fall and developed deep neutral networks to perform three-class classification three deep learning models, convolutional neural network cnn, long short term memory lstm, and a novel hybrid model integrating both convolution and long short term memory convlstm were proposed and evaluated on a large public dataset of various falls and activities of daily living adl acquired with wearable inertial sensors accelerometer and gyroscope fivefold cross validation results showed that the hybrid convlstm model had mean sensitivities of 9315, 9378, and 9600% for non-fall, pre-impact fall and fall, respectively, which were higher than both lstm except the fall class and cnn models convlstm model also showed higher specificities for all three classes 9659, 9449, and 9869% than lstm and cnn models in addition, latency test on a microcontroller unit showed that convlstm model had a short latency of 106 ms, which was much lower than lstm model 315 ms and comparable with cnn model 077 ms high prediction accuracy especially for pre-impact fall and low latency on the microboard indicated that the proposed hybrid convlstm model outperformed both lstm and cnn models these findings suggest that our proposed novel hybrid convlstm model has great potential to be embedded into wearable inertial sensor-based systems to predict pre-impact fall in real-time so that protective devices could be triggered in time to prevent fall-related injuries for older people   \n",
       "94882   extracting aerobic system dynamics during unsupervised activities of daily living using wearable sensor machine learning models physical activity levels are related through algorithms to the energetic demand, with no information regarding the integrity of the multiple physiological systems involved in the energetic supply longitudinal analysis of the oxygen uptake v̇o<sub>2</sub> by wearable sensors in realistic settings might permit development of a practical tool for the study of the longitudinal aerobic system dynamics ie, v̇o<sub>2</sub> kinetics this study evaluated aerobic system dynamics based on predicted v̇o<sub>2</sub> data obtained from wearable sensors during unsupervised activities of daily living μadl thirteen healthy men performed a laboratory-controlled moderate exercise protocol and were monitored for ≈6 h/day for 4 days μadl data variables derived from hip accelerometer acc<sub>hip</sub>, heart rate monitor, and respiratory bands during μadl were extracted and processed by a validated random forest regression model to predict v̇o<sub>2</sub> the aerobic system analysis was based on the frequency-domain analysis of acc<sub>hip</sub> and predicted v̇o<sub>2</sub> data obtained during μadl optimal samples for frequency domain analysis constrained to ≤001 hz were selected when acc<sub>hip</sub> was higher than 005 g at a given frequency ie, participants were active the temporal characteristics of predicted v̇o<sub>2</sub> data during μadl correlated with the temporal characteristics of measured v̇o<sub>2</sub> data during laboratory-controlled protocol formula: see text = 082, p < 0001, n = 13 in conclusion, aerobic system dynamics can be investigated during unsupervised activities of daily living by wearable sensors although speculative, these algorithms have the potential to be incorporated into wearable systems for early detection of changes in health status in realistic environments by detecting changes in aerobic response dynamics new & noteworthy the early detection of subclinical aerobic system impairments might be indicative of impaired physiological reserves that impact the capacity for physical activity this study is the first to use wearable sensors in unsupervised activities of daily living in combination with novel machine learning algorithms to investigate the aerobic system dynamics with the potential to contribute to models of functional health status and guide future individualized health care in the normal population   \n",
       "17351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           deep chores: estimating hallmark measures of physical activity using deep learning wrist accelerometers for assessing hallmark measures of physical activity pa are rapidly growing with the advent of smartwatch technology given the growing popularity of wrist-worn accelerometers, there needs to be a rigorous evaluation for recognizing pa type and estimating energy expenditure ee across the lifespan participants 66% women, aged 20-89 yrs performed a battery of 33 daily activities in a standardized laboratory setting while a tri-axial accelerometer collected data from the right wrist a portable metabolic unit was worn to measure metabolic intensity we built deep learning networks to extract spatial and temporal representations from the time-series data, and used them to recognize pa type and estimate ee the deep learning models resulted in high performance; the f1 score was: 082, 081, and 95 for recognizing sedentary, locomotor, and lifestyle activities, respectively the root mean square error was 11 +/-013 for the estimation of ee   \n",
       "116860                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          dynamic bayesian networks for context-aware fall risk assessment fall incidents among the elderly often occur in the home and can cause serious injuries affecting their independent living this paper presents an approach where data from wearable sensors integrated in a smart home environment is combined using a dynamic bayesian network the smart home environment provides contextual data, obtained from environmental sensors, and contributes to assessing a fall risk probability the evaluation of the developed system is performed through simulation each time step is represented by a single user activity and interacts with a fall sensors located on a mobile device a posterior probability is calculated for each recognized activity or contextual information the output of the system provides a total risk assessment of falling given a response from the fall sensor    \n",
       "125139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  impact of study design on development and evaluation of an activity-type classifier methods to classify activity types are often evaluated with an experimental protocol involving prescribed physical activities under confined laboratory conditions, which may not reflect real-life conditions the present study aims to evaluate how study design may impact on classifier performance in real life twenty-eight healthy participants 21-53 yr were asked to wear nine triaxial accelerometers while performing 58 activity types selected to simulate activities in real life for each sensor location, logistic classifiers were trained in subsets of up to 8 activities to distinguish between walking and nonwalking activities and were then evaluated in all 58 activities different weighting factors were used to convert the resulting confusion matrices into an estimation of the confusion matrix as would apply in the real-life setting by creating four different real-life scenarios, as well as one traditional laboratory scenario the sensitivity of a classifier estimated with a traditional laboratory protocol is within the range of estimates derived from real-life scenarios for any body location the specificity, however, was systematically overestimated by the traditional laboratory scenario walking time was systematically overestimated, except for lower back sensor data range: 7-757% in conclusion, classifier performance under confined conditions may not accurately reflect classifier performance in real life future studies that aim to evaluate activity classification methods are warranted to pay special attention to the representativeness of experimental conditions for real-life conditions   \n",
       "81852                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               imu-based classification of parkinsons disease from gait: a sensitivity analysis on sensor location and feature selection inertial measurement units imus have a long-lasting popularity in a variety of industrial applications from navigation systems to guidance and robotics their use in clinical practice is now becoming more common, thanks to miniaturization and the ability to integrate on-board computational and decision-support features imu-based gait analysis is a paradigm of this evolving process, and in this study its use for the assessment of parkinsons disease pd is comprehensively analyzed data coming from 25 individuals with different levels of pd symptoms severity and an equal number of age-matched healthy individuals were included into a set of 6 different machine learning ml techniques, processing 18 different configurations of gait parameters taken from 8 imu sensors classification accuracy was calculated for each configuration and ml technique, adding two meta-classifiers based on the results obtained from all individual techniques through majority of voting, with two different weighting schemes average classification accuracy ranged between 63% and 80% among classifiers and increased up to 96% for one meta-classifier configuration configurations based on a statistical preselection process showed the highest average classification accuracy when reducing the number of sensors, features based on the joint range of motion were more accurate than those based on spatio-temporal parameters in particular, best results were obtained with the knee range of motion, calculated with four imus, placed bilaterally the obtained findings provide data-driven evidence on which combination of sensor configurations and classification methods to be used during imu-based gait analysis to grade the severity level of pd   \n",
       "36550                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              skeleton data pre-processing for human pose recognition using neural network automatic monitoring of daily living activities can greatly improve the possibility of living autonomously for frail individuals pose recognition based on skeleton tracking data is promising for identifying dangerous situations and trigger external intervention or other alarms, while avoiding privacy issues and the need for patient compliance here we present the benefits of pre-processing kinect-recorded skeleton data to limit the several errors produced by the system when the subject is not in ideal tracking conditions the accuracy of our two hidden layers mlp classifier improved from about 82% to over 92% in recognizing actors in four different poses: standing, sitting, lying and dangerous sitting   \n",
       "921                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   a comparative study of time frequency representation techniques for freeze of gait detection and prediction freezing of gait fog is an impairment that affects the majority of patients in the advanced stages of parkinsons disease pd fog can lead to sudden falls and injuries, negatively impacting the quality of life for the patients and their families rhythmic auditory stimulation ras can be used to help patients recover from fog and resume normal gait ras might be ineffective due to the latency between the start of a fog event, its detection and initialization of ras we propose a system capable of both fog prediction and detection using signals from tri-axial accelerometer sensors that will be useful in initializing ras with minimal latency we compared the performance of several time frequency analysis techniques, including moving windows extracted from the signals, handcrafted features, recurrence plots rp, short time fourier transform stft, discreet wavelet transform dwt and pseudo wigner ville distribution pwvd with deep learning dl based long short term memory lstm and convolutional neural networks cnn we also propose three ensemble network architectures that combine all the time frequency representations and dl architectures experimental results show that our ensemble architectures significantly improve the performance compared with existing techniques we also present the results of applying our method trained on a publicly available dataset to data collected from patients using wearable sensors in collaboration with at still university   \n",
       "9522                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       hand tremor detection in videos with cluttered background using neural network based approaches with the increasing prevalence of neurodegenerative diseases, including parkinsons disease, hand tremor detection has become a popular research topic because it helps with the diagnosis and tracking of disease progression conventional hand tremor detection algorithms involved wearable sensors a non-invasive hand tremor detection algorithm using videos as input is desirable but the existing video-based algorithms are sensitive to environmental conditions an algorithm, with the capability of detecting hand tremor from videos with a cluttered background, would allow the videos recorded in a non-research environment to be used clinicians and researchers could use videos collected from patients and participants in their own home environment or standard clinical settings neural network based machine learning architectures provide high accuracy classification results in related fields including hand gesture recognition and body movement detection systems we thus investigated the accuracy of advanced neural network architectures to automatically detect hand tremor in videos with a cluttered background we examined configurations with different sets of features and neural network based classification models we compared the performance of different combinations of features and classification models and then selected the combination which provided the highest accuracy of hand tremor detection we used cross validation to test the accuracy of the trained model predictions the highest classification accuracy for automatically detecting tremor vs non tremor was 806% and this was obtained using convolutional neural network-long short-term memory and features based on measures of frequency and amplitude change   \n",
       "96342                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        prediction of oxygen uptake dynamics by machine learning analysis of wearable sensors during activities of daily living currently, oxygen uptake  is the most precise means of investigating aerobic fitness and level of physical activity; however, can only be directly measured in supervised conditions with the advancement of new wearable sensor technologies and data processing approaches, it is possible to accurately infer work rate and predict during activities of daily living adl the main objective of this study was to develop and verify the methods required to predict and investigate the dynamics during adl the variables derived from the wearable sensors were used to create a predictor based on a random forest method the temporal dynamics were assessed by the mean normalized gain amplitude mng obtained from frequency domain analysis the mng provides a means to assess aerobic fitness the predicted during adl was strongly correlated r = 087, p < 0001 with the measured and the prediction bias was 02 ml·min<sup>-1</sup>·kg<sup>-1</sup> the mng calculated based on predicted was strongly correlated r = 071, p < 0001 with mng calculated based on measured data this new technology provides an important advance in ambulatory and continuous assessment of aerobic fitness with potential for future applications such as the early detection of deterioration of physical health   \n",
       "41884                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  human activity recognition based on dynamic active learning activity of daily living is an important indicator of the health status and functional capabilities of an individual activity recognition, which aims at understanding the behavioral patterns of people, has increasingly received attention in recent years however, there are still a number of challenges confronting the task first, labelling training data is expensive and time-consuming, leading to limited availability of annotations secondly, activities performed by individuals have considerable variability, which renders the generally used supervised learning with a fixed label set unsuitable to address these issues, we propose a dynamic active learning-based activity recognition method in this work different from traditional active learning methods which select samples based on a fixed label set, the proposed method not only selects informative samples from known classes, but also dynamically identifies new activities which are not included in the predefined label set starting with a classifier that has access to a limited number of labelled samples, we iteratively extend the training set with informative labels by fully considering the uncertainty, diversity and representativeness of samples, based on which better-informed classifiers can be trained, further reducing the annotation cost we evaluate the proposed method on two synthetic datasets and two existing benchmark datasets experimental results demonstrate that our method not only boosts the activity recognition performance with considerably reduced annotation cost, but also enables adaptive daily activity analysis allowing the presence and detection of novel activities and patterns   \n",
       "74910                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             a hierarchical deep fusion framework for egocentric activity recognition using a wearable hybrid sensor system recently, egocentric activity recognition has attracted considerable attention in the pattern recognition and artificial intelligence communities because of its wide applicability in medical care, smart homes, and security monitoring in this study, we developed and implemented a deep-learning-based hierarchical fusion framework for the recognition of egocentric activities of daily living adls in a wearable hybrid sensor system comprising motion sensors and cameras long short-term memory lstm and a convolutional neural network are used to perform egocentric adl recognition based on motion sensor data and photo streaming in different layers, respectively the motion sensor data are used solely for activity classification according to motion state, while the photo stream is used for further specific activity recognition in the motion state groups thus, both motion sensor data and photo stream work in their most suitable classification mode to significantly reduce the negative influence of sensor differences on the fusion results experimental results show that the proposed method not only is more accurate than the existing direct fusion method by up to 6% but also avoids the time-consuming computation of optical flow in the existing method, which makes the proposed algorithm less complex and more suitable for practical application   \n",
       "37198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  automatic diagnosis of cerebral palsy gait using computational intelligence techniques: a low-cost multi-sensor approach automatic diagnosing of cerebral palsy cp gait is crucial in quantitative evaluation of a therapeutic intervention existing systems for such gait assessment are expensive and require user intervention this study proposes a low-cost gait assessment system equipped with multiple kinect sensors forty subjects 20 cp patients and 20 normal were recruited for the experiment to remove outlier frames from the combined gait signal of multiple sensors a data driven algorithm was proposed different supervised classifiers along with extreme learning machine were investigated to diagnose cp gait in addition, a feature level analysis was also performed several spatio-temporal features ie step length, stride length, stride time, etc were extracted the strength of walking ratio, a speed invariant feature, to detect cp gait was thoroughly analyzed the proposed system outperformed state-of-the-art with ≈98% of accuracy sensitivity: 100%, and specificity: 9687% results indicate a substantial improvement in abnormality detection performance after outlier removal based on relieff feature ranking algorithm, walking ratio ranked the best among other classical gait features performance of all classifiers increased substantially using walking ratio as a feature extreme learning machine demonstrated a competing performance in all cases the higher classification accuracy of this low-cost system using only a single feature makes it attractive for cp gait detection   \n",
       "110254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           in-home behavioral monitoring using simultaneous localization and activity detection shifting demographics in the us has created an urgent need to reform the policies, practices, and technology associated with delivering healthcare to geriatric populations automated monitoring systems can improve the quality of life while reducing healthcare costs for individuals aging in place for these systems to be successful, both activity detection and localization are important, but most existing research focuses on only one of these technologies and systems that do collect both data treat these data sources separately here, we present slad simultaneous localization and activity detection a novel framework for simultaneously processing data collected from localization and activity classification systems using a hidden markov model and machine learning techniques, slad fuses these two sources of data in realtime using a probabilistic likelihood framework, which allows activity data to refine localization, and vice-versa to evaluate the system, a wireless sensor network was deployed to collect rssi data and imu data concurrently from a wrist-worn watch; the rssi data was processed using a radial basis function neural network localization algorithm, and the resulting position likelihoods were combined with the likelihoods from an imu acitivty classification algorithm in an experiment conducted in an indoor office environment, the proposed method produces 97% localization accuracy and 85% activity classification    \n",
       "110201                                                                                                                                                                                  evaluation of three state-of-the-art classifiers for recognition of activities of daily living from smart home ambient data smart homes for the aging population have recently started attracting the attention of the research community the health state of smart homes is comprised of many different levels; starting with the physical health of citizens, it also includes longer-term health norms and outcomes, as well as the arena of positive behavior changes one of the problems of interest is to monitor the activities of daily living adl of the elderly, aiming at their protection and well-being for this purpose, we installed passive infrared pir sensors to detect motion in a specific area inside a smart apartment and used them to collect a set of adl in a novel approach, we describe a technology that allows the ground truth collected in one smart home to train activity recognition systems for other smart homes we asked the users to label all instances of all adl only once and subsequently applied data mining techniques to cluster in-home sensor firings each cluster would therefore represent the instances of the same activity once the clusters were associated to their corresponding activities, our system was able to recognize future activities to improve the activity recognition accuracy, our system preprocessed raw sensor data by identifying overlapping activities to evaluate the recognition performance from a 200-day dataset, we implemented three different active learning classification algorithms and compared their performance: naive bayesian nb, support vector machine svm and random forest rf based on our results, the rf classifier recognized activities with an average specificity of 9653%, a sensitivity of 6849%, a precision of 7441% and an f-measure of 7133%, outperforming both the nb and svm classifiers further clustering markedly improved the results of the rf classifier an activity recognition system based on pir sensors in conjunction with a clustering classification approach was able to detect adl from datasets collected from different homes thus, our pir-based smart home technology could improve care and provide valuable information to better understand the functioning of our societies, as well as to inform both individual and collective action in a smart city scenario    \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "52887         0       0       0           0          0        0       0   \n",
       "94882         0       0       0           0          0        0       0   \n",
       "17351         0       0       0           0          0        0       0   \n",
       "116860        0       0       0           0          0        0       0   \n",
       "125139        0       0       0           0          0        0       0   \n",
       "81852         0       0       0           0          0        0       0   \n",
       "36550         0       0       0           0          0        0       0   \n",
       "921           0       0       0           0          0        0       0   \n",
       "9522          0       0       0           0          0        0       0   \n",
       "96342         0       0       0           0          0        0       0   \n",
       "41884         0       0       0           0          0        0       0   \n",
       "74910         0       0       0           0          0        0       0   \n",
       "37198         0       0       0           0          0        0       0   \n",
       "110254        0       0       0           0          0        0       0   \n",
       "110201        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "52887            0            0         0           0        0       0   \n",
       "94882            0            0         0           0        0       0   \n",
       "17351            0            0         0           0        0       0   \n",
       "116860           0            0         0           0        0       0   \n",
       "125139           0            0         0           0        0       0   \n",
       "81852            0            0         0           0        0       0   \n",
       "36550            0            0         0           0        0       0   \n",
       "921              0            0         0           0        0       0   \n",
       "9522             0            0         0           0        0       0   \n",
       "96342            0            0         0           0        0       0   \n",
       "41884            0            0         0           0        0       0   \n",
       "74910            0            0         0           0        0       0   \n",
       "37198            0            0         0           0        0       0   \n",
       "110254           0            0         0           0        0       0   \n",
       "110201           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "52887            0             0           0            0         0   \n",
       "94882            0             0           0            0         0   \n",
       "17351            0             0           0            0         0   \n",
       "116860           0             0           0            0         0   \n",
       "125139           0             0           0            0         0   \n",
       "81852            0             0           0            0         0   \n",
       "36550            0             0           0            0         0   \n",
       "921              0             0           0            0         0   \n",
       "9522             0             0           0            0         0   \n",
       "96342            0             0           0            0         0   \n",
       "41884            0             0           0            0         0   \n",
       "74910            0             0           0            0         0   \n",
       "37198            0             0           0            0         0   \n",
       "110254           0             0           0            0         0   \n",
       "110201           0             0           0            0         0   \n",
       "\n",
       "       hepca_text urology_text prosca_text renalca_text gynonc_text  \\\n",
       "52887           0            0           0            0           0   \n",
       "94882           0            0           0            0           0   \n",
       "17351           0            0           0            0           0   \n",
       "116860          0            0           0            0           0   \n",
       "125139          0            0           0            0           0   \n",
       "81852           0            0           0            0           0   \n",
       "36550           0            0           0            0           0   \n",
       "921             0            0           0            0           0   \n",
       "9522            0            0           0            0           0   \n",
       "96342           0            0           0            0           0   \n",
       "41884           0            0           0            0           0   \n",
       "74910           0            0           0            0           0   \n",
       "37198           0            0           0            0           0   \n",
       "110254          0            0           0            0           0   \n",
       "110201          0            0           0            0           0   \n",
       "\n",
       "       haemonc_text psych_text suicide_text msk_text frac_text rheum_text  \\\n",
       "52887             0          0            0        0         0          0   \n",
       "94882             0          0            0        0         0          0   \n",
       "17351             0          0            0        0         0          0   \n",
       "116860            0          0            0        0         0          0   \n",
       "125139            0          0            0        0         0          0   \n",
       "81852             0          0            0        0         0          0   \n",
       "36550             0          0            0        0         0          0   \n",
       "921               0          0            0        0         0          0   \n",
       "9522              0          0            0        0         0          0   \n",
       "96342             0          0            0        0         0          0   \n",
       "41884             0          0            0        0         0          0   \n",
       "74910             0          0            0        0         0          0   \n",
       "37198             0          0            0        0         0          0   \n",
       "110254            0          0            0        0         0          0   \n",
       "110201            0          0            0        0         0          0   \n",
       "\n",
       "       gi_text hep_text resp_text pneum_text osa_text pe_text pubh_text  \\\n",
       "52887        0        0         0          0        0       0         1   \n",
       "94882        0        0         1          0        0       0         0   \n",
       "17351        0        0         0          0        0       0         0   \n",
       "116860       0        0         0          0        0       0         0   \n",
       "125139       0        0         0          0        0       0         0   \n",
       "81852        0        0         0          0        0       0         0   \n",
       "36550        0        0         0          0        0       0         0   \n",
       "921          0        0         0          0        0       0         0   \n",
       "9522         0        0         0          0        0       0         0   \n",
       "96342        0        0         0          0        0       0         0   \n",
       "41884        0        0         0          0        0       0         0   \n",
       "74910        0        0         0          0        0       0         0   \n",
       "37198        0        0         0          0        0       0         0   \n",
       "110254       0        0         0          0        0       0         0   \n",
       "110201       0        0         0          0        0       0         0   \n",
       "\n",
       "       neuro_text cva_text epilep_text alzh_text cvs_text ihd_text hf_text  \\\n",
       "52887           0        0           0         0        0        0       0   \n",
       "94882           0        0           0         0        0        0       0   \n",
       "17351           0        0           0         0        0        0       0   \n",
       "116860          0        0           0         0        0        0       0   \n",
       "125139          0        0           0         0        0        0       0   \n",
       "81852           1        0           0         0        0        0       0   \n",
       "36550           0        0           0         0        0        0       0   \n",
       "921             1        0           0         0        0        0       0   \n",
       "9522            1        0           0         0        0        0       0   \n",
       "96342           0        0           0         0        0        0       0   \n",
       "41884           0        0           0         0        0        0       0   \n",
       "74910           0        0           0         0        0        0       0   \n",
       "37198           1        0           0         0        0        0       0   \n",
       "110254          0        0           0         0        0        0       0   \n",
       "110201          0        0           0         0        0        0       0   \n",
       "\n",
       "       arrhyt_text endo_text dm_text insulin_text retina_text eye_text  \\\n",
       "52887            0         0       0            0           0        0   \n",
       "94882            0         0       0            0           0        0   \n",
       "17351            0         0       0            0           0        0   \n",
       "116860           0         0       0            0           0        0   \n",
       "125139           0         0       0            0           0        0   \n",
       "81852            0         0       0            0           0        0   \n",
       "36550            0         0       0            0           0        0   \n",
       "921              0         0       0            0           0        0   \n",
       "9522             0         0       0            0           0        0   \n",
       "96342            0         0       0            0           0        0   \n",
       "41884            0         0       0            0           0        0   \n",
       "74910            0         0       0            0           0        0   \n",
       "37198            0         0       0            0           0        0   \n",
       "110254           0         0       0            0           0        0   \n",
       "110201           0         0       0            0           0        0   \n",
       "\n",
       "       haem_text obs_text renal_text ackd_text paeds_text dent_text  \\\n",
       "52887          0        0          0         0          0         0   \n",
       "94882          0        0          0         0          0         0   \n",
       "17351          0        0          0         0          0         0   \n",
       "116860         0        0          0         0          0         0   \n",
       "125139         0        0          0         0          0         0   \n",
       "81852          0        0          0         0          0         0   \n",
       "36550          0        0          0         0          0         0   \n",
       "921            0        0          0         0          0         0   \n",
       "9522           0        0          0         0          0         0   \n",
       "96342          0        0          0         0          0         0   \n",
       "41884          0        0          0         0          0         0   \n",
       "74910          0        0          0         0          0         0   \n",
       "37198          0        0          0         0          0         0   \n",
       "110254         0        0          0         0          0         0   \n",
       "110201         0        0          0         0          0         0   \n",
       "\n",
       "       audio_text bci_text prosth_text assist_text activity_text  \n",
       "52887           0        0           0           0             1  \n",
       "94882           0        0           0           0             1  \n",
       "17351           0        0           0           0             1  \n",
       "116860          0        0           0           0             1  \n",
       "125139          0        0           0           0             1  \n",
       "81852           0        0           0           0             1  \n",
       "36550           0        0           0           0             1  \n",
       "921             0        0           0           0             1  \n",
       "9522            0        0           0           0             1  \n",
       "96342           0        0           0           0             1  \n",
       "41884           0        0           0           0             1  \n",
       "74910           0        0           0           0             1  \n",
       "37198           0        0           0           0             1  \n",
       "110254          0        0           0           0             1  \n",
       "110201          0        0           0           0             1  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['activity_text']=='1'].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a0883546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-130-def89f50e481>:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_retina'] = np.where(spec['retina_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_haem'] = np.where(spec['haem_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_obs'] = np.where(spec['obs_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_renal'] = np.where(spec['renal_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_ackd'] = np.where(spec['ackd_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_pubh'] = np.where(spec['pubh_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_bci'] = np.where(spec['bci_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_prosth'] = np.where(spec['prosth_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_assist'] = np.where(spec['assist_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "<ipython-input-130-def89f50e481>:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_activity'] = np.where(spec['activity_text'].str.contains(\"1\"), \"1\", \"0\")\n"
     ]
    }
   ],
   "source": [
    "## combine\n",
    "\n",
    "labelled['subspec_icu'] = np.where(spec['icu_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_ed'] = np.where(spec['ed_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_paeds'] = np.where(spec['paeds_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_dent'] = np.where(spec['dent_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_audio'] = np.where(spec['audio_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_id'] = np.where(spec['id_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_sepsis'] = np.where(spec['sepsis_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_hiv'] = np.where(spec['hiv_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_cov19'] = np.where(spec['cov19_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_tb'] = np.where(spec['tb_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_malaria'] = np.where(spec['malaria_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_tropic'] = np.where(spec['tropic_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_derm'] = np.where(spec['derm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_dermca'] = np.where(spec['dermca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_onc'] = np.where(spec['onc_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_rx'] = np.where(spec['rx_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_lungca'] = np.where(spec['lungca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_brainca'] = np.where(spec['brainca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_gica'] = np.where(spec['gica_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_hepca'] = np.where(spec['hepca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_prosca'] = np.where(spec['prosca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_gynonc'] = np.where(spec['gynonc_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_renalca'] = np.where(spec['renalca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_haemonc'] = np.where(spec['haemonc_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_breast'] = np.where(spec['breast_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_breastca'] = np.where(spec['breastca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_urology'] = np.where(spec['urology_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_psych'] = np.where(spec['psych_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_suicide'] = np.where(spec['suicide_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_msk'] = np.where(spec['msk_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_frac'] = np.where(spec['frac_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_rheum'] = np.where(spec['rheum_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_gi'] = np.where(spec['gi_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_hep'] = np.where(spec['hep_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_resp'] = np.where(spec['resp_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_pneum'] = np.where(spec['pneum_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_osa'] = np.where(spec['osa_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_pe'] = np.where(spec['pe_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_neuro'] = np.where(spec['neuro_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_epilep'] = np.where(spec['epilep_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_cva'] = np.where(spec['cva_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_alzh'] = np.where(spec['alzh_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_cvs'] = np.where(spec['cvs_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_ihd'] = np.where(spec['ihd_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_hf'] = np.where(spec['hf_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_arrhyt'] =  np.where(spec['arrhyt_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_endo'] = np.where(spec['endo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_dm'] = np.where(spec['dm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_insulin'] = np.where(spec['insulin_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_eye'] = np.where(spec['eye_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_retina'] = np.where(spec['retina_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_haem'] = np.where(spec['haem_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_obs'] = np.where(spec['obs_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_renal'] = np.where(spec['renal_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_ackd'] = np.where(spec['ackd_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_pubh'] = np.where(spec['pubh_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_bci'] = np.where(spec['bci_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_prosth'] = np.where(spec['prosth_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_assist'] = np.where(spec['assist_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_activity'] = np.where(spec['activity_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "#spec.to_csv('output/spec_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1656064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why NER?\n",
    "# non specific e.g. TB could be in the middle of a ward. NER recognises context\n",
    "# words separate by unspecified distance -> lung and adenocarcinoma\n",
    "## too many possible specific terms for subconditions e.g. lung adenocarcinoma, NSCLC -> adenocarcinoma of the lung\n",
    "\n",
    "## Combination of general terms in main text\n",
    "## NER for specific terms\n",
    "\n",
    "## What are the most used **use-cases**\n",
    "## Can we find what the prediction target is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42368f94",
   "metadata": {},
   "source": [
    "## Other Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2cff7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lmic_list = ['afghanistan', 'albania', 'algeria', 'angola', 'antigua', 'barbuda', 'argentina', 'armenia', 'china',\n",
    "#             'azerbaijan', 'bangladesh', 'belarus', 'belize', 'benin', 'bhutan', 'bolivia', 'bosnia', 'herzegovina', \n",
    "#             'botswana', 'brazil', 'burkina', 'faso', 'burundi', 'verde', 'cambodia', 'cameroon', 'africa', 'chad', \n",
    "#             'colombia', 'comoros', 'congo', 'costa rica', 'ivoire', 'cuba', 'djibouti', 'dominica', 'dominica', \n",
    "#             'ecuador', 'egypt', 'salvador', 'guinea', 'eritrea', 'eswatini', 'ethiopia', 'fiji', 'gabon', 'gambia', \n",
    "#             'georgia', 'ghana', 'grenada', 'guatemala', 'guinea', 'guyana', 'haiti', 'honduras', 'india', \n",
    "#             'indonesia', 'iran', 'iraq', 'jamaica', 'jordan', 'kazakhstan', 'kenya', 'kiribati', 'dpr', 'north korea', \n",
    "#             'kosovo', 'kyrgyzstan', 'lao', 'lebanon', 'lesotho', 'liberia', 'libya', 'macedonia', 'madagascar', 'malawi', \n",
    "#             'malaysia', 'maldives', 'mali', 'marshall', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', \n",
    "#             'mongolia', 'montenegro', 'montserrat', 'morocco', 'mozambique', 'myanmar', 'namibia', 'nauru', 'nepal', \n",
    "#             'nicaragua', 'niger', 'nigeria', 'niue', 'pakistan', 'palau', 'panama', 'papua', 'paraguay', 'peru', \n",
    "#             'philippines', 'rwanda', 'helena', 'samoa', 'príncipe', 'senegal', 'serbia', 'sierra leone', 'solomon', \n",
    "#             'somalia', 'south africa', 'sudan', 'sri lanka', 'saint lucia', 'saint vincent', 'grenadines', 'sudan', \n",
    "#             'suriname', 'syria', 'tajikistan', 'tanzania', 'thailand', 'timor', 'togo', 'tokelau', 'tonga', 'tunisia', \n",
    "#             'turkey', 'turkmenistan', 'tuvalu', 'uganda', 'ukraine', 'uzbekistan', 'vanuatu', 'venezuela', 'vietnam', \n",
    "#             'wallis', 'west bank', 'gaza', 'palestine', 'yemen', 'zambia', 'zimbabwe', 'low-income', 'middle-income', \n",
    "#             'lmic', 'scarce', 'resource limited', 'resource-limited']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0966b",
   "metadata": {},
   "source": [
    "## Final Tagged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "176252a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34179\n"
     ]
    }
   ],
   "source": [
    "#all_tagged = pd.concat([algo, feat, spec], axis=1)\n",
    "#\n",
    "print(len(labelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c1fc669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34179 entries, 1 to 172538\n",
      "Data columns (total 110 columns):\n",
      " #    Column               Dtype \n",
      "---   ------               ----- \n",
      " 0    pmid                 string\n",
      " 1    doi                  string\n",
      " 2    title                string\n",
      " 3    abstract             string\n",
      " 4    article_date         string\n",
      " 5    pubmed_date          string\n",
      " 6    article_type         string\n",
      " 7    lang                 string\n",
      " 8    journal              string\n",
      " 9    journal_short        string\n",
      " 10   journal_country      string\n",
      " 11   authors              string\n",
      " 12   author_affils        string\n",
      " 13   keywords             string\n",
      " 14   mesh_terms           string\n",
      " 15   references_pmids     string\n",
      " 16   feature              string\n",
      " 17   include              string\n",
      " 18   mature               string\n",
      " 19   algo_neural_net      object\n",
      " 20   algo_support_vector  object\n",
      " 21   algo_regression      object\n",
      " 22   algo_decision_tree   object\n",
      " 23   algo_discriminant    object\n",
      " 24   algo_naive_bayes     object\n",
      " 25   algo_transfer        object\n",
      " 26   algo_federated       object\n",
      " 27   algo_k_nearest       object\n",
      " 28   algo_unsupervised    object\n",
      " 29   feat_xr              object\n",
      " 30   feat_ct              object\n",
      " 31   feat_mri             object\n",
      " 32   feat_eeg             object\n",
      " 33   feat_ecg             object\n",
      " 34   feat_emg             object\n",
      " 35   feat_us              object\n",
      " 36   feat_echo            object\n",
      " 37   feat_histo           object\n",
      " 38   feat_oct             object\n",
      " 39   feat_mamm            object\n",
      " 40   feat_endoscop        object\n",
      " 41   feat_derm            object\n",
      " 42   feat_gene            object\n",
      " 43   feat_bio             object\n",
      " 44   feat_nlp             object\n",
      " 45   feat_ehr             object\n",
      " 46   feat_sensor          object\n",
      " 47   feat_phone           object\n",
      " 48   feat_prom            object\n",
      " 49   feat_sound           object\n",
      " 50   subspec_icu          object\n",
      " 51   subspec_ed           object\n",
      " 52   spec_paeds           object\n",
      " 53   spec_dent            object\n",
      " 54   spec_audio           object\n",
      " 55   spec_id              object\n",
      " 56   subspec_sepsis       object\n",
      " 57   subspec_hiv          object\n",
      " 58   subspec_cov19        object\n",
      " 59   subspec_tb           object\n",
      " 60   subspec_malaria      object\n",
      " 61   subspec_tropic       object\n",
      " 62   spec_derm            object\n",
      " 63   subspec_dermca       object\n",
      " 64   spec_onc             object\n",
      " 65   subspec_rx           object\n",
      " 66   subspec_lungca       object\n",
      " 67   subspec_brainca      object\n",
      " 68   subspec_gica         object\n",
      " 69   subspec_hepca        object\n",
      " 70   subspec_prosca       object\n",
      " 71   subspec_gynonc       object\n",
      " 72   subspec_renalca      object\n",
      " 73   subspec_haemonc      object\n",
      " 74   subspec_breast       object\n",
      " 75   subspec_breastca     object\n",
      " 76   subspec_urology      object\n",
      " 77   spec_psych           object\n",
      " 78   subspec_suicide      object\n",
      " 79   spec_msk             object\n",
      " 80   subspec_frac         object\n",
      " 81   spec_rheum           object\n",
      " 82   spec_gi              object\n",
      " 83   spec_hep             object\n",
      " 84   spec_resp            object\n",
      " 85   subspec_pneum        object\n",
      " 86   subspec_osa          object\n",
      " 87   subspec_pe           object\n",
      " 88   spec_neuro           object\n",
      " 89   subspec_epilep       object\n",
      " 90   subspec_cva          object\n",
      " 91   subspec_alzh         object\n",
      " 92   spec_cvs             object\n",
      " 93   subspec_ihd          object\n",
      " 94   subspec_hf           object\n",
      " 95   subspec_arrhyt       object\n",
      " 96   spec_endo            object\n",
      " 97   spec_dm              object\n",
      " 98   subspec_insulin      object\n",
      " 99   spec_eye             object\n",
      " 100  subspec_retina       object\n",
      " 101  spec_haem            object\n",
      " 102  spec_obs             object\n",
      " 103  spec_renal           object\n",
      " 104  subspec_ackd         object\n",
      " 105  spec_pubh            object\n",
      " 106  subspec_bci          object\n",
      " 107  subspec_prosth       object\n",
      " 108  subspec_assist       object\n",
      " 109  subspec_activity     object\n",
      "dtypes: object(91), string(19)\n",
      "memory usage: 28.9+ MB\n"
     ]
    }
   ],
   "source": [
    "labelled.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b6183aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_date</th>\n",
       "      <th>pubmed_date</th>\n",
       "      <th>article_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_short</th>\n",
       "      <th>journal_country</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_affils</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>references_pmids</th>\n",
       "      <th>feature</th>\n",
       "      <th>include</th>\n",
       "      <th>mature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>subspec_urology</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104924</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.</td>\n",
       "      <td>Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']</td>\n",
       "      <td>['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']</td>\n",
       "      <td>['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104927</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.</td>\n",
       "      <td>The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']</td>\n",
       "      <td>['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']</td>\n",
       "      <td>['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118652</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States.</td>\n",
       "      <td>Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>NeuroImage</td>\n",
       "      <td>Neuroimage</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']</td>\n",
       "      <td>['Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']</td>\n",
       "      <td>['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34687853</td>\n",
       "      <td>10.1016/j.mri.2021.10.024</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.</td>\n",
       "      <td>To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']</td>\n",
       "      <td>['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']</td>\n",
       "      <td>['Endometrial cancer', 'Radiomic machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34687850</td>\n",
       "      <td>10.1016/j.mri.2021.10.023</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.</td>\n",
       "      <td>Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']</td>\n",
       "      <td>['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']</td>\n",
       "      <td>['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34687347</td>\n",
       "      <td>10.1007/s00330-021-08284-z</td>\n",
       "      <td>Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements.</td>\n",
       "      <td>To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.</td>\n",
       "      <td>2021-10-23</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>European radiology</td>\n",
       "      <td>Eur Radiol</td>\n",
       "      <td>Germany</td>\n",
       "      <td>['Rühling Sebastian', 'Navarro Fernando', 'Sekuboyina Anjany', 'El Husseini Malek', 'Baum Thomas', 'Menze Bjoern', 'Braren Rickmer', 'Zimmer Claus', 'Kirschke Jan S']</td>\n",
       "      <td>['Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Informatics, Technical University of Munich, Munich, Germany.', 'Department of Diagnostic and Interventional Radiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany. jan.kirschke@tum.de.']</td>\n",
       "      <td>['Bone density', 'Machine learning', 'Multidetector computed tomography', 'Osteoporosis', 'Screening']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements. To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>34686914</td>\n",
       "      <td>10.1007/s00467-021-05321-3</td>\n",
       "      <td>Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves.</td>\n",
       "      <td>Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Pediatric nephrology (Berlin, Germany)</td>\n",
       "      <td>Pediatr Nephrol</td>\n",
       "      <td>Germany</td>\n",
       "      <td>['Kwong Jethro Cc', 'Khondker Adree', 'Kim Jin Kyu', 'Chua Michael', 'Keefe Daniel T', 'Dos Santos Joana', 'Skreta Marta', 'Erdman Lauren', \"D'Souza Neeta\", 'Selman Antoine Fermin', 'Weaver John', 'Weiss Dana A', 'Long Christopher', 'Tasian Gregory', 'Teoh Chia Wei', 'Rickard Mandy', 'Lorenzo Armando J']</td>\n",
       "      <td>['Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", 'Division of Nephrology, Hospital for Sick Children, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada. armando.lorenzo@sickkids.ca.']</td>\n",
       "      <td>['Catheterization', 'Chronic kidney disease', 'Dialysis', 'Machine learning', 'Posterior urethral valve', 'Transplant']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves. Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34686815</td>\n",
       "      <td>10.1038/s41415-021-3526-6</td>\n",
       "      <td>The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software.</td>\n",
       "      <td>Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p &lt;0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>British dental journal</td>\n",
       "      <td>Br Dent J</td>\n",
       "      <td>England</td>\n",
       "      <td>['Devlin Hugh', 'Williams Tomos', 'Graham Jim', 'Ashley Martin']</td>\n",
       "      <td>['Professor of Restorative Dentistry, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Director, Manchester Imaging Ltd, UK.', 'Honorary Research Assistant, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Software Manager, Manchester Imaging Ltd, UK. tomos.williams@manchester.ac.uk.', 'Director, Manchester Imaging Ltd, UK; Honorary Reader, Division of Informatics, Imaging and Data Sciences, School of Health Sciences, University of Manchester, UK.', 'Consultant and MAHSC Honorary Professor in Restorative Dentistry and Oral Health, University Dental Hospital of Manchester, Manchester University NHS Foundation Trust, UK.']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software. Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p &lt;0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34686646</td>\n",
       "      <td>10.1097/CMR.0000000000000774</td>\n",
       "      <td>Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas.</td>\n",
       "      <td>To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.</td>\n",
       "      <td>2021-10-21</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Melanoma research</td>\n",
       "      <td>Melanoma Res</td>\n",
       "      <td>England</td>\n",
       "      <td>['Moglia Andrea', 'Cerri Amilcare', 'Moglia Alessandra', 'Berchiolli Raffaella', 'Ferrari Mauro', 'Betti Roberto']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas. To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>34686573</td>\n",
       "      <td>10.1136/neurintsurg-2021-017976</td>\n",
       "      <td>Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population.</td>\n",
       "      <td>Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of neurointerventional surgery</td>\n",
       "      <td>J Neurointerv Surg</td>\n",
       "      <td>England</td>\n",
       "      <td>['Salimi Ashkezari Seyedeh Fatemeh', 'Mut Fernando', 'Slawski Martin', 'Cheng Boyle', 'Yu Alexander K', 'White Tim G', 'Woo Henry H', 'Koch Matthew J', 'Amin-Hanjani Sepideh', 'Charbel Fady T', 'Rezai Jahromi Behnam', 'Niemelä Mika', 'Koivisto Timo', 'Frosen Juhana', 'Tobe Yasutaka', 'Maiti Spandan', 'Robertson Anne M', 'Cebral Juan R']</td>\n",
       "      <td>['Department of Bioengineering, George Mason University, Fairfax, Virginia, USA ssalimia@gmu.edu.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.', 'Department of Statistics, George Mason University, Fairfax, Virginia, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Neurosurgery Research Group, Biomedicum Helsinki, University of Helsinki, Helsinki, Uusimaa, Finland.', 'Department of Neurosurgery, Töölö Hospital, University of Helsinki, Helsinki, Finland.', 'Department of Neurosurgery, Kuopio University Hospital, Kuopio, Pohjois-Savo, Finland.', 'Department of Neurosurgery, Tampere University Hospital, Tampere, Finland.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.']</td>\n",
       "      <td>['aneurysm', 'blood flow', 'hemorrhage', 'statistics']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population. Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmid                               doi  \\\n",
       "1   34688173  10.1016/j.compbiomed.2021.104924   \n",
       "2   34688172  10.1016/j.compbiomed.2021.104927   \n",
       "8   34687858  10.1016/j.neuroimage.2021.118652   \n",
       "9   34687853         10.1016/j.mri.2021.10.024   \n",
       "10  34687850         10.1016/j.mri.2021.10.023   \n",
       "14  34687347        10.1007/s00330-021-08284-z   \n",
       "21  34686914        10.1007/s00467-021-05321-3   \n",
       "25  34686815         10.1038/s41415-021-3526-6   \n",
       "29  34686646      10.1097/CMR.0000000000000774   \n",
       "31  34686573   10.1136/neurintsurg-2021-017976   \n",
       "\n",
       "                                                                                                                                                                                               title  \\\n",
       "1                                                                              A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.   \n",
       "2                                                                                              A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.   \n",
       "8                                                                                                                                        Causal Decoding of Individual Cortical Excitability States.   \n",
       "9                    Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.   \n",
       "10                                                        MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.   \n",
       "14                                       Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements.   \n",
       "21                                    Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves.   \n",
       "25  The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software.   \n",
       "29                                        Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas.   \n",
       "31                                  Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                 The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8                                                                         Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.   \n",
       "25                                                                                                                                         Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p <0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.   \n",
       "29  To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.   \n",
       "\n",
       "   article_date pubmed_date     article_type lang  \\\n",
       "1    2021-10-06  2021-10-24  Journal Article  eng   \n",
       "2    2021-10-11  2021-10-24  Journal Article  eng   \n",
       "8    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "9    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "10   2021-10-20  2021-10-24  Journal Article  eng   \n",
       "14   2021-10-23  2021-10-24  Journal Article  eng   \n",
       "21   2021-10-22  2021-10-24  Journal Article  eng   \n",
       "25   2021-10-22  2021-10-24  Journal Article  eng   \n",
       "29   2021-10-21  2021-10-24  Journal Article  eng   \n",
       "31   2021-10-22  2021-10-24  Journal Article  eng   \n",
       "\n",
       "                                   journal       journal_short  \\\n",
       "1        Computers in biology and medicine     Comput Biol Med   \n",
       "2        Computers in biology and medicine     Comput Biol Med   \n",
       "8                               NeuroImage          Neuroimage   \n",
       "9               Magnetic resonance imaging  Magn Reson Imaging   \n",
       "10              Magnetic resonance imaging  Magn Reson Imaging   \n",
       "14                      European radiology          Eur Radiol   \n",
       "21  Pediatric nephrology (Berlin, Germany)     Pediatr Nephrol   \n",
       "25                  British dental journal           Br Dent J   \n",
       "29                       Melanoma research        Melanoma Res   \n",
       "31  Journal of neurointerventional surgery  J Neurointerv Surg   \n",
       "\n",
       "   journal_country  \\\n",
       "1    United States   \n",
       "2    United States   \n",
       "8    United States   \n",
       "9      Netherlands   \n",
       "10     Netherlands   \n",
       "14         Germany   \n",
       "21         Germany   \n",
       "25         England   \n",
       "29         England   \n",
       "31         England   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                               authors  \\\n",
       "1                                                                                                                                                                                                                             ['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']   \n",
       "2                                                                                                                                                                                                                                                                                  ['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']   \n",
       "8                                                                                                                                                                                                                                                                             ['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']   \n",
       "9                                                                                                                                   ['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']   \n",
       "10                                                                                                                                                                                                                                                                          ['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']   \n",
       "14                                                                                                                                                                              ['Rühling Sebastian', 'Navarro Fernando', 'Sekuboyina Anjany', 'El Husseini Malek', 'Baum Thomas', 'Menze Bjoern', 'Braren Rickmer', 'Zimmer Claus', 'Kirschke Jan S']   \n",
       "21                                   ['Kwong Jethro Cc', 'Khondker Adree', 'Kim Jin Kyu', 'Chua Michael', 'Keefe Daniel T', 'Dos Santos Joana', 'Skreta Marta', 'Erdman Lauren', \"D'Souza Neeta\", 'Selman Antoine Fermin', 'Weaver John', 'Weiss Dana A', 'Long Christopher', 'Tasian Gregory', 'Teoh Chia Wei', 'Rickard Mandy', 'Lorenzo Armando J']   \n",
       "25                                                                                                                                                                                                                                                                                    ['Devlin Hugh', 'Williams Tomos', 'Graham Jim', 'Ashley Martin']   \n",
       "29                                                                                                                                                                                                                                  ['Moglia Andrea', 'Cerri Amilcare', 'Moglia Alessandra', 'Berchiolli Raffaella', 'Ferrari Mauro', 'Betti Roberto']   \n",
       "31  ['Salimi Ashkezari Seyedeh Fatemeh', 'Mut Fernando', 'Slawski Martin', 'Cheng Boyle', 'Yu Alexander K', 'White Tim G', 'Woo Henry H', 'Koch Matthew J', 'Amin-Hanjani Sepideh', 'Charbel Fady T', 'Rezai Jahromi Behnam', 'Niemelä Mika', 'Koivisto Timo', 'Frosen Juhana', 'Tobe Yasutaka', 'Maiti Spandan', 'Robertson Anne M', 'Cebral Juan R']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         author_affils  \\\n",
       "1   ['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Informatics, Technical University of Munich, Munich, Germany.', 'Department of Diagnostic and Interventional Radiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany. jan.kirschke@tum.de.']   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                            ['Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", 'Division of Nephrology, Hospital for Sick Children, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada. armando.lorenzo@sickkids.ca.']   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ['Professor of Restorative Dentistry, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Director, Manchester Imaging Ltd, UK.', 'Honorary Research Assistant, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Software Manager, Manchester Imaging Ltd, UK. tomos.williams@manchester.ac.uk.', 'Director, Manchester Imaging Ltd, UK; Honorary Reader, Division of Informatics, Imaging and Data Sciences, School of Health Sciences, University of Manchester, UK.', 'Consultant and MAHSC Honorary Professor in Restorative Dentistry and Oral Health, University Dental Hospital of Manchester, Manchester University NHS Foundation Trust, UK.']   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <NA>   \n",
       "31                                                                                                                                                                                                                                                                                                                                     ['Department of Bioengineering, George Mason University, Fairfax, Virginia, USA ssalimia@gmu.edu.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.', 'Department of Statistics, George Mason University, Fairfax, Virginia, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Neurosurgery Research Group, Biomedicum Helsinki, University of Helsinki, Helsinki, Uusimaa, Finland.', 'Department of Neurosurgery, Töölö Hospital, University of Helsinki, Helsinki, Finland.', 'Department of Neurosurgery, Kuopio University Hospital, Kuopio, Pohjois-Savo, Finland.', 'Department of Neurosurgery, Tampere University Hospital, Tampere, Finland.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.']   \n",
       "\n",
       "                                                                                                                         keywords  \\\n",
       "1           ['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']   \n",
       "2   ['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']   \n",
       "8                                             ['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']   \n",
       "9                                                                             ['Endometrial cancer', 'Radiomic machine learning']   \n",
       "10                              ['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']   \n",
       "14                         ['Bone density', 'Machine learning', 'Multidetector computed tomography', 'Osteoporosis', 'Screening']   \n",
       "21        ['Catheterization', 'Chronic kidney disease', 'Dialysis', 'Machine learning', 'Posterior urethral valve', 'Transplant']   \n",
       "25                                                                                                                           <NA>   \n",
       "29                                                                                                                           <NA>   \n",
       "31                                                                         ['aneurysm', 'blood flow', 'hemorrhage', 'statistics']   \n",
       "\n",
       "   mesh_terms references_pmids  \\\n",
       "1        <NA>             <NA>   \n",
       "2        <NA>             <NA>   \n",
       "8        <NA>             <NA>   \n",
       "9        <NA>             <NA>   \n",
       "10       <NA>             <NA>   \n",
       "14       <NA>             <NA>   \n",
       "21       <NA>             <NA>   \n",
       "25       <NA>             <NA>   \n",
       "29       <NA>             <NA>   \n",
       "31       <NA>             <NA>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                      A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8                                                                                                                                                                        Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements. To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves. Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.   \n",
       "25                                                                                                   The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software. Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p <0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.   \n",
       "29  Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas. To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population. Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.   \n",
       "\n",
       "   include mature algo_neural_net algo_support_vector algo_regression  \\\n",
       "1      1.0    1.0               1                   0               0   \n",
       "2      1.0    0.0               0                   0               0   \n",
       "8      1.0    0.0               0                   0               0   \n",
       "9      1.0    1.0               0                   0               0   \n",
       "10     1.0    0.0               0                   0               0   \n",
       "14     1.0    0.0               1                   0               0   \n",
       "21     1.0    0.0               0                   0               0   \n",
       "25     1.0    1.0               0                   0               0   \n",
       "29     1.0    0.0               0                   0               0   \n",
       "31     1.0    0.0               0                   0               0   \n",
       "\n",
       "   algo_decision_tree algo_discriminant algo_naive_bayes algo_transfer  \\\n",
       "1                   0                 0                0             0   \n",
       "2                   0                 0                0             1   \n",
       "8                   0                 0                0             0   \n",
       "9                   0                 0                0             0   \n",
       "10                  0                 0                0             0   \n",
       "14                  0                 0                0             0   \n",
       "21                  0                 0                0             0   \n",
       "25                  0                 0                0             0   \n",
       "29                  0                 0                0             0   \n",
       "31                  0                 0                0             0   \n",
       "\n",
       "   algo_federated algo_k_nearest algo_unsupervised feat_xr feat_ct feat_mri  \\\n",
       "1               0              0                 0       0       0        0   \n",
       "2               0              0                 0       1       0        0   \n",
       "8               0              0                 0       0       0        0   \n",
       "9               0              0                 0       0       0        1   \n",
       "10              0              0                 0       0       0        1   \n",
       "14              0              0                 0       0       1        0   \n",
       "21              0              0                 0       0       0        0   \n",
       "25              0              0                 0       1       0        0   \n",
       "29              0              0                 0       0       0        0   \n",
       "31              0              0                 0       0       0        0   \n",
       "\n",
       "   feat_eeg feat_ecg feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm  \\\n",
       "1         0        0        0       0         0          0        0         0   \n",
       "2         0        0        0       0         0          0        0         0   \n",
       "8         1        0        0       0         0          0        0         0   \n",
       "9         0        0        0       0         0          0        0         0   \n",
       "10        0        0        0       0         0          0        0         0   \n",
       "14        0        0        0       0         0          0        0         0   \n",
       "21        0        0        0       0         0          0        0         0   \n",
       "25        0        0        0       0         0          0        0         0   \n",
       "29        0        0        0       0         0          1        0         0   \n",
       "31        0        0        0       0         0          0        0         0   \n",
       "\n",
       "   feat_endoscop feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor  \\\n",
       "1              0         1         0        0        0        0           0   \n",
       "2              0         0         0        0        0        0           0   \n",
       "8              0         0         0        0        0        0           0   \n",
       "9              0         0         0        0        0        0           0   \n",
       "10             0         0         0        1        0        0           0   \n",
       "14             0         0         0        0        0        0           0   \n",
       "21             0         0         0        0        0        0           0   \n",
       "25             0         0         0        0        0        0           0   \n",
       "29             0         0         0        0        0        0           0   \n",
       "31             0         0         0        0        0        0           0   \n",
       "\n",
       "   feat_phone feat_prom feat_sound subspec_icu subspec_ed spec_paeds  \\\n",
       "1           0         0          0           0          0          0   \n",
       "2           0         0          0           0          0          0   \n",
       "8           0         0          0           0          0          0   \n",
       "9           0         0          0           0          0          0   \n",
       "10          0         0          0           0          0          0   \n",
       "14          0         0          0           0          0          0   \n",
       "21          0         0          0           0          0          0   \n",
       "25          0         0          0           0          0          0   \n",
       "29          0         0          0           0          0          0   \n",
       "31          0         0          0           0          0          0   \n",
       "\n",
       "   spec_dent spec_audio spec_id subspec_sepsis subspec_hiv subspec_cov19  \\\n",
       "1          0          0       0              0           0             0   \n",
       "2          0          0       1              0           0             1   \n",
       "8          0          0       0              0           0             0   \n",
       "9          0          0       0              0           0             0   \n",
       "10         0          0       0              0           0             0   \n",
       "14         0          0       0              0           0             0   \n",
       "21         0          0       0              0           0             0   \n",
       "25         1          0       0              0           0             0   \n",
       "29         0          0       0              0           0             0   \n",
       "31         0          0       0              0           0             0   \n",
       "\n",
       "   subspec_tb subspec_malaria subspec_tropic spec_derm subspec_dermca  \\\n",
       "1           0               0              0         1              0   \n",
       "2           0               0              0         0              0   \n",
       "8           0               0              0         0              0   \n",
       "9           0               0              0         0              0   \n",
       "10          0               0              0         0              0   \n",
       "14          0               0              0         0              0   \n",
       "21          0               0              0         0              0   \n",
       "25          0               0              0         0              0   \n",
       "29          0               0              0         1              1   \n",
       "31          0               0              0         0              0   \n",
       "\n",
       "   spec_onc subspec_rx subspec_lungca subspec_brainca subspec_gica  \\\n",
       "1         0          0              0               0            0   \n",
       "2         0          0              0               0            0   \n",
       "8         0          0              0               0            0   \n",
       "9         1          0              0               0            0   \n",
       "10        1          0              0               1            0   \n",
       "14        0          0              0               0            0   \n",
       "21        0          0              0               0            0   \n",
       "25        0          0              0               0            0   \n",
       "29        1          0              0               0            0   \n",
       "31        0          0              0               0            0   \n",
       "\n",
       "   subspec_hepca subspec_prosca subspec_gynonc subspec_renalca  \\\n",
       "1              0              0              0               0   \n",
       "2              0              0              0               0   \n",
       "8              0              0              0               0   \n",
       "9              0              0              1               0   \n",
       "10             0              0              0               0   \n",
       "14             0              0              0               0   \n",
       "21             0              0              0               0   \n",
       "25             0              0              0               0   \n",
       "29             0              0              0               0   \n",
       "31             0              0              0               0   \n",
       "\n",
       "   subspec_haemonc subspec_breast subspec_breastca subspec_urology spec_psych  \\\n",
       "1                0              0                0               0          1   \n",
       "2                0              0                0               0          0   \n",
       "8                0              0                0               0          0   \n",
       "9                0              0                0               0          0   \n",
       "10               0              0                0               0          0   \n",
       "14               0              0                0               0          0   \n",
       "21               0              0                0               1          0   \n",
       "25               0              0                0               0          0   \n",
       "29               0              0                0               0          0   \n",
       "31               0              0                0               0          0   \n",
       "\n",
       "   subspec_suicide spec_msk subspec_frac spec_rheum spec_gi spec_hep  \\\n",
       "1                0        0            0          0       0        0   \n",
       "2                0        0            0          0       0        0   \n",
       "8                0        0            0          0       0        0   \n",
       "9                0        0            0          0       0        0   \n",
       "10               0        0            0          0       0        0   \n",
       "14               0        1            0          0       0        0   \n",
       "21               0        0            0          0       0        0   \n",
       "25               0        0            0          0       0        0   \n",
       "29               0        0            0          0       0        0   \n",
       "31               0        0            0          0       0        0   \n",
       "\n",
       "   spec_resp subspec_pneum subspec_osa subspec_pe spec_neuro subspec_epilep  \\\n",
       "1          0             0           0          0          0              0   \n",
       "2          1             1           0          0          0              0   \n",
       "8          0             0           0          0          1              0   \n",
       "9          0             0           0          0          0              0   \n",
       "10         0             0           0          0          1              0   \n",
       "14         0             0           0          0          0              0   \n",
       "21         0             0           0          0          0              0   \n",
       "25         0             0           0          0          0              0   \n",
       "29         0             0           0          0          0              0   \n",
       "31         0             0           0          0          1              0   \n",
       "\n",
       "   subspec_cva subspec_alzh spec_cvs subspec_ihd subspec_hf subspec_arrhyt  \\\n",
       "1            0            0        0           0          0              0   \n",
       "2            0            0        0           0          0              0   \n",
       "8            0            0        0           0          0              0   \n",
       "9            0            0        0           0          0              0   \n",
       "10           0            0        0           0          0              0   \n",
       "14           0            0        0           0          0              0   \n",
       "21           0            0        0           0          0              0   \n",
       "25           0            0        0           0          0              0   \n",
       "29           0            0        0           0          0              0   \n",
       "31           1            0        0           0          0              0   \n",
       "\n",
       "   spec_endo spec_dm subspec_insulin spec_eye subspec_retina spec_haem  \\\n",
       "1          0       0               0        0              0         0   \n",
       "2          0       0               0        0              0         0   \n",
       "8          0       0               0        0              0         0   \n",
       "9          0       0               0        0              0         0   \n",
       "10         0       0               0        0              0         0   \n",
       "14         0       0               0        0              0         0   \n",
       "21         0       0               0        0              0         0   \n",
       "25         0       0               0        0              0         0   \n",
       "29         0       0               0        0              0         0   \n",
       "31         0       0               0        0              0         0   \n",
       "\n",
       "   spec_obs spec_renal subspec_ackd spec_pubh subspec_bci subspec_prosth  \\\n",
       "1         0          0            0         0           0              0   \n",
       "2         0          0            0         0           0              0   \n",
       "8         0          0            0         0           0              0   \n",
       "9         1          0            0         0           0              0   \n",
       "10        0          0            0         0           0              0   \n",
       "14        0          0            0         0           0              0   \n",
       "21        0          1            0         0           0              0   \n",
       "25        0          0            0         0           0              0   \n",
       "29        0          0            0         0           0              0   \n",
       "31        0          0            0         0           0              0   \n",
       "\n",
       "   subspec_assist subspec_activity  \n",
       "1               0                0  \n",
       "2               0                0  \n",
       "8               0                0  \n",
       "9               0                0  \n",
       "10              0                0  \n",
       "14              0                0  \n",
       "21              0                0  \n",
       "25              0                0  \n",
       "29              0                0  \n",
       "31              0                0  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4deaf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_ner = all_tagged[['text', \n",
    "#                        'algo_neural_net', 'algo_support_vector', 'algo_regression', 'algo_decision_tree', \n",
    "#                       'algo_discriminant', 'algo_naive_bayes', 'algo_transfer', 'algo_federated', 'algo_k_nearest',\n",
    "#                       'algo_unsupervised',\n",
    "#                        'feat_imaging', 'feat_xr', 'feat_ct', 'feat_mri', 'feat_eeg', 'feat_ecg',\n",
    "#                       'feat_us', 'feat_echo', 'feat_histo', 'feat_oct', 'feat_mamm', 'feat_endoscop', 'feat_derm',\n",
    "#                       'feat_gene', 'feat_bio', 'feat_nlp', 'feat_ehr', 'feat_sensor', 'feat_phone', \n",
    "#                        'subspec_icu', 'subspec_ed', 'spec_id', 'subspec_sepsis', 'subspec_hiv', 'subspec_cov19', 'subspec_tb',\n",
    "#                       'subspec_malaria', 'spec_derm', 'subspec_dermca', 'spec_onc', 'subspec_rx', 'subspec_gynonc', \n",
    "#                       'subspec_lungca', 'subspec_brainca', 'subspec_gica', 'subspec_hepca', 'subspec_prosca',\n",
    "#                       'subspec_renalca', 'subspec_haemonc', 'subspec_breast', 'spec_psych','subspec_suicide', 'spec_msk', \n",
    "#                        'subspec_frac', 'spec_rheum', 'spec_gi', 'spec_hep', 'spec_resp', 'subspec_pneum',\n",
    "#                        'spec_neuro', 'subspec_epilep', 'subspec_cva', 'subspec_alzh', 'spec_cvs', 'subspec_ihd', 'subspec_hf', \n",
    "#                       'spec_endo', 'subspec_dm', 'spec_eye', 'subspec_retina', 'spec_haem', 'spec_obs', 'spec_renal', \n",
    "#                        'subspec_ackd', 'spec_paeds', 'spec_dent',  'spec_audio', 'spec_pubh', 'subspec_bci',\n",
    "#                       'subspec_prosth', 'subspec_assist','subspec_activity', 'subspec_arrhyt', 'countries', 'lmic_flag']].copy()\n",
    "#\n",
    "#final_ner.to_csv('output/final_ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a915ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled.to_csv('data/char_labelled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41263789",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "74d770bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_eval = labelled.drop(['doi', 'title', 'abstract', 'article_date', 'pubmed_date', 'article_type', 'lang', 'journal', 'journal_short',\n",
    "                         'journal_country', 'authors', 'author_affils', 'keywords', 'mesh_terms', 'references_pmids', 'include', 'mature'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f786f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>feature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>subspec_urology</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid  \\\n",
       "1  34688173   \n",
       "2  34688172   \n",
       "8  34687858   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8  Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "\n",
       "  algo_neural_net algo_support_vector algo_regression algo_decision_tree  \\\n",
       "1               1                   0               0                  0   \n",
       "2               0                   0               0                  0   \n",
       "8               0                   0               0                  0   \n",
       "\n",
       "  algo_discriminant algo_naive_bayes algo_transfer algo_federated  \\\n",
       "1                 0                0             0              0   \n",
       "2                 0                0             1              0   \n",
       "8                 0                0             0              0   \n",
       "\n",
       "  algo_k_nearest algo_unsupervised feat_xr feat_ct feat_mri feat_eeg feat_ecg  \\\n",
       "1              0                 0       0       0        0        0        0   \n",
       "2              0                 0       1       0        0        0        0   \n",
       "8              0                 0       0       0        0        1        0   \n",
       "\n",
       "  feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm feat_endoscop  \\\n",
       "1        0       0         0          0        0         0             0   \n",
       "2        0       0         0          0        0         0             0   \n",
       "8        0       0         0          0        0         0             0   \n",
       "\n",
       "  feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor feat_phone  \\\n",
       "1         1         0        0        0        0           0          0   \n",
       "2         0         0        0        0        0           0          0   \n",
       "8         0         0        0        0        0           0          0   \n",
       "\n",
       "  feat_prom feat_sound subspec_icu subspec_ed spec_paeds spec_dent spec_audio  \\\n",
       "1         0          0           0          0          0         0          0   \n",
       "2         0          0           0          0          0         0          0   \n",
       "8         0          0           0          0          0         0          0   \n",
       "\n",
       "  spec_id subspec_sepsis subspec_hiv subspec_cov19 subspec_tb subspec_malaria  \\\n",
       "1       0              0           0             0          0               0   \n",
       "2       1              0           0             1          0               0   \n",
       "8       0              0           0             0          0               0   \n",
       "\n",
       "  subspec_tropic spec_derm subspec_dermca spec_onc subspec_rx subspec_lungca  \\\n",
       "1              0         1              0        0          0              0   \n",
       "2              0         0              0        0          0              0   \n",
       "8              0         0              0        0          0              0   \n",
       "\n",
       "  subspec_brainca subspec_gica subspec_hepca subspec_prosca subspec_gynonc  \\\n",
       "1               0            0             0              0              0   \n",
       "2               0            0             0              0              0   \n",
       "8               0            0             0              0              0   \n",
       "\n",
       "  subspec_renalca subspec_haemonc subspec_breast subspec_breastca  \\\n",
       "1               0               0              0                0   \n",
       "2               0               0              0                0   \n",
       "8               0               0              0                0   \n",
       "\n",
       "  subspec_urology spec_psych subspec_suicide spec_msk subspec_frac spec_rheum  \\\n",
       "1               0          1               0        0            0          0   \n",
       "2               0          0               0        0            0          0   \n",
       "8               0          0               0        0            0          0   \n",
       "\n",
       "  spec_gi spec_hep spec_resp subspec_pneum subspec_osa subspec_pe spec_neuro  \\\n",
       "1       0        0         0             0           0          0          0   \n",
       "2       0        0         1             1           0          0          0   \n",
       "8       0        0         0             0           0          0          1   \n",
       "\n",
       "  subspec_epilep subspec_cva subspec_alzh spec_cvs subspec_ihd subspec_hf  \\\n",
       "1              0           0            0        0           0          0   \n",
       "2              0           0            0        0           0          0   \n",
       "8              0           0            0        0           0          0   \n",
       "\n",
       "  subspec_arrhyt spec_endo spec_dm subspec_insulin spec_eye subspec_retina  \\\n",
       "1              0         0       0               0        0              0   \n",
       "2              0         0       0               0        0              0   \n",
       "8              0         0       0               0        0              0   \n",
       "\n",
       "  spec_haem spec_obs spec_renal subspec_ackd spec_pubh subspec_bci  \\\n",
       "1         0        0          0            0         0           0   \n",
       "2         0        0          0            0         0           0   \n",
       "8         0        0          0            0         0           0   \n",
       "\n",
       "  subspec_prosth subspec_assist subspec_activity  \n",
       "1              0              0                0  \n",
       "2              0              0                0  \n",
       "8              0              0                0  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9576fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdata = ner_eval.apply(lambda s: [s.name if v == \"1\" else np.nan for v in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b26572aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>feature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>subspec_urology</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algo_neural_net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feat_derm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_derm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_psych</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algo_transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feat_xr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_id</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>subspec_cov19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_resp</td>\n",
       "      <td>subspec_pneum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feat_eeg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_neuro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmid  feature  algo_neural_net algo_support_vector algo_regression  \\\n",
       "1   NaN      NaN  algo_neural_net                 NaN             NaN   \n",
       "2   NaN      NaN              NaN                 NaN             NaN   \n",
       "8   NaN      NaN              NaN                 NaN             NaN   \n",
       "\n",
       "  algo_decision_tree algo_discriminant algo_naive_bayes  algo_transfer  \\\n",
       "1                NaN               NaN              NaN            NaN   \n",
       "2                NaN               NaN              NaN  algo_transfer   \n",
       "8                NaN               NaN              NaN            NaN   \n",
       "\n",
       "  algo_federated algo_k_nearest algo_unsupervised  feat_xr feat_ct feat_mri  \\\n",
       "1            NaN            NaN               NaN      NaN     NaN      NaN   \n",
       "2            NaN            NaN               NaN  feat_xr     NaN      NaN   \n",
       "8            NaN            NaN               NaN      NaN     NaN      NaN   \n",
       "\n",
       "   feat_eeg feat_ecg feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm  \\\n",
       "1       NaN      NaN      NaN     NaN       NaN        NaN      NaN       NaN   \n",
       "2       NaN      NaN      NaN     NaN       NaN        NaN      NaN       NaN   \n",
       "8  feat_eeg      NaN      NaN     NaN       NaN        NaN      NaN       NaN   \n",
       "\n",
       "  feat_endoscop  feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor  \\\n",
       "1           NaN  feat_derm       NaN      NaN      NaN      NaN         NaN   \n",
       "2           NaN        NaN       NaN      NaN      NaN      NaN         NaN   \n",
       "8           NaN        NaN       NaN      NaN      NaN      NaN         NaN   \n",
       "\n",
       "  feat_phone feat_prom feat_sound subspec_icu subspec_ed spec_paeds spec_dent  \\\n",
       "1        NaN       NaN        NaN         NaN        NaN        NaN       NaN   \n",
       "2        NaN       NaN        NaN         NaN        NaN        NaN       NaN   \n",
       "8        NaN       NaN        NaN         NaN        NaN        NaN       NaN   \n",
       "\n",
       "  spec_audio  spec_id subspec_sepsis subspec_hiv  subspec_cov19 subspec_tb  \\\n",
       "1        NaN      NaN            NaN         NaN            NaN        NaN   \n",
       "2        NaN  spec_id            NaN         NaN  subspec_cov19        NaN   \n",
       "8        NaN      NaN            NaN         NaN            NaN        NaN   \n",
       "\n",
       "  subspec_malaria subspec_tropic  spec_derm subspec_dermca spec_onc  \\\n",
       "1             NaN            NaN  spec_derm            NaN      NaN   \n",
       "2             NaN            NaN        NaN            NaN      NaN   \n",
       "8             NaN            NaN        NaN            NaN      NaN   \n",
       "\n",
       "  subspec_rx subspec_lungca subspec_brainca subspec_gica subspec_hepca  \\\n",
       "1        NaN            NaN             NaN          NaN           NaN   \n",
       "2        NaN            NaN             NaN          NaN           NaN   \n",
       "8        NaN            NaN             NaN          NaN           NaN   \n",
       "\n",
       "  subspec_prosca subspec_gynonc subspec_renalca subspec_haemonc  \\\n",
       "1            NaN            NaN             NaN             NaN   \n",
       "2            NaN            NaN             NaN             NaN   \n",
       "8            NaN            NaN             NaN             NaN   \n",
       "\n",
       "  subspec_breast subspec_breastca subspec_urology  spec_psych subspec_suicide  \\\n",
       "1            NaN              NaN             NaN  spec_psych             NaN   \n",
       "2            NaN              NaN             NaN         NaN             NaN   \n",
       "8            NaN              NaN             NaN         NaN             NaN   \n",
       "\n",
       "  spec_msk subspec_frac spec_rheum spec_gi spec_hep  spec_resp  subspec_pneum  \\\n",
       "1      NaN          NaN        NaN     NaN      NaN        NaN            NaN   \n",
       "2      NaN          NaN        NaN     NaN      NaN  spec_resp  subspec_pneum   \n",
       "8      NaN          NaN        NaN     NaN      NaN        NaN            NaN   \n",
       "\n",
       "  subspec_osa subspec_pe  spec_neuro subspec_epilep subspec_cva subspec_alzh  \\\n",
       "1         NaN        NaN         NaN            NaN         NaN          NaN   \n",
       "2         NaN        NaN         NaN            NaN         NaN          NaN   \n",
       "8         NaN        NaN  spec_neuro            NaN         NaN          NaN   \n",
       "\n",
       "  spec_cvs subspec_ihd subspec_hf subspec_arrhyt spec_endo spec_dm  \\\n",
       "1      NaN         NaN        NaN            NaN       NaN     NaN   \n",
       "2      NaN         NaN        NaN            NaN       NaN     NaN   \n",
       "8      NaN         NaN        NaN            NaN       NaN     NaN   \n",
       "\n",
       "  subspec_insulin spec_eye subspec_retina spec_haem spec_obs spec_renal  \\\n",
       "1             NaN      NaN            NaN       NaN      NaN        NaN   \n",
       "2             NaN      NaN            NaN       NaN      NaN        NaN   \n",
       "8             NaN      NaN            NaN       NaN      NaN        NaN   \n",
       "\n",
       "  subspec_ackd spec_pubh subspec_bci subspec_prosth subspec_assist  \\\n",
       "1          NaN       NaN         NaN            NaN            NaN   \n",
       "2          NaN       NaN         NaN            NaN            NaN   \n",
       "8          NaN       NaN         NaN            NaN            NaN   \n",
       "\n",
       "  subspec_activity  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "8              NaN  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerdata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ec367934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_eval['result'] = nerdata.apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd418773",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_eval.to_csv('data/char_labelled_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79062737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
